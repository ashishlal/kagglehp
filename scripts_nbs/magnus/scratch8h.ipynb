{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/mathormad/inceptionv3-baseline-lb-0-379/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) + (1 - K.mean(f1))\n",
    "\n",
    "# POS_WEIGHT = 1.0  # multiplier for positive targets, needs to be tuned\n",
    "\n",
    "# def weighted_binary_crossentropy(target, output):\n",
    "#     \"\"\"\n",
    "#     Weighted binary crossentropy between an output tensor \n",
    "#     and a target tensor. POS_WEIGHT is used as a multiplier \n",
    "#     for the positive targets.\n",
    "\n",
    "#     Combination of the following functions:\n",
    "#     * keras.losses.binary_crossentropy\n",
    "#     * keras.backend.tensorflow_backend.binary_crossentropy\n",
    "#     * tf.nn.weighted_cross_entropy_with_logits\n",
    "#     \"\"\"\n",
    "#     # transform back to logits\n",
    "# #     _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "#     _epsilon = K.epsilon()\n",
    "#     output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "#     output = tf.log(output / (1 - output))\n",
    "#     # compute weighted loss\n",
    "#     loss = tf.nn.weighted_cross_entropy_with_logits(targets=target,\n",
    "#                                                     logits=output,\n",
    "#                                                     pos_weight=POS_WEIGHT)\n",
    "#     return tf.reduce_mean(loss, axis=-1)\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.python.framework import ops\n",
    "# from functools import reduce\n",
    "\n",
    "# def binaryRound(x):\n",
    "#     \"\"\"\n",
    "#     Rounds a tensor whose values are in [0,1] to a tensor with values in {0, 1},\n",
    "#     using the straight through estimator for the gradient.\n",
    "#     \"\"\"\n",
    "#     g = tf.get_default_graph()\n",
    "\n",
    "#     with ops.name_scope(\"BinaryRound\") as name:\n",
    "#         with g.gradient_override_map({\"Round\": \"Identity\"}):\n",
    "#             return tf.round(x, name=name)\n",
    "\n",
    "#         # For Tensorflow v0.11 and below use:\n",
    "#         #with g.gradient_override_map({\"Floor\": \"Identity\"}):\n",
    "#         #    return tf.round(x, name=name)\n",
    "        \n",
    "# def f1_loss2(y_true, y_pred):\n",
    "#     y_pred = binaryRound(y_pred)\n",
    "#     tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "#     tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "#     fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "#     fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "#     p = tp / (tp + fp + K.epsilon())\n",
    "#     r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "#     f1 = 2*p*r / (p+r+K.epsilon())\n",
    "#     f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "#     return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) + (1-K.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "path_to_train = '../data/train/'\n",
    "data = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_info = []\n",
    "for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "    train_dataset_info.append({\n",
    "        'path':os.path.join(path_to_train, name),\n",
    "        'labels':np.array([int(label) for label in labels])})\n",
    "train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'path': '../data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0', 'labels': array([16,  0])},\n",
       "       {'path': '../data/train/000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', 'labels': array([7, 1, 2, 0])},\n",
       "       {'path': '../data/train/000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([5])},\n",
       "       ...,\n",
       "       {'path': '../data/train/fff189d8-bbab-11e8-b2ba-ac1f6b6435d0', 'labels': array([7])},\n",
       "       {'path': '../data/train/fffdf7e0-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([25,  2, 21])},\n",
       "       {'path': '../data/train/fffe0ffe-bbc0-11e8-b2bb-ac1f6b6435d0', 'labels': array([2, 0])}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rgb_arr = np.memmap('../cache/tmp_rgb_arr', dtype='uint8', mode='r+', \n",
    "#                    shape=(len(train_dataset_info),299,299,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            dataset_info = shuffle(dataset_info)\n",
    "            for start in range(0, len(dataset_info), batch_size):\n",
    "                end = min(start + batch_size, len(dataset_info))\n",
    "                batch_images = []\n",
    "                X_train_batch = dataset_info[start:end]\n",
    "                batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "                for i in range(len(X_train_batch)):\n",
    "                    image = data_generator.load_image(\n",
    "                        X_train_batch[i]['path'], shape) \n",
    "#                     image = data_generator.load_image(\n",
    "#                         i, shape) \n",
    "                    if augument:\n",
    "                        image = data_generator.augment(image)\n",
    "                    batch_images.append(image/255.)\n",
    "                    batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "                yield np.array(batch_images, np.float32), batch_labels\n",
    "    def load_image(path, shape):\n",
    "        img1 = cv2.imread(path+'_red.png', cv2.IMREAD_GRAYSCALE)\n",
    "        img2 = cv2.imread(path+'_green.png', cv2.IMREAD_GRAYSCALE)\n",
    "        img3 = cv2.imread(path+'_blue.png', cv2.IMREAD_GRAYSCALE)\n",
    "        image = np.stack((img1,img2,img3), -1)\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    def load_image3(idx, shape):\n",
    "#         print(idx)\n",
    "        name = '../cache/RGB/img-{}.png'.format(idx)\n",
    "        image = cv2.imread(name)\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    def load_image2(path, shape):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image = np.stack((\n",
    "            np.array(image_red_ch),\n",
    "            np.array(image_green_ch), \n",
    "            np.array(image_blue_ch)), -1)\n",
    "        w, h = 512, 512\n",
    "        zero_data = np.zeros((h, w), dtype=np.uint8)\n",
    "#         image2 = np.stack((\n",
    "#             np.array(image_yellow_ch),\n",
    "#             zero_data, zero_data), -1)\n",
    "#         print(image1.shape, image2.shape)\n",
    "#         image = np.vstack((image1, image2))\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    \n",
    "    def augment(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=0),\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "            ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=input_shape)\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = base_model(bn)\n",
    "    x = Conv2D(512, kernel_size=(1,1), activation='relu', name='conv2')(x)\n",
    "#     x = Conv2D(32, kernel_size=(1,1), activation='relu', name='conv1')(x)\n",
    "    x = Flatten()(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "# https://github.com/ailias/Focal-Loss-implement-on-Tensorflow/blob/master/focal_loss.py\n",
    "def focal_loss_org(prediction_tensor, target_tensor, weights=None, alpha=0.25, gamma=2):\n",
    "    r\"\"\"Compute focal loss for predictions.\n",
    "        Multi-labels Focal loss formula:\n",
    "            FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                 ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "    Args:\n",
    "     prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing the predicted logits for each class\n",
    "     target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing one-hot encoded classification targets\n",
    "     weights: A float tensor of shape [batch_size, num_anchors]\n",
    "     alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "     gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "    Returns:\n",
    "        loss: A (scalar) tensor representing the value of the loss function\n",
    "    \"\"\"\n",
    "    sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                          - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(weights=None, alpha=0.25, gamma=2):\n",
    "    def focal_loss_my(target_tensor, prediction_tensor, ):\n",
    "        r\"\"\"Compute focal loss for predictions.\n",
    "            Multi-labels Focal loss formula:\n",
    "                FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                     ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "        Args:\n",
    "         prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "            num_classes] representing the predicted logits for each class\n",
    "         target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "            num_classes] representing one-hot encoded classification targets\n",
    "         weights: A float tensor of shape [batch_size, num_anchors]\n",
    "         alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "         gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "        Returns:\n",
    "            loss: A (scalar) tensor representing the value of the loss function\n",
    "        \"\"\"\n",
    "        sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "        zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "\n",
    "        # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "        # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "        pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "\n",
    "        # For negative prediction, only need consider back part loss, front part is 0;\n",
    "        # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "        neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "        per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                              - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "        return tf.reduce_sum(per_entry_cross_ent)\n",
    "#         return K.mean(K.binary_crossentropy(target_tensor, prediction_tensor), axis=-1) + tf.reduce_sum(per_entry_cross_ent)\n",
    "    return focal_loss_my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_fixed(y_true, y_pred):\n",
    "    gamma = 2.\n",
    "    alpha = 0.25\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "#     pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "#     pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def focal_loss(gamma=2., alpha=.25):\n",
    "#     def focal_loss_fixed(y_true, y_pred):\n",
    "#         pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "#         pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "#         pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "#         pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "#         return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "#     return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epochs = 10; batch_size = 16\n",
    "checkpoint = ModelCheckpoint('../cache/InceptionV3-8h.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
    "                                   verbose=1, mode='auto', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=6)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split data into train, valid\n",
    "indexes = np.arange(train_dataset_info.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "train_indexes, valid_indexes = train_test_split(indexes, test_size=0.15, random_state=8)\n",
    "\n",
    "# create train and valid datagens\n",
    "train_generator = data_generator.create_train(\n",
    "    train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "validation_generator = data_generator.create_train(\n",
    "    train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 299, 299, 3)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 299, 299, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 8, 8, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 8, 8, 512)         1049088   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              33555456  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 56,436,040\n",
      "Trainable params: 56,401,602\n",
      "Non-trainable params: 34,438\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# warm up model\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3), \n",
    "    n_out=28)\n",
    "model.summary()\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "model.layers[-1].trainable = True\n",
    "model.layers[-2].trainable = True\n",
    "model.layers[-3].trainable = True\n",
    "model.layers[-4].trainable = True\n",
    "model.layers[-5].trainable = True\n",
    "# model.layers[-6].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=f1_loss, \n",
    "    optimizer=Adam(1e-03),\n",
    "    metrics=[f1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1651/1651 [==============================] - 344s 209ms/step - loss: 1.1137 - f1: 0.0466 - val_loss: 1.1747 - val_f1: 0.0305\n",
      "Epoch 2/2\n",
      "1651/1651 [==============================] - 334s 202ms/step - loss: 1.0954 - f1: 0.0633 - val_loss: 1.1759 - val_f1: 0.0307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe56815f828>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=16\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=2, \n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "1651/1651 [==============================] - 453s 275ms/step - loss: 1.0321 - f1: 0.1226 - val_loss: 0.9579 - val_f1: 0.2104\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.95789, saving model to ../cache/InceptionV3-8h.h5\n",
      "Epoch 2/120\n",
      "1651/1651 [==============================] - 434s 263ms/step - loss: 0.9539 - f1: 0.1934 - val_loss: 0.9002 - val_f1: 0.2535\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.95789 to 0.90015, saving model to ../cache/InceptionV3-8h.h5\n",
      "Epoch 3/120\n",
      "1651/1651 [==============================] - 432s 262ms/step - loss: 0.9108 - f1: 0.2269 - val_loss: 0.8600 - val_f1: 0.2781\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.90015 to 0.86000, saving model to ../cache/InceptionV3-8h.h5\n",
      "Epoch 4/120\n",
      "1651/1651 [==============================] - 431s 261ms/step - loss: 0.8888 - f1: 0.2406 - val_loss: 0.8345 - val_f1: 0.2999\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.86000 to 0.83453, saving model to ../cache/InceptionV3-8h.h5\n",
      "Epoch 5/120\n",
      "1651/1651 [==============================] - 431s 261ms/step - loss: 0.8719 - f1: 0.2511 - val_loss: 0.8186 - val_f1: 0.3213\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.83453 to 0.81859, saving model to ../cache/InceptionV3-8h.h5\n",
      "Epoch 6/120\n",
      "1651/1651 [==============================] - 433s 262ms/step - loss: 0.8593 - f1: 0.2605 - val_loss: 0.8445 - val_f1: 0.2977\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.81859\n",
      "Epoch 7/120\n",
      "1651/1651 [==============================] - 432s 261ms/step - loss: 0.8449 - f1: 0.2704 - val_loss: 0.8002 - val_f1: 0.3291\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.81859 to 0.80019, saving model to ../cache/InceptionV3-8h.h5\n",
      "Epoch 8/120\n",
      "1651/1651 [==============================] - 435s 264ms/step - loss: 0.8377 - f1: 0.2738 - val_loss: 0.7773 - val_f1: 0.3424\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.80019 to 0.77730, saving model to ../cache/InceptionV3-8h.h5\n",
      "Epoch 9/120\n",
      "1651/1651 [==============================] - 434s 263ms/step - loss: 0.8275 - f1: 0.2818 - val_loss: 0.7701 - val_f1: 0.3509\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.77730 to 0.77012, saving model to ../cache/InceptionV3-8h.h5\n",
      "Epoch 10/120\n",
      "1651/1651 [==============================] - 435s 263ms/step - loss: 0.8194 - f1: 0.2866 - val_loss: 0.8109 - val_f1: 0.3271\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.77012\n",
      "Epoch 11/120\n",
      "1651/1651 [==============================] - 434s 263ms/step - loss: 0.8135 - f1: 0.2905 - val_loss: 0.8083 - val_f1: 0.3306\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.77012\n",
      "Epoch 12/120\n",
      "1651/1651 [==============================] - 434s 263ms/step - loss: 0.8055 - f1: 0.2952 - val_loss: 0.7816 - val_f1: 0.3475\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.77012\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 13/120\n",
      "1651/1651 [==============================] - 435s 263ms/step - loss: 0.7783 - f1: 0.3112 - val_loss: 0.7258 - val_f1: 0.3841\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.77012 to 0.72577, saving model to ../cache/InceptionV3-8h.h5\n",
      "Epoch 14/120\n",
      "1651/1651 [==============================] - 433s 262ms/step - loss: 0.7664 - f1: 0.3195 - val_loss: 0.7308 - val_f1: 0.3788\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.72577\n",
      "Epoch 15/120\n",
      "1651/1651 [==============================] - 434s 263ms/step - loss: 0.7623 - f1: 0.3214 - val_loss: 0.7290 - val_f1: 0.3802\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.72577\n",
      "Epoch 16/120\n",
      "1651/1651 [==============================] - 434s 263ms/step - loss: 0.7575 - f1: 0.3246 - val_loss: 0.7242 - val_f1: 0.3839\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.72577 to 0.72424, saving model to ../cache/InceptionV3-8h.h5\n",
      "Epoch 17/120\n",
      "1651/1651 [==============================] - 435s 264ms/step - loss: 0.7544 - f1: 0.3266 - val_loss: 0.7259 - val_f1: 0.3786\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.72424\n",
      "Epoch 18/120\n",
      "1651/1651 [==============================] - 435s 263ms/step - loss: 0.7502 - f1: 0.3287 - val_loss: 0.7248 - val_f1: 0.3826\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.72424\n",
      "Epoch 19/120\n",
      "1651/1651 [==============================] - 434s 263ms/step - loss: 0.7487 - f1: 0.3293 - val_loss: 0.7267 - val_f1: 0.3824\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.72424\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 20/120\n",
      "1651/1651 [==============================] - 433s 262ms/step - loss: 0.7433 - f1: 0.3329 - val_loss: 0.7249 - val_f1: 0.3840\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.72424\n",
      "Epoch 21/120\n",
      "1651/1651 [==============================] - 433s 262ms/step - loss: 0.7408 - f1: 0.3344 - val_loss: 0.7250 - val_f1: 0.3851\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.72424\n",
      "Epoch 22/120\n",
      "1651/1651 [==============================] - 437s 264ms/step - loss: 0.7405 - f1: 0.3343 - val_loss: 0.7220 - val_f1: 0.3866\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.72424 to 0.72203, saving model to ../cache/InceptionV3-8h.h5\n",
      "Epoch 23/120\n",
      "1651/1651 [==============================] - 435s 263ms/step - loss: 0.7413 - f1: 0.3336 - val_loss: 0.7233 - val_f1: 0.3853\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.72203\n",
      "Epoch 24/120\n",
      "1651/1651 [==============================] - 436s 264ms/step - loss: 0.7400 - f1: 0.3356 - val_loss: 0.7237 - val_f1: 0.3853\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.72203\n",
      "Epoch 25/120\n",
      "1651/1651 [==============================] - 434s 263ms/step - loss: 0.7400 - f1: 0.3348 - val_loss: 0.7244 - val_f1: 0.3818\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.72203\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 26/120\n",
      "1651/1651 [==============================] - 434s 263ms/step - loss: 0.7406 - f1: 0.3337 - val_loss: 0.7229 - val_f1: 0.3860\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.72203\n",
      "Epoch 27/120\n",
      "1651/1651 [==============================] - 436s 264ms/step - loss: 0.7398 - f1: 0.3348 - val_loss: 0.7256 - val_f1: 0.3833\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.72203\n",
      "Epoch 28/120\n",
      "1651/1651 [==============================] - 436s 264ms/step - loss: 0.7394 - f1: 0.3347 - val_loss: 0.7240 - val_f1: 0.3853\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.72203\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe56815f5f8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train all layers\n",
    "epochs=120\n",
    "batch_size=16\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.compile(loss=f1_loss,\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=[f1])\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=epochs, \n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ii in tqdm(np.arange(len(train_dataset_info))):\n",
    "#     img1 = cv2.imread(train_dataset_info[ii]['path']+'_red.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img2 = cv2.imread(train_dataset_info[ii]['path']+'_green.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img3 = cv2.imread(train_dataset_info[ii]['path']+'_blue.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img1 = np.stack((img1, img2, img3), -1)\n",
    "#     name = '../cache/RGB/img-{}.png'.format(ii)\n",
    "#     cv2.imwrite(name,img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit = pd.read_csv('../data/sample_submission.csv')\n",
    "# for name in tqdm(submit['Id']):\n",
    "#     path = os.path.join('../data/test/', name)\n",
    "#     img1 = cv2.imread(path+'_red.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img2 = cv2.imread(path+'_green.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img3 = cv2.imread(path+'_blue.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img1 = np.stack((img1, img2, img3), -1)\n",
    "#     name1 = '../cache/RGB/test/img-{}.png'.format(name)\n",
    "#     cv2.imwrite(name1,img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [07:17<00:00, 26.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create submit\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "predicted = []\n",
    "draw_predict = []\n",
    "# model = create_model(\n",
    "#     input_shape=(SIZE,SIZE,3), \n",
    "#     n_out=28)\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = True\n",
    "# model.compile(loss=f1_loss,\n",
    "#             optimizer=Adam(lr=1e-4),\n",
    "#             metrics=[f1])\n",
    "model.load_weights('../cache/InceptionV3-8h.h5')\n",
    "for name in tqdm(submit['Id']):\n",
    "    path = os.path.join('../data/test/', name)\n",
    "    image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "    score_predict = model.predict(image[np.newaxis])[0]\n",
    "    draw_predict.append(score_predict)\n",
    "    label_predict = np.arange(28)[score_predict>=0.2]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted\n",
    "# np.save('../cache/draw_predict_InceptionV3-8.npy', score_predict)\n",
    "# submit.to_csv('../submissions/submit_InceptionV3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('../submissions/sub8h.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/1855095/how-to-create-a-zip-archive-of-a-directory\n",
    "def backup_project_as_zip(project_dir, zip_file):\n",
    "    assert(os.path.isdir(project_dir))\n",
    "    assert(os.path.isdir(os.path.dirname(zip_file)))\n",
    "    shutil.make_archive(zip_file.replace('.zip',''), 'zip', project_dir)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-14 05:17:31.259029\n"
     ]
    }
   ],
   "source": [
    "import datetime, shutil\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "PROJECT_PATH = '/home/watts/lal/Kaggle/kagglehp/scripts_nbs'\n",
    "backup_project_as_zip(PROJECT_PATH, '../cache/code.scripts_nbs.%s.zip'%now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 482k/482k [00:08<00:00, 51.3kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 450 ms, sys: 167 ms, total: 617 ms\n",
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub8h.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub8h.csv     2018-11-13 23:48:00               complete  0.434        None          \r\n",
      "sub8g.csv     2018-11-13 07:17:59               complete  0.389        None          \r\n",
      "sub8c.csv     2018-11-11 14:31:02               complete  0.429        None          \r\n",
      "sub30.csv     2018-11-09 07:02:56               complete  0.033        None          \r\n",
      "sub29.csv     2018-11-08 22:07:11               complete  0.389        None          \r\n",
      "sub28-c.csv   2018-11-08 15:47:08               complete  0.457        None          \r\n",
      "sub28-bb.csv  2018-11-08 15:46:13               complete  0.458        None          \r\n",
      "sub28-b.csv   2018-11-08 15:45:28               complete  0.454        None          \r\n",
      "sub28-a.csv   2018-11-08 15:44:27               complete  0.454        None          \r\n",
      "sub25.csv     2018-11-07 10:45:12               complete  0.421        None          \r\n",
      "sub25.csv     2018-11-07 10:26:20               complete  0.421        None          \r\n",
      "sub8a.csv     2018-11-07 05:53:08               complete  0.425        None          \r\n",
      "sub24.csv     2018-11-07 05:09:26               complete  0.410        None          \r\n",
      "sub23-d.csv   2018-11-07 00:21:06               complete  0.469        None          \r\n",
      "sub23-c.csv   2018-11-06 14:34:37               complete  0.472        None          \r\n",
      "sub23-bb.csv  2018-11-06 14:33:35               complete  0.466        None          \r\n",
      "sub23-b.csv   2018-11-06 14:32:47               complete  0.463        None          \r\n",
      "sub23-a.csv   2018-11-06 14:31:57               complete  0.461        None          \r\n",
      "sub22-a.csv   2018-11-06 14:30:53               complete  0.467        None          \r\n",
      "sub22-b.csv   2018-11-05 16:50:03               complete  0.467        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(60)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.3.8)\r\n",
      "fileName  date                 description  status    publicScore  privateScore  \r\n",
      "--------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub8.csv  2018-10-20 20:08:45               complete  0.422        None          \r\n",
      "sub7.csv  2018-10-20 17:06:09               complete  0.389        None          \r\n",
      "sub5.csv  2018-10-19 18:27:33               complete  0.387        None          \r\n",
      "sub4.csv  2018-10-19 14:45:15               complete  0.411        None          \r\n",
      "sub3.csv  2018-10-19 10:19:26               complete  0.377        None          \r\n",
      "sub2.csv  2018-10-19 08:07:30               complete  0.135        None          \r\n",
      "sub1.csv  2018-10-19 06:28:57               complete  0.374        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(60)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpg",
   "language": "python",
   "name": "hpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/mathormad/inceptionv3-baseline-lb-0-379/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) + (1 - K.mean(f1))\n",
    "\n",
    "# POS_WEIGHT = 1.0  # multiplier for positive targets, needs to be tuned\n",
    "\n",
    "# def weighted_binary_crossentropy(target, output):\n",
    "#     \"\"\"\n",
    "#     Weighted binary crossentropy between an output tensor \n",
    "#     and a target tensor. POS_WEIGHT is used as a multiplier \n",
    "#     for the positive targets.\n",
    "\n",
    "#     Combination of the following functions:\n",
    "#     * keras.losses.binary_crossentropy\n",
    "#     * keras.backend.tensorflow_backend.binary_crossentropy\n",
    "#     * tf.nn.weighted_cross_entropy_with_logits\n",
    "#     \"\"\"\n",
    "#     # transform back to logits\n",
    "# #     _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "#     _epsilon = K.epsilon()\n",
    "#     output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "#     output = tf.log(output / (1 - output))\n",
    "#     # compute weighted loss\n",
    "#     loss = tf.nn.weighted_cross_entropy_with_logits(targets=target,\n",
    "#                                                     logits=output,\n",
    "#                                                     pos_weight=POS_WEIGHT)\n",
    "#     return tf.reduce_mean(loss, axis=-1)\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.python.framework import ops\n",
    "# from functools import reduce\n",
    "\n",
    "# def binaryRound(x):\n",
    "#     \"\"\"\n",
    "#     Rounds a tensor whose values are in [0,1] to a tensor with values in {0, 1},\n",
    "#     using the straight through estimator for the gradient.\n",
    "#     \"\"\"\n",
    "#     g = tf.get_default_graph()\n",
    "\n",
    "#     with ops.name_scope(\"BinaryRound\") as name:\n",
    "#         with g.gradient_override_map({\"Round\": \"Identity\"}):\n",
    "#             return tf.round(x, name=name)\n",
    "\n",
    "#         # For Tensorflow v0.11 and below use:\n",
    "#         #with g.gradient_override_map({\"Floor\": \"Identity\"}):\n",
    "#         #    return tf.round(x, name=name)\n",
    "        \n",
    "# def f1_loss2(y_true, y_pred):\n",
    "#     y_pred = binaryRound(y_pred)\n",
    "#     tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "#     tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "#     fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "#     fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "#     p = tp / (tp + fp + K.epsilon())\n",
    "#     r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "#     f1 = 2*p*r / (p+r+K.epsilon())\n",
    "#     f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "#     return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) + (1-K.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "path_to_train = '../data/train/'\n",
    "data = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_info = []\n",
    "for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "    train_dataset_info.append({\n",
    "        'path':os.path.join(path_to_train, name),\n",
    "        'labels':np.array([int(label) for label in labels])})\n",
    "train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'path': '../data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0', 'labels': array([16,  0])},\n",
       "       {'path': '../data/train/000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', 'labels': array([7, 1, 2, 0])},\n",
       "       {'path': '../data/train/000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([5])},\n",
       "       ...,\n",
       "       {'path': '../data/train/fff189d8-bbab-11e8-b2ba-ac1f6b6435d0', 'labels': array([7])},\n",
       "       {'path': '../data/train/fffdf7e0-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([25,  2, 21])},\n",
       "       {'path': '../data/train/fffe0ffe-bbc0-11e8-b2bb-ac1f6b6435d0', 'labels': array([2, 0])}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 68224.52it/s]\n"
     ]
    }
   ],
   "source": [
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "path_to_test = '../data/test/'\n",
    "test_dataset_info = []\n",
    "for name in tqdm(submit['Id']):\n",
    "    test_dataset_info.append({\n",
    "        'path':os.path.join(path_to_test, name)})\n",
    "test_dataset_info = np.array(test_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rgb_arr = np.memmap('../cache/tmp_rgb_arr', dtype='uint8', mode='r+', \n",
    "#                    shape=(len(train_dataset_info),299,299,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            dataset_info = shuffle(dataset_info)\n",
    "            for start in range(0, len(dataset_info), batch_size):\n",
    "                end = min(start + batch_size, len(dataset_info))\n",
    "                batch_images = []\n",
    "                X_train_batch = dataset_info[start:end]\n",
    "                batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "                for i in range(len(X_train_batch)):\n",
    "                    image = data_generator.load_image(\n",
    "                        X_train_batch[i]['path'], shape) \n",
    "#                     image = data_generator.load_image(\n",
    "#                         i, shape) \n",
    "                    if augument:\n",
    "                        image = data_generator.augment(image)\n",
    "                    batch_images.append(image/255.)\n",
    "                    batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "                yield np.array(batch_images, np.float32), [batch_labels,np.array(batch_images, np.float32)]\n",
    "    def create_train2(dataset_info, batch_size, shape, augument=False):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            dataset_info = shuffle(dataset_info)\n",
    "            for start in range(0, len(dataset_info), batch_size):\n",
    "                end = min(start + batch_size, len(dataset_info))\n",
    "                batch_images = []\n",
    "                X_train_batch = dataset_info[start:end]\n",
    "                batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "                for i in range(len(X_train_batch)):\n",
    "                    image = data_generator.load_image(\n",
    "                        X_train_batch[i]['path'], shape) \n",
    "#                     image = data_generator.load_image(\n",
    "#                         i, shape) \n",
    "                    \n",
    "                    batch_images.append(image/255.)\n",
    "                    \n",
    "                yield np.array(batch_images, np.float32), None\n",
    "    def create_test(dataset_info, batch_size, shape, augument=False):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            dataset_info = shuffle(dataset_info)\n",
    "            for start in range(0, len(dataset_info), batch_size):\n",
    "                end = min(start + batch_size, len(dataset_info))\n",
    "                batch_images = []\n",
    "                X_test_batch = dataset_info[start:end]\n",
    "                batch_labels = np.zeros((len(X_test_batch), 28))\n",
    "                for i in range(len(X_test_batch)):\n",
    "                    image = data_generator.load_image(\n",
    "                        X_test_batch[i]['path'], shape) \n",
    "#                     image = data_generator.load_image(\n",
    "#                         i, shape) \n",
    "                    \n",
    "                    batch_images.append(image/255.)\n",
    "                    \n",
    "                yield np.array(batch_images, np.float32), None\n",
    "    def load_image(path, shape):\n",
    "        img1 = cv2.imread(path+'_red.png', cv2.IMREAD_GRAYSCALE)\n",
    "        img2 = cv2.imread(path+'_green.png', cv2.IMREAD_GRAYSCALE)\n",
    "        img3 = cv2.imread(path+'_blue.png', cv2.IMREAD_GRAYSCALE)\n",
    "        image = np.stack((img1,img2,img3), -1)\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    def load_image3(idx, shape):\n",
    "#         print(idx)\n",
    "        name = '../cache/RGB/img-{}.png'.format(idx)\n",
    "        image = cv2.imread(name)\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    def load_image2(path, shape):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image = np.stack((\n",
    "            np.array(image_red_ch),\n",
    "            np.array(image_green_ch), \n",
    "            np.array(image_blue_ch)), -1)\n",
    "        w, h = 512, 512\n",
    "#         zero_data = np.zeros((h, w), dtype=np.uint8)\n",
    "#         image2 = np.stack((\n",
    "#             np.array(image_yellow_ch),\n",
    "#             zero_data, zero_data), -1)\n",
    "#         print(image1.shape, image2.shape)\n",
    "#         image = np.vstack((image1, image2))\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    \n",
    "    def augment2(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=0),\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "            ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n",
    "    def augment(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                    iaa.Fliplr(0.5), # horizontal flips\n",
    "                    iaa.Affine(rotate=0),\n",
    "                    iaa.Affine(rotate=90),\n",
    "                    iaa.Affine(rotate=180),\n",
    "                    iaa.Affine(rotate=270),\n",
    "                    iaa.Flipud(0.5),\n",
    "                    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "                    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "                    # But we only blur about 50% of all images.\n",
    "                    iaa.Sometimes(0.5,\n",
    "                        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "                    ),\n",
    "                    # Strengthen or weaken the contrast in each image.\n",
    "                    iaa.ContrastNormalization((0.75, 1.5)),\n",
    "                    # Add gaussian noise.\n",
    "                    # For 50% of all images, we sample the noise once per pixel.\n",
    "                    # For the other 50% of all images, we sample the noise per pixel AND\n",
    "                    # channel. This can change the color (not only brightness) of the\n",
    "                    # pixels.\n",
    "                    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "                    # Make some images brighter and some darker.\n",
    "                    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "                    # which can end up changing the color of the images.\n",
    "                    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "                    # Apply affine transformations to each image.\n",
    "                    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "                    iaa.Affine(\n",
    "                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                        rotate=(-180, 180),\n",
    "                        shear=(-8, 8)\n",
    "                    )\n",
    "                ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras as k\n",
    "print(k.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D\n",
    "from keras.layers import UpSampling2D, Lambda, Reshape, Conv2DTranspose\n",
    "from keras.layers import  MaxPooling2D, Concatenate, ReLU, LeakyReLU, GlobalAveragePooling2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Brian decoder\n",
    "def decoder_block(x,blocks=6,start_filters=512):\n",
    "    for i in range(blocks):\n",
    "        x = Conv2D(start_filters//(2**i),(3,3), activation='relu', padding='same')(x)\n",
    "        x = Conv2D(start_filters//(2**i),(3,3), activation='relu', padding='same')(x)\n",
    "        x = UpSampling2D()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(32768), Dimension(32768), Dimension(16)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = decoder_block(x = Input(shape=(512,512,3)))\n",
    "model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_shape = (512, )\n",
    "intermediate_dim = 128\n",
    "batch_size = 16\n",
    "latent_dim = 2\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling1(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
    "    mean=0., stddev=1.)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "def create_encoder(input_shape):\n",
    "    inputs = Input(shape=input_shape, name='encoder_input')\n",
    "    x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "    # instantiate encoder model\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    \n",
    "    return encoder\n",
    "def create_decoder(latent_dim):\n",
    "    # build decoder model\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "    outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "    # instantiate decoder model\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "    return decoder\n",
    "\n",
    "def create_vae():\n",
    "    encoder = create_encoder(input_shape)\n",
    "    decoder = create_decoder(latent_dim)\n",
    "    # instantiate VAE model\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, outputs, name='vae_mlp')\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def sampling(args):\n",
    "#     z_mean, z_log_var = args\n",
    "#     epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),mean=0., stddev=1.)\n",
    "#     return z_mean + K.exp(z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2)\n",
      "(?, 2)\n",
      "(?, 4194304)\n",
      "(None, 256, 256, 64)\n",
      "(?, 256, 256, 64)\n",
      "(?, ?, ?, 32)\n",
      "(?, ?, ?, 3)\n"
     ]
    }
   ],
   "source": [
    "img_shape = (512, 512, 3)\n",
    "batch_size = 16\n",
    "latent_dim = 2\n",
    "\n",
    "\n",
    "input_img = Input(shape=img_shape)\n",
    "\n",
    "x = Conv2D(32, 3,padding='same', activation='relu')(input_img)\n",
    "x = Conv2D(64, 3,padding='same', activation='relu',strides=(2, 2))(x)\n",
    "x = Conv2D(64, 3,padding='same', activation='relu')(x)\n",
    "x = Conv2D(64, 3,padding='same', activation='relu')(x)\n",
    "shape_before_flattening = K.int_shape(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(x)\n",
    "z_log_var = Dense(latent_dim)(x)\n",
    "z = Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "print(z.shape)\n",
    "# This is the input where we will feed `z`.\n",
    "decoder_input = Input(K.int_shape(z)[1:])\n",
    "\n",
    "print(decoder_input.shape)\n",
    "# Upsample to the correct number of units\n",
    "x = Dense(np.prod(shape_before_flattening[1:]),\n",
    "             activation='relu')(decoder_input)\n",
    "\n",
    "print(x.shape)\n",
    "# Reshape into an image of the same shape as before our last `Flatten` layer\n",
    "x = Reshape(shape_before_flattening[1:])(x)\n",
    "\n",
    "print(shape_before_flattening)\n",
    "print(x.shape)\n",
    "# We then apply then reverse operation to the initial\n",
    "# stack of convolution layers: a `Conv2DTranspose` layers\n",
    "# with corresponding parameters.\n",
    "x = Conv2DTranspose(32, 3,\n",
    "                           padding='same', activation='relu',\n",
    "                           strides=(2, 2))(x)\n",
    "\n",
    "print(x.shape)\n",
    "x = Conv2D(3, 3,\n",
    "                  padding='same', activation='sigmoid')(x)\n",
    "# We end up with a feature map of the same size as the original input.\n",
    "print(x.shape)\n",
    "# This is our decoder model.\n",
    "decoder = Model(decoder_input, x)\n",
    "\n",
    "# We then apply it to `z` to recover the decoded `z`.\n",
    "z_decoded = decoder(z)\n",
    "# return z_decoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomVariationalLayer(keras.layers.Layer):\n",
    "\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        kl_loss = -5e-4 * K.mean(\n",
    "            1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We don't use this output.\n",
    "        return x\n",
    "\n",
    "# We call our custom layer on the input and the decoded output,\n",
    "# to obtain the final model output.\n",
    "y = CustomVariationalLayer()([input_img, z_decoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vae = Model(input_img, y)\n",
    "# vae.compile(optimizer='rmsprop', loss=None)\n",
    "# vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_train = len(train_dataset_info) \n",
    "l_test = 11702"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42774"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_all = l_train + l_test\n",
    "len_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into train, valid\n",
    "# indexes = np.arange(train_dataset_info.shape[0])\n",
    "# np.random.shuffle(indexes)\n",
    "# # train_indexes, valid_indexes = train_test_split(indexes, test_size=0.15, random_state=8)\n",
    "# train_indexes = indexes\n",
    "# valid_indexes = np.arange(l_test)\n",
    "# # create train and valid datagens\n",
    "# train_generator = data_generator.create_train2(\n",
    "#     train_dataset_info[train_indexes], 32, (SIZE,SIZE,3), augument=True)\n",
    "# validation_generator = data_generator.create_test(\n",
    "#     test_dataset_info[valid_indexes], 16, (SIZE,SIZE,3), augument=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch_size=32\n",
    "# vae.fit_generator(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=np.ceil(float(len(valid_indexes)) / 16.),\n",
    "#     epochs=3, \n",
    "#     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vae.save_weights('../cache/vae-34-1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vae.load_weights('../cache/vae-33-1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.stats import norm\n",
    "\n",
    "# # Display a 2D manifold of the digits\n",
    "# n = 15  # figure with 15x15 digits\n",
    "# digit_size = 512\n",
    "# figure = np.zeros((n, digit_size, digit_size, 3))\n",
    "# # Linearly spaced coordinates on the unit square were transformed\n",
    "# # through the inverse CDF (ppf) of the Gaussian\n",
    "# # to produce values of the latent variables z,\n",
    "# # since the prior of the latent space is Gaussian\n",
    "# grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "# grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "# batch_size = 8\n",
    "# for i, yi in enumerate(grid_x):\n",
    "#     for j, xi in enumerate(grid_y):\n",
    "#         z_sample = np.array([[xi, yi]])\n",
    "#         z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "#         x_decoded = decoder.predict(z_sample, batch_size=batch_size)\n",
    "# #         print(x_decoded.shape)\n",
    "#         digit = x_decoded[0].reshape(digit_size, digit_size, 3)\n",
    "# #         digit = np.squeeze(x_decoded)\n",
    "# #         print(digit.shape)\n",
    "# #         figure[i * digit_size: (i + 1) * digit_size,\n",
    "# #                j * digit_size: (j + 1) * digit_size] = digit * 255\n",
    "#         figure[i+j*8] = digit*255\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(figure)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def global_average_pooling(x):\n",
    "    return K.mean(x, axis = (2, 3))\n",
    "    \n",
    "def global_average_pooling_shape(input_shape):\n",
    "    return input_shape[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_sample(z_sample):\n",
    "#     z_sample = z_sample.reshape(1,2)\n",
    "#     x_decoded = decoder.predict(K.squeeze(z_sample, axis=-1), batch_size=1)\n",
    "    x_decoded = decoder(z_sample)\n",
    "    return x_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 16\n",
    "kernel_size = 3\n",
    "layer_filters = [32, 64, 128, 256, 512]\n",
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=input_shape)\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = base_model(bn)\n",
    "    \n",
    "    shape = K.int_shape(x)\n",
    "#     x = Lambda(global_average_pooling, output_shape=global_average_pooling_shape)(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x1 = x\n",
    "    \n",
    "    latent_inputs = Dense(latent_dim, name='latent_vector')(x)\n",
    "    \n",
    "#     latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
    "    x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n",
    "#     x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "    x = Reshape((16, 16, 1568))(x)\n",
    "#     x = latent_inputs\n",
    "#     print(shape)\n",
    "#     print(x.shape)\n",
    "    # Stack of Transposed Conv2D blocks\n",
    "    # Notes:\n",
    "    # 1) Use Batch Normalization before ReLU on deep networks\n",
    "    # 2) Use UpSampling2D as alternative to strides>1\n",
    "    # - faster but not as good as strides>1\n",
    "    for filters in layer_filters[::-1]:\n",
    "        x = Conv2DTranspose(filters=filters,\n",
    "                            kernel_size=kernel_size,\n",
    "                            strides=2,\n",
    "                            activation='relu',\n",
    "                            padding='same')(x)\n",
    "\n",
    "    x = Conv2DTranspose(filters=3,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding='same')(x)\n",
    "\n",
    "    img_out = Activation('sigmoid', name='decoder_output')(x)\n",
    "   \n",
    "#     x = Flatten()(x1)\n",
    "    x = Dense(512, activation='relu')(x1)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    img_out = Dense(3, activation='relu',name='img_out')(img_out)\n",
    "    output = Dense(n_out, activation='sigmoid', name='output')(x)\n",
    "    model = Model(input_tensor, [output, img_out])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "401408/(16*16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[4194304,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: dense_1/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=8913512, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dense_1/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'dense_1/random_uniform/RandomUniform', defined at:\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/asyncio/base_events.py\", line 1426, in _run_once\n    handle._run()\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/asyncio/events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-20-b65919cc0c6f>\", line 14, in <module>\n    x = Dense(32, activation='relu')(x)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 431, in __call__\n    self.build(unpack_singleton(input_shapes))\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/layers/core.py\", line 866, in build\n    constraint=self.kernel_constraint)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 249, in add_weight\n    weight = K.variable(initializer(shape),\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/initializers.py\", line 218, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 4139, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\", line 242, in random_uniform\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 674, in random_uniform\n    name=name)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[4194304,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: dense_1/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=8913512, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dense_1/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[4194304,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: dense_1/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=8913512, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dense_1/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-67c9ddf896d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model = create_model(\n\u001b[0;32m----> 2\u001b[0;31m     input_shape=(SIZE,SIZE,3), n_out=28)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-bd8abfa8c357>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(input_shape, n_out)\u001b[0m\n\u001b[1;32m      6\u001b[0m     base_model = InceptionV3(include_top=False,\n\u001b[1;32m      7\u001b[0m                    \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                    input_shape=input_shape)\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/applications/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utils'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/applications/inception_v3.py\u001b[0m in \u001b[0;36mInceptionV3\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mkeras_modules_injection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minception_v3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/keras_applications/inception_v3.py\u001b[0m in \u001b[0;36mInceptionV3\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mchannel_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/keras_applications/inception_v3.py\u001b[0m in \u001b[0;36mconv2d_bn\u001b[0;34m(x, filters, num_row, num_col, padding, strides, name)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         name=conv_name)(x)\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbn_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/layers/normalization.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    183\u001b[0m         normed_training, mean, variance = K.normalize_batch_in_training(\n\u001b[1;32m    184\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_axes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             epsilon=self.epsilon)\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'cntk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mnormalize_batch_in_training\u001b[0;34m(x, gamma, beta, reduction_axes, epsilon)\u001b[0m\n\u001b[1;32m   1856\u001b[0m     \"\"\"\n\u001b[1;32m   1857\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction_axes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1858\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_has_nchw_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction_axes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1859\u001b[0m             return _broadcast_normalize_batch_in_training(x, gamma, beta,\n\u001b[1;32m   1860\u001b[0m                                                           \u001b[0mreduction_axes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_has_nchw_support\u001b[0;34m()\u001b[0m\n\u001b[1;32m    290\u001b[0m     \"\"\"\n\u001b[1;32m    291\u001b[0m     \u001b[0mexplicitly_on_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_is_current_explicit_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0mgpus_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_available_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mexplicitly_on_cpu\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpus_available\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_get_available_gpus\u001b[0;34m()\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_LOCAL_DEVICES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_LOCAL_DEVICES\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0m_LOCAL_DEVICES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_LOCAL_DEVICES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'GPU'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[4194304,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: dense_1/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=8913512, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dense_1/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'dense_1/random_uniform/RandomUniform', defined at:\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/asyncio/base_events.py\", line 1426, in _run_once\n    handle._run()\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/asyncio/events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-20-b65919cc0c6f>\", line 14, in <module>\n    x = Dense(32, activation='relu')(x)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 431, in __call__\n    self.build(unpack_singleton(input_shapes))\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/layers/core.py\", line 866, in build\n    constraint=self.kernel_constraint)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 249, in add_weight\n    weight = K.variable(initializer(shape),\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/initializers.py\", line 218, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 4139, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\", line 242, in random_uniform\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 674, in random_uniform\n    name=name)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[4194304,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: dense_1/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=8913512, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dense_1/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3), n_out=28)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epochs = 10; batch_size = 16\n",
    "checkpoint = ModelCheckpoint('../cache/iv3-37.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
    "                                   verbose=1, mode='auto', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=6)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# split data into train, valid\n",
    "indexes = np.arange(train_dataset_info.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "train_indexes, valid_indexes = train_test_split(indexes, test_size=0.15, random_state=8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# warm up model\n",
    "# model = create_model(\n",
    "#     input_shape=(SIZE,SIZE,3), \n",
    "#     n_out=28)\n",
    "model.summary()\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "model.layers[-1].trainable = True\n",
    "model.layers[-2].trainable = True\n",
    "model.layers[-3].trainable = True\n",
    "model.layers[-4].trainable = True\n",
    "model.layers[-5].trainable = True\n",
    "model.layers[-6].trainable = True\n",
    "model.layers[-7].trainable = True\n",
    "model.layers[-8].trainable = True\n",
    "model.layers[-9].trainable = True\n",
    "model.layers[-10].trainable = True\n",
    "model.layers[-11].trainable = True\n",
    "model.layers[-12].trainable = True\n",
    "model.layers[-13].trainable = True\n",
    "model.layers[-14].trainable = True\n",
    "# model.layers[-15].trainable = True\n",
    "# model.layers[-16].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "def on_train_begin(self, logs={}):\n",
    "    self.val_f1s = []\n",
    "    self.val_recalls = []\n",
    "    self.val_precisions = []\n",
    "\n",
    "def on_epoch_end(self, epoch, logs={}):\n",
    "    val_predict = (np.asarray(self.model.predict(self.model.validation_data[0]))).round()\n",
    "    val_targ = self.model.validation_data[1]\n",
    "    _val_f1 = f1_score(val_targ, val_predict)\n",
    "    _val_recall = recall_score(val_targ, val_predict)\n",
    "    _val_precision = precision_score(val_targ, val_predict)\n",
    "    self.val_f1s.append(_val_f1)\n",
    "    self.val_recalls.append(_val_recall)\n",
    "    self.val_precisions.append(_val_precision)\n",
    "    print('— val_f1: %f — val_precision: %f — val_recall %f' %(_val_f1, _val_precision, _val_recall))\n",
    "    return\n",
    " \n",
    "metrics = Metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=[f1_loss, 'mse'], \n",
    "    optimizer=Adam(1e-03),\n",
    "    loss_weights = {\"output\": 1.0, \"img_out\": 10.0 }, \n",
    "    metrics=[f1, metrics])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26411 4661\n"
     ]
    }
   ],
   "source": [
    "print(len(train_indexes), len(valid_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "\n",
    "# create train and valid datagens\n",
    "train_generator = data_generator.create_train(\n",
    "    train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "validation_generator = data_generator.create_train(\n",
    "    train_dataset_info[valid_indexes], 16, (SIZE,SIZE,3), augument=False)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=2, \n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=120\n",
    "\n",
    "# batch_size=8\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.compile(loss=[f1_loss, 'mse'],\n",
    "              optimizer=Adam(lr=1e-4),\n",
    "              loss_weights = {\"output\": 1.0, \"img_out\": 10.0 }, \n",
    "              metrics=[f1, metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "6603/6603 [==============================] - 2838s 430ms/step - loss: 1.1880 - output_loss: 0.9539 - img_out_loss: 0.0234 - output_f1: 0.1330 - img_out_f1: 0.0000e+00 - val_loss: 0.9385 - val_output_loss: 0.6994 - val_img_out_loss: 0.0239 - val_output_f1: 0.4061 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.93850, saving model to ../cache/iv3-37.h5\n",
      "Epoch 2/120\n",
      "6603/6603 [==============================] - 2648s 401ms/step - loss: 1.1920 - output_loss: 0.9579 - img_out_loss: 0.0234 - output_f1: 0.1313 - img_out_f1: 0.0000e+00 - val_loss: 0.9531 - val_output_loss: 0.7139 - val_img_out_loss: 0.0239 - val_output_f1: 0.3915 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.93850\n",
      "Epoch 3/120\n",
      "6603/6603 [==============================] - 2649s 401ms/step - loss: 1.1883 - output_loss: 0.9543 - img_out_loss: 0.0234 - output_f1: 0.1331 - img_out_f1: 0.0000e+00 - val_loss: 0.9616 - val_output_loss: 0.7225 - val_img_out_loss: 0.0239 - val_output_f1: 0.3853 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.93850\n",
      "Epoch 4/120\n",
      "6603/6603 [==============================] - 2646s 401ms/step - loss: 1.1872 - output_loss: 0.9532 - img_out_loss: 0.0234 - output_f1: 0.1334 - img_out_f1: 0.0000e+00 - val_loss: 0.9646 - val_output_loss: 0.7255 - val_img_out_loss: 0.0239 - val_output_f1: 0.3808 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.93850\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 5/120\n",
      "6603/6603 [==============================] - 2586s 392ms/step - loss: 1.1731 - output_loss: 0.9388 - img_out_loss: 0.0234 - output_f1: 0.1393 - img_out_f1: 0.0000e+00 - val_loss: 0.9181 - val_output_loss: 0.6791 - val_img_out_loss: 0.0239 - val_output_f1: 0.4148 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.93850 to 0.91805, saving model to ../cache/iv3-37.h5\n",
      "Epoch 6/120\n",
      "6603/6603 [==============================] - 2556s 387ms/step - loss: 1.1678 - output_loss: 0.9340 - img_out_loss: 0.0234 - output_f1: 0.1409 - img_out_f1: 0.0000e+00 - val_loss: 0.9153 - val_output_loss: 0.6761 - val_img_out_loss: 0.0239 - val_output_f1: 0.4163 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.91805 to 0.91528, saving model to ../cache/iv3-37.h5\n",
      "Epoch 7/120\n",
      "6603/6603 [==============================] - 2550s 386ms/step - loss: 1.1660 - output_loss: 0.9321 - img_out_loss: 0.0234 - output_f1: 0.1418 - img_out_f1: 0.0000e+00 - val_loss: 0.9135 - val_output_loss: 0.6743 - val_img_out_loss: 0.0239 - val_output_f1: 0.4179 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.91528 to 0.91354, saving model to ../cache/iv3-37.h5\n",
      "Epoch 8/120\n",
      "6603/6603 [==============================] - 2555s 387ms/step - loss: 1.1636 - output_loss: 0.9296 - img_out_loss: 0.0234 - output_f1: 0.1428 - img_out_f1: 0.0000e+00 - val_loss: 0.9139 - val_output_loss: 0.6747 - val_img_out_loss: 0.0239 - val_output_f1: 0.4174 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.91354\n",
      "Epoch 9/120\n",
      "6603/6603 [==============================] - 2553s 387ms/step - loss: 1.1620 - output_loss: 0.9281 - img_out_loss: 0.0234 - output_f1: 0.1430 - img_out_f1: 0.0000e+00 - val_loss: 0.9061 - val_output_loss: 0.6671 - val_img_out_loss: 0.0239 - val_output_f1: 0.4248 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.91354 to 0.90606, saving model to ../cache/iv3-37.h5\n",
      "Epoch 10/120\n",
      "6603/6603 [==============================] - 2549s 386ms/step - loss: 1.1605 - output_loss: 0.9262 - img_out_loss: 0.0234 - output_f1: 0.1437 - img_out_f1: 0.0000e+00 - val_loss: 0.9042 - val_output_loss: 0.6649 - val_img_out_loss: 0.0239 - val_output_f1: 0.4240 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.90606 to 0.90422, saving model to ../cache/iv3-37.h5\n",
      "Epoch 11/120\n",
      "6603/6603 [==============================] - 2549s 386ms/step - loss: 1.1598 - output_loss: 0.9259 - img_out_loss: 0.0234 - output_f1: 0.1440 - img_out_f1: 0.0000e+00 - val_loss: 0.9049 - val_output_loss: 0.6661 - val_img_out_loss: 0.0239 - val_output_f1: 0.4235 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.90422\n",
      "Epoch 12/120\n",
      "6603/6603 [==============================] - 2559s 388ms/step - loss: 1.1576 - output_loss: 0.9236 - img_out_loss: 0.0234 - output_f1: 0.1450 - img_out_f1: 0.0000e+00 - val_loss: 0.9054 - val_output_loss: 0.6662 - val_img_out_loss: 0.0239 - val_output_f1: 0.4224 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.90422\n",
      "Epoch 13/120\n",
      "6603/6603 [==============================] - 2553s 387ms/step - loss: 1.1569 - output_loss: 0.9230 - img_out_loss: 0.0234 - output_f1: 0.1453 - img_out_f1: 0.0000e+00 - val_loss: 0.9047 - val_output_loss: 0.6656 - val_img_out_loss: 0.0239 - val_output_f1: 0.4229 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.90422\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 14/120\n",
      "6603/6603 [==============================] - 2551s 386ms/step - loss: 1.1548 - output_loss: 0.9206 - img_out_loss: 0.0234 - output_f1: 0.1462 - img_out_f1: 0.0000e+00 - val_loss: 0.9010 - val_output_loss: 0.6615 - val_img_out_loss: 0.0239 - val_output_f1: 0.4252 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.90422 to 0.90097, saving model to ../cache/iv3-37.h5\n",
      "Epoch 15/120\n",
      "6603/6603 [==============================] - 2541s 385ms/step - loss: 1.1546 - output_loss: 0.9203 - img_out_loss: 0.0234 - output_f1: 0.1461 - img_out_f1: 0.0000e+00 - val_loss: 0.9007 - val_output_loss: 0.6617 - val_img_out_loss: 0.0239 - val_output_f1: 0.4258 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.90097 to 0.90065, saving model to ../cache/iv3-37.h5\n",
      "Epoch 16/120\n",
      "6603/6603 [==============================] - 2547s 386ms/step - loss: 1.1551 - output_loss: 0.9208 - img_out_loss: 0.0234 - output_f1: 0.1458 - img_out_f1: 0.0000e+00 - val_loss: 0.9004 - val_output_loss: 0.6616 - val_img_out_loss: 0.0239 - val_output_f1: 0.4252 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.90065 to 0.90037, saving model to ../cache/iv3-37.h5\n",
      "Epoch 17/120\n",
      "6603/6603 [==============================] - 2542s 385ms/step - loss: 1.1542 - output_loss: 0.9202 - img_out_loss: 0.0234 - output_f1: 0.1463 - img_out_f1: 0.0000e+00 - val_loss: 0.8997 - val_output_loss: 0.6603 - val_img_out_loss: 0.0239 - val_output_f1: 0.4257 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.90037 to 0.89967, saving model to ../cache/iv3-37.h5\n",
      "Epoch 18/120\n",
      "6603/6603 [==============================] - 2538s 384ms/step - loss: 1.1544 - output_loss: 0.9201 - img_out_loss: 0.0234 - output_f1: 0.1460 - img_out_f1: 0.0000e+00 - val_loss: 0.8991 - val_output_loss: 0.6602 - val_img_out_loss: 0.0239 - val_output_f1: 0.4268 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.89967 to 0.89913, saving model to ../cache/iv3-37.h5\n",
      "Epoch 19/120\n",
      "6603/6603 [==============================] - 2541s 385ms/step - loss: 1.1544 - output_loss: 0.9202 - img_out_loss: 0.0234 - output_f1: 0.1463 - img_out_f1: 0.0000e+00 - val_loss: 0.8990 - val_output_loss: 0.6602 - val_img_out_loss: 0.0239 - val_output_f1: 0.4266 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.89913 to 0.89896, saving model to ../cache/iv3-37.h5\n",
      "Epoch 20/120\n",
      "6603/6603 [==============================] - 2550s 386ms/step - loss: 1.1535 - output_loss: 0.9194 - img_out_loss: 0.0234 - output_f1: 0.1467 - img_out_f1: 0.0000e+00 - val_loss: 0.9001 - val_output_loss: 0.6607 - val_img_out_loss: 0.0239 - val_output_f1: 0.4255 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.89896\n",
      "Epoch 21/120\n",
      "6603/6603 [==============================] - 2543s 385ms/step - loss: 1.1533 - output_loss: 0.9195 - img_out_loss: 0.0234 - output_f1: 0.1464 - img_out_f1: 0.0000e+00 - val_loss: 0.8994 - val_output_loss: 0.6602 - val_img_out_loss: 0.0239 - val_output_f1: 0.4261 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.89896\n",
      "Epoch 22/120\n",
      "6603/6603 [==============================] - 2541s 385ms/step - loss: 1.1539 - output_loss: 0.9197 - img_out_loss: 0.0234 - output_f1: 0.1465 - img_out_f1: 0.0000e+00 - val_loss: 0.8997 - val_output_loss: 0.6609 - val_img_out_loss: 0.0239 - val_output_f1: 0.4261 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.89896\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 23/120\n",
      "6603/6603 [==============================] - 2543s 385ms/step - loss: 1.1527 - output_loss: 0.9185 - img_out_loss: 0.0234 - output_f1: 0.1471 - img_out_f1: 0.0000e+00 - val_loss: 0.8999 - val_output_loss: 0.6605 - val_img_out_loss: 0.0239 - val_output_f1: 0.4261 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.89896\n",
      "Epoch 24/120\n",
      "6603/6603 [==============================] - 2545s 385ms/step - loss: 1.1533 - output_loss: 0.9191 - img_out_loss: 0.0234 - output_f1: 0.1470 - img_out_f1: 0.0000e+00 - val_loss: 0.9024 - val_output_loss: 0.6635 - val_img_out_loss: 0.0239 - val_output_f1: 0.4226 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.89896\n",
      "Epoch 25/120\n",
      "6603/6603 [==============================] - 2541s 385ms/step - loss: 1.1542 - output_loss: 0.9200 - img_out_loss: 0.0234 - output_f1: 0.1461 - img_out_f1: 0.0000e+00 - val_loss: 0.9012 - val_output_loss: 0.6618 - val_img_out_loss: 0.0239 - val_output_f1: 0.4240 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.89896\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f40e232a4e0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train all layers\n",
    "batch_size=4\n",
    "\n",
    "train_generator = data_generator.create_train(\n",
    "    train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "validation_generator = data_generator.create_train(\n",
    "    train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)\n",
    "model.load_weights('../cache/iv3-37.h5')\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=epochs, \n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for ii in tqdm(np.arange(len(train_dataset_info))):\n",
    "#     img1 = cv2.imread(train_dataset_info[ii]['path']+'_red.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img2 = cv2.imread(train_dataset_info[ii]['path']+'_green.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img3 = cv2.imread(train_dataset_info[ii]['path']+'_blue.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img1 = np.stack((img1, img2, img3), -1)\n",
    "#     name = '../cache/RGB/img-{}.png'.format(ii)\n",
    "#     cv2.imwrite(name,img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# submit = pd.read_csv('../data/sample_submission.csv')\n",
    "# for name in tqdm(submit['Id']):\n",
    "#     path = os.path.join('../data/test/', name)\n",
    "#     img1 = cv2.imread(path+'_red.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img2 = cv2.imread(path+'_green.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img3 = cv2.imread(path+'_blue.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img1 = np.stack((img1, img2, img3), -1)\n",
    "#     name1 = '../cache/RGB/test/img-{}.png'.format(name)\n",
    "#     cv2.imwrite(name1,img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [17:38<00:00, 11.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create submit\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "predicted = []\n",
    "draw_predict = []\n",
    "# model = create_model(\n",
    "#     input_shape=(SIZE,SIZE,3), \n",
    "#     n_out=28)\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = True\n",
    "# model.compile(loss=f1_loss,\n",
    "#             optimizer=Adam(lr=1e-4),\n",
    "#             metrics=[f1])\n",
    "model.load_weights('../cache/iv3-37.h5')\n",
    "for name in tqdm(submit['Id']):\n",
    "    path = os.path.join('../data/test/', name)\n",
    "    image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "    score_predict = model.predict(image[np.newaxis])[0]\n",
    "    draw_predict.append(score_predict)\n",
    "    label_predict = np.arange(28)[score_predict[0]>=0.2]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted\n",
    "# np.save('../cache/draw_predict_InceptionV3-8.npy', score_predict)\n",
    "# submit.to_csv('../submissions/submit_InceptionV3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv('../submissions/sub37-a.csv', index=False)\n",
    "\n",
    "        \n",
    "# sub37-c.csv      2018-11-26 21:04:50               complete  0.437        None          \n",
    "# sub37-b.csv      2018-11-26 21:04:21               complete  0.457        None          \n",
    "# sub37-a.csv      2018-11-26 21:03:26               complete  0.464        None          \n",
    "# sub35-max.csv    2018-11-26 00:58:38               complete  0.473        None          \n",
    "# sub34b-max.csv   2018-11-24 19:03:59               complete  0.459        None          \n",
    "# sub34a-max.csv   2018-11-24 18:50:22               complete  0.469        None          \n",
    "# sub34-max.csv    2018-11-24 17:27:36               complete  0.473        None          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 52496.99it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for score_predict in tqdm(draw_predict):\n",
    "    \n",
    "    label_predict = np.arange(28)[score_predict[0]>=0.25]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub37-b.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 46732.37it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for score_predict in tqdm(draw_predict):\n",
    "    \n",
    "    label_predict = np.arange(28)[score_predict[0]>=0.5]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub37-c.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 42185.08it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for score_predict in tqdm(draw_predict):\n",
    "    \n",
    "    label_predict = np.arange(28)[score_predict[0]>=0.45]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub37-d.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 42386.68it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for score_predict in tqdm(draw_predict):\n",
    "    \n",
    "    label_predict = np.arange(28)[score_predict[0]>=0.4]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub37-e.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 42451.94it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for score_predict in tqdm(draw_predict):\n",
    "    \n",
    "    label_predict = np.arange(28)[score_predict[0]>=0.35]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub37-f.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/1855095/how-to-create-a-zip-archive-of-a-directory\n",
    "def backup_project_as_zip(project_dir, zip_file):\n",
    "    assert(os.path.isdir(project_dir))\n",
    "    assert(os.path.isdir(os.path.dirname(zip_file)))\n",
    "    shutil.make_archive(zip_file.replace('.zip',''), 'zip', project_dir)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-19 12:33:39.384598\n"
     ]
    }
   ],
   "source": [
    "import datetime, shutil\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "PROJECT_PATH = '/home/watts/lal/Kaggle/kagglehp/scripts_nbs'\n",
    "backup_project_as_zip(PROJECT_PATH, '../cache/code.scripts_nbs.%s.zip'%now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 480k/480k [00:08<00:00, 51.3kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 332 ms, sys: 134 ms, total: 467 ms\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# !kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub8i1.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub8i.csv     2018-11-14 04:28:35               complete  0.443        None          \r\n",
      "sub8h.csv     2018-11-13 23:48:00               complete  0.434        None          \r\n",
      "sub8g.csv     2018-11-13 07:17:59               complete  0.389        None          \r\n",
      "sub8c.csv     2018-11-11 14:31:02               complete  0.429        None          \r\n",
      "sub30.csv     2018-11-09 07:02:56               complete  0.033        None          \r\n",
      "sub29.csv     2018-11-08 22:07:11               complete  0.389        None          \r\n",
      "sub28-c.csv   2018-11-08 15:47:08               complete  0.457        None          \r\n",
      "sub28-bb.csv  2018-11-08 15:46:13               complete  0.458        None          \r\n",
      "sub28-b.csv   2018-11-08 15:45:28               complete  0.454        None          \r\n",
      "sub28-a.csv   2018-11-08 15:44:27               complete  0.454        None          \r\n",
      "sub25.csv     2018-11-07 10:45:12               complete  0.421        None          \r\n",
      "sub25.csv     2018-11-07 10:26:20               complete  0.421        None          \r\n",
      "sub8a.csv     2018-11-07 05:53:08               complete  0.425        None          \r\n",
      "sub24.csv     2018-11-07 05:09:26               complete  0.410        None          \r\n",
      "sub23-d.csv   2018-11-07 00:21:06               complete  0.469        None          \r\n",
      "sub23-c.csv   2018-11-06 14:34:37               complete  0.472        None          \r\n",
      "sub23-bb.csv  2018-11-06 14:33:35               complete  0.466        None          \r\n",
      "sub23-b.csv   2018-11-06 14:32:47               complete  0.463        None          \r\n",
      "sub23-a.csv   2018-11-06 14:31:57               complete  0.461        None          \r\n",
      "sub22-a.csv   2018-11-06 14:30:53               complete  0.467        None          \r\n"
     ]
    }
   ],
   "source": [
    "# from time import sleep\n",
    "# sleep(60)\n",
    "# !kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.3.8)\r\n",
      "fileName  date                 description  status    publicScore  privateScore  \r\n",
      "--------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub8.csv  2018-10-20 20:08:45               complete  0.422        None          \r\n",
      "sub7.csv  2018-10-20 17:06:09               complete  0.389        None          \r\n",
      "sub5.csv  2018-10-19 18:27:33               complete  0.387        None          \r\n",
      "sub4.csv  2018-10-19 14:45:15               complete  0.411        None          \r\n",
      "sub3.csv  2018-10-19 10:19:26               complete  0.377        None          \r\n",
      "sub2.csv  2018-10-19 08:07:30               complete  0.135        None          \r\n",
      "sub1.csv  2018-10-19 06:28:57               complete  0.374        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(60)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpg",
   "language": "python",
   "name": "hpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

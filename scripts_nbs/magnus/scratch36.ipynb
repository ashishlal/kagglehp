{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/mathormad/inceptionv3-baseline-lb-0-379/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) + (1 - K.mean(f1))\n",
    "\n",
    "# POS_WEIGHT = 1.0  # multiplier for positive targets, needs to be tuned\n",
    "\n",
    "# def weighted_binary_crossentropy(target, output):\n",
    "#     \"\"\"\n",
    "#     Weighted binary crossentropy between an output tensor \n",
    "#     and a target tensor. POS_WEIGHT is used as a multiplier \n",
    "#     for the positive targets.\n",
    "\n",
    "#     Combination of the following functions:\n",
    "#     * keras.losses.binary_crossentropy\n",
    "#     * keras.backend.tensorflow_backend.binary_crossentropy\n",
    "#     * tf.nn.weighted_cross_entropy_with_logits\n",
    "#     \"\"\"\n",
    "#     # transform back to logits\n",
    "# #     _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "#     _epsilon = K.epsilon()\n",
    "#     output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "#     output = tf.log(output / (1 - output))\n",
    "#     # compute weighted loss\n",
    "#     loss = tf.nn.weighted_cross_entropy_with_logits(targets=target,\n",
    "#                                                     logits=output,\n",
    "#                                                     pos_weight=POS_WEIGHT)\n",
    "#     return tf.reduce_mean(loss, axis=-1)\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.python.framework import ops\n",
    "# from functools import reduce\n",
    "\n",
    "# def binaryRound(x):\n",
    "#     \"\"\"\n",
    "#     Rounds a tensor whose values are in [0,1] to a tensor with values in {0, 1},\n",
    "#     using the straight through estimator for the gradient.\n",
    "#     \"\"\"\n",
    "#     g = tf.get_default_graph()\n",
    "\n",
    "#     with ops.name_scope(\"BinaryRound\") as name:\n",
    "#         with g.gradient_override_map({\"Round\": \"Identity\"}):\n",
    "#             return tf.round(x, name=name)\n",
    "\n",
    "#         # For Tensorflow v0.11 and below use:\n",
    "#         #with g.gradient_override_map({\"Floor\": \"Identity\"}):\n",
    "#         #    return tf.round(x, name=name)\n",
    "        \n",
    "# def f1_loss2(y_true, y_pred):\n",
    "#     y_pred = binaryRound(y_pred)\n",
    "#     tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "#     tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "#     fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "#     fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "#     p = tp / (tp + fp + K.epsilon())\n",
    "#     r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "#     f1 = 2*p*r / (p+r+K.epsilon())\n",
    "#     f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "#     return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) + (1-K.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "path_to_train = '../data/train/'\n",
    "data = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_info = []\n",
    "for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "    train_dataset_info.append({\n",
    "        'path':os.path.join(path_to_train, name),\n",
    "        'labels':np.array([int(label) for label in labels])})\n",
    "train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'path': '../data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0', 'labels': array([16,  0])},\n",
       "       {'path': '../data/train/000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', 'labels': array([7, 1, 2, 0])},\n",
       "       {'path': '../data/train/000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([5])},\n",
       "       ...,\n",
       "       {'path': '../data/train/fff189d8-bbab-11e8-b2ba-ac1f6b6435d0', 'labels': array([7])},\n",
       "       {'path': '../data/train/fffdf7e0-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([25,  2, 21])},\n",
       "       {'path': '../data/train/fffe0ffe-bbc0-11e8-b2bb-ac1f6b6435d0', 'labels': array([2, 0])}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 55521.21it/s]\n"
     ]
    }
   ],
   "source": [
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "path_to_test = '../data/test/'\n",
    "test_dataset_info = []\n",
    "for name in tqdm(submit['Id']):\n",
    "    test_dataset_info.append({\n",
    "        'path':os.path.join(path_to_test, name)})\n",
    "test_dataset_info = np.array(test_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rgb_arr = np.memmap('../cache/tmp_rgb_arr', dtype='uint8', mode='r+', \n",
    "#                    shape=(len(train_dataset_info),299,299,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            dataset_info = shuffle(dataset_info)\n",
    "            for start in range(0, len(dataset_info), batch_size):\n",
    "                end = min(start + batch_size, len(dataset_info))\n",
    "                batch_images = []\n",
    "                X_train_batch = dataset_info[start:end]\n",
    "                batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "                for i in range(len(X_train_batch)):\n",
    "                    image = data_generator.load_image(\n",
    "                        X_train_batch[i]['path'], shape) \n",
    "#                     image = data_generator.load_image(\n",
    "#                         i, shape) \n",
    "                    if augument:\n",
    "                        image = data_generator.augment(image)\n",
    "                    batch_images.append(image/255.)\n",
    "                    batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "                yield np.array(batch_images, np.float32), [batch_labels, np.array(batch_images, np.float32)]\n",
    "    def create_train2(dataset_info, batch_size, shape, augument=False):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            dataset_info = shuffle(dataset_info)\n",
    "            for start in range(0, len(dataset_info), batch_size):\n",
    "                end = min(start + batch_size, len(dataset_info))\n",
    "                batch_images = []\n",
    "                X_train_batch = dataset_info[start:end]\n",
    "                batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "                for i in range(len(X_train_batch)):\n",
    "                    image = data_generator.load_image(\n",
    "                        X_train_batch[i]['path'], shape) \n",
    "#                     image = data_generator.load_image(\n",
    "#                         i, shape) \n",
    "                    \n",
    "                    batch_images.append(image/255.)\n",
    "                    \n",
    "                yield np.array(batch_images, np.float32), None\n",
    "    def create_test(dataset_info, batch_size, shape, augument=False):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            dataset_info = shuffle(dataset_info)\n",
    "            for start in range(0, len(dataset_info), batch_size):\n",
    "                end = min(start + batch_size, len(dataset_info))\n",
    "                batch_images = []\n",
    "                X_test_batch = dataset_info[start:end]\n",
    "                batch_labels = np.zeros((len(X_test_batch), 28))\n",
    "                for i in range(len(X_test_batch)):\n",
    "                    image = data_generator.load_image(\n",
    "                        X_test_batch[i]['path'], shape) \n",
    "#                     image = data_generator.load_image(\n",
    "#                         i, shape) \n",
    "                    \n",
    "                    batch_images.append(image/255.)\n",
    "                    \n",
    "                yield np.array(batch_images, np.float32), None\n",
    "    def load_image(path, shape):\n",
    "        img1 = cv2.imread(path+'_red.png', cv2.IMREAD_GRAYSCALE)\n",
    "        img2 = cv2.imread(path+'_green.png', cv2.IMREAD_GRAYSCALE)\n",
    "        img3 = cv2.imread(path+'_blue.png', cv2.IMREAD_GRAYSCALE)\n",
    "        image = np.stack((img1,img2,img3), -1)\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    def load_image3(idx, shape):\n",
    "#         print(idx)\n",
    "        name = '../cache/RGB/img-{}.png'.format(idx)\n",
    "        image = cv2.imread(name)\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    def load_image2(path, shape):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image = np.stack((\n",
    "            np.array(image_red_ch),\n",
    "            np.array(image_green_ch), \n",
    "            np.array(image_blue_ch)), -1)\n",
    "        w, h = 512, 512\n",
    "#         zero_data = np.zeros((h, w), dtype=np.uint8)\n",
    "#         image2 = np.stack((\n",
    "#             np.array(image_yellow_ch),\n",
    "#             zero_data, zero_data), -1)\n",
    "#         print(image1.shape, image2.shape)\n",
    "#         image = np.vstack((image1, image2))\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    \n",
    "    def augment2(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=0),\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "            ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n",
    "    def augment(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                    iaa.Fliplr(0.5), # horizontal flips\n",
    "                    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "                    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "                    # But we only blur about 50% of all images.\n",
    "                    iaa.Sometimes(0.5,\n",
    "                        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "                    ),\n",
    "                    # Strengthen or weaken the contrast in each image.\n",
    "                    iaa.ContrastNormalization((0.75, 1.5)),\n",
    "                    # Add gaussian noise.\n",
    "                    # For 50% of all images, we sample the noise once per pixel.\n",
    "                    # For the other 50% of all images, we sample the noise per pixel AND\n",
    "                    # channel. This can change the color (not only brightness) of the\n",
    "                    # pixels.\n",
    "                    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "                    # Make some images brighter and some darker.\n",
    "                    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "                    # which can end up changing the color of the images.\n",
    "                    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "                    # Apply affine transformations to each image.\n",
    "                    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "                    iaa.Affine(\n",
    "                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                        rotate=(-180, 180),\n",
    "                        shear=(-8, 8)\n",
    "                    )\n",
    "                ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras as k\n",
    "print(k.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D\n",
    "from keras.layers import UpSampling2D, Lambda, Reshape, Conv2DTranspose\n",
    "from keras.layers import  MaxPooling2D, Concatenate, ReLU, LeakyReLU, GlobalAveragePooling2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_shape = (512, )\n",
    "intermediate_dim = 128\n",
    "batch_size = 16\n",
    "latent_dim = 16\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "layer_filters = [32, 64, 128, 256, 512]\n",
    "\n",
    "# Build the Autoencoder Model\n",
    "# First build the Encoder Model\n",
    "def encoder(input_shape):\n",
    "    inputs = Input(shape=input_shape, name='encoder_input')\n",
    "    x = inputs\n",
    "    # Stack of Conv2D blocks\n",
    "    # Notes:\n",
    "    # 1) Use Batch Normalization before ReLU on deep networks\n",
    "    # 2) Use MaxPooling2D as alternative to strides>1\n",
    "    # - faster but not as good as strides>1\n",
    "    for filters in layer_filters:\n",
    "        x = Conv2D(filters=filters,\n",
    "                   kernel_size=kernel_size,\n",
    "                   strides=2,\n",
    "                   activation='relu',\n",
    "                   padding='same')(x)\n",
    "\n",
    "    shape = K.int_shape(x)\n",
    "\n",
    "    # Generate the latent vector\n",
    "    x = Flatten()(x)\n",
    "    latent = Dense(latent_dim, name='latent_vector')(x)\n",
    "    print(latent.shape)\n",
    "    return latent, shape\n",
    "\n",
    "def decoder(shape):\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
    "    x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n",
    "    x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "    # Stack of Transposed Conv2D blocks\n",
    "    # Notes:\n",
    "    # 1) Use Batch Normalization before ReLU on deep networks\n",
    "    # 2) Use UpSampling2D as alternative to strides>1\n",
    "    # - faster but not as good as strides>1\n",
    "    for filters in layer_filters[::-1]:\n",
    "        x = Conv2DTranspose(filters=filters,\n",
    "                            kernel_size=kernel_size,\n",
    "                            strides=2,\n",
    "                            activation='relu',\n",
    "                            padding='same')(x)\n",
    "\n",
    "    x = Conv2DTranspose(filters=1,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding='same')(x)\n",
    "\n",
    "    outputs = Activation('sigmoid', name='decoder_output')(x)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_train = len(train_dataset_info) \n",
    "l_test = 11702"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42774"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_all = l_train + l_test\n",
    "len_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def global_average_pooling(x):\n",
    "    return K.mean(x, axis = (2, 3))\n",
    "    \n",
    "def global_average_pooling_shape(input_shape):\n",
    "    return input_shape[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out=28):\n",
    "    \n",
    "    inputs = Input(shape=input_shape, name='encoder_input')\n",
    "    x = inputs\n",
    "    # Stack of Conv2D blocks\n",
    "    # Notes:\n",
    "    # 1) Use Batch Normalization before ReLU on deep networks\n",
    "    # 2) Use MaxPooling2D as alternative to strides>1\n",
    "    # - faster but not as good as strides>1\n",
    "    for filters in layer_filters:\n",
    "        x = Conv2D(filters=filters,\n",
    "                   kernel_size=kernel_size,\n",
    "                   strides=2,\n",
    "                   activation='relu',\n",
    "                   padding='same')(x)\n",
    "\n",
    "    shape = K.int_shape(x)\n",
    "\n",
    "    # Generate the latent vector\n",
    "    x = Flatten()(x)\n",
    "    latent_inputs = Dense(latent_dim, name='latent_vector')(x)\n",
    "    \n",
    "#     latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
    "    x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n",
    "    x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "    # Stack of Transposed Conv2D blocks\n",
    "    # Notes:\n",
    "    # 1) Use Batch Normalization before ReLU on deep networks\n",
    "    # 2) Use UpSampling2D as alternative to strides>1\n",
    "    # - faster but not as good as strides>1\n",
    "    for filters in layer_filters[::-1]:\n",
    "        x = Conv2DTranspose(filters=filters,\n",
    "                            kernel_size=kernel_size,\n",
    "                            strides=2,\n",
    "                            activation='relu',\n",
    "                            padding='same')(x)\n",
    "\n",
    "    x = Conv2DTranspose(filters=3,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding='same')(x)\n",
    "\n",
    "    img_out = Activation('sigmoid', name='decoder_output')(x)\n",
    "   \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    img_out = Dense(3, activation='relu')(img_out)\n",
    "    output = Dense(n_out, activation='sigmoid', name='output')(x)\n",
    "    model = Model(inputs, [output, img_out])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 256, 256, 32) 896         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 128, 128, 64) 18496       conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 64, 64, 128)  73856       conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 32, 32, 256)  295168      conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 16, 16, 512)  1180160     conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 131072)       0           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "latent_vector (Dense)           (None, 16)           2097168     flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 131072)       2228224     latent_vector[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 16, 16, 512)  0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_60 (Conv2DTran (None, 32, 32, 512)  2359808     reshape_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_61 (Conv2DTran (None, 64, 64, 256)  1179904     conv2d_transpose_60[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_62 (Conv2DTran (None, 128, 128, 128 295040      conv2d_transpose_61[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_63 (Conv2DTran (None, 256, 256, 64) 73792       conv2d_transpose_62[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_64 (Conv2DTran (None, 512, 512, 32) 18464       conv2d_transpose_63[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_65 (Conv2DTran (None, 512, 512, 3)  867         conv2d_transpose_64[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 786432)       0           conv2d_transpose_65[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 256)          201326848   flatten_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 256)          0           dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Activation)     (None, 512, 512, 3)  0           conv2d_transpose_65[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 28)           7196        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 512, 512, 3)  12          decoder_output[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 211,155,899\n",
      "Trainable params: 211,155,899\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model1(input_shape, n_out=28):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=input_shape)\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = base_model(bn)\n",
    "    x = Lambda(global_average_pooling, output_shape=global_average_pooling_shape)(x)\n",
    "    \n",
    "#     x = Conv2D(256, kernel_size=(1,1), activation='relu', name='conv2')(x)\n",
    "#     x = Conv2D(32, kernel_size=(1,1), activation='relu', name='conv1')(x)\n",
    "#     x = Flatten()(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    z_mean = Dense(latent_dim)(x)\n",
    "    z_log_var = Dense(latent_dim)(x)\n",
    "    z = Lambda(sampling)([z_mean, z_log_var])\n",
    "#     z = Reshape((1,latent_dim))(z)\n",
    "    img_out = Lambda(decode_sample)(z)\n",
    "    img_out = Dense(3, activation='relu')(img_out)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, [output, img_out])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some basic useless model\n",
    "def create_mode2l(input_shape):\n",
    "    \n",
    "    dropRate = 0.5\n",
    "    \n",
    "    init = Input(input_shape)\n",
    "    x = BatchNormalization(axis=-1)(init)\n",
    "    x = Conv2D(32, (3, 3))(x) #, strides=(2,2))(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    ginp1 = Dropout(dropRate)(x)\n",
    "    \n",
    "    x = BatchNormalization(axis=-1)(ginp1)\n",
    "    x = Conv2D(64, (3, 3), strides=(2,2))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(64, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(64, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    ginp2 = Dropout(dropRate)(x)\n",
    "    \n",
    "    x = BatchNormalization(axis=-1)(ginp2)\n",
    "    x = Conv2D(128, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(128, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(128, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    ginp3 = Dropout(dropRate)(x)\n",
    "    \n",
    "    gap1 = GlobalAveragePooling2D()(ginp1)\n",
    "    gap2 = GlobalAveragePooling2D()(ginp2)\n",
    "    gap3 = GlobalAveragePooling2D()(ginp3)\n",
    "    \n",
    "    x = Concatenate()([gap1, gap2, gap3])\n",
    "    \n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    \n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Dense(28)(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    \n",
    "    model = Model(init, x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epochs = 10; batch_size = 16\n",
    "checkpoint = ModelCheckpoint('../cache/ae36.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
    "                                   verbose=1, mode='auto', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=6)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# split data into train, valid\n",
    "indexes = np.arange(train_dataset_info.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "train_indexes, valid_indexes = train_test_split(indexes, test_size=0.15, random_state=8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 256, 256, 32) 896         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 128, 128, 64) 18496       conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 64, 64, 128)  73856       conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 32, 32, 256)  295168      conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 16, 16, 512)  1180160     conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 131072)       0           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "latent_vector (Dense)           (None, 16)           2097168     flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 131072)       2228224     latent_vector[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 16, 16, 512)  0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_60 (Conv2DTran (None, 32, 32, 512)  2359808     reshape_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_61 (Conv2DTran (None, 64, 64, 256)  1179904     conv2d_transpose_60[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_62 (Conv2DTran (None, 128, 128, 128 295040      conv2d_transpose_61[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_63 (Conv2DTran (None, 256, 256, 64) 73792       conv2d_transpose_62[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_64 (Conv2DTran (None, 512, 512, 32) 18464       conv2d_transpose_63[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_65 (Conv2DTran (None, 512, 512, 3)  867         conv2d_transpose_64[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 786432)       0           conv2d_transpose_65[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 256)          201326848   flatten_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 256)          0           dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Activation)     (None, 512, 512, 3)  0           conv2d_transpose_65[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 28)           7196        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 512, 512, 3)  12          decoder_output[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 211,155,899\n",
      "Trainable params: 211,155,899\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# warm up model\n",
    "# model = create_model(\n",
    "#     input_shape=(SIZE,SIZE,3), \n",
    "#     n_out=28)\n",
    "model.summary()\n",
    "\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = False\n",
    "# model.layers[-1].trainable = True\n",
    "# model.layers[-2].trainable = True\n",
    "# model.layers[-3].trainable = True\n",
    "# model.layers[-4].trainable = True\n",
    "# model.layers[-5].trainable = True\n",
    "# model.layers[-6].trainable = True\n",
    "# model.layers[-7].trainable = True\n",
    "# model.layers[-8].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.compile(\n",
    "#     loss=[f1_loss], \n",
    "#     optimizer=Adam(1e-03),\n",
    "#     metrics=[f1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26411 4661\n"
     ]
    }
   ],
   "source": [
    "print(len(train_indexes), len(valid_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch_size=8\n",
    "\n",
    "# # create train and valid datagens\n",
    "# train_generator = data_generator.create_train(\n",
    "#     train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "# validation_generator = data_generator.create_train(\n",
    "#     train_dataset_info[valid_indexes], batch_size, (SIZE,SIZE,3), augument=False)\n",
    "\n",
    "# model.fit_generator(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "#     epochs=2, \n",
    "#     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for layer in model.layers:\n",
    "#     layer.trainable = False\n",
    "# model.layers[-1].trainable = True\n",
    "# model.layers[-2].trainable = True\n",
    "# model.layers[-3].trainable = True\n",
    "# model.layers[-4].trainable = True\n",
    "# model.layers[-5].trainable = True\n",
    "# model.layers[-6].trainable = True\n",
    "# model.layers[-7].trainable = True\n",
    "# model.layers[-8].trainable = True\n",
    "# model.layers[-9].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.compile(\n",
    "#     loss=[f1_loss, 'mse'], \n",
    "#     optimizer=Adam(1e-04),\n",
    "#     metrics=[f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=16\n",
    "\n",
    "# train_generator = data_generator.create_train(\n",
    "#     train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "# validation_generator = data_generator.create_train(\n",
    "#     train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)\n",
    "\n",
    "# model.fit_generator(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "#     epochs=2, \n",
    "#     verbose=1,\n",
    "#     callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=120\n",
    "\n",
    "batch_size=8\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.compile(loss=[f1_loss, 'mse'],\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=[f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "3302/3302 [==============================] - 1871s 567ms/step - loss: 1.1769 - output_loss: 1.1544 - dense_35_loss: 0.0225 - output_f1: 0.0254 - dense_35_f1: 0.0000e+00 - val_loss: 1.1440 - val_output_loss: 1.1221 - val_dense_35_loss: 0.0220 - val_output_f1: 0.0369 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.14404, saving model to ../cache/ae36.h5\n",
      "Epoch 2/120\n",
      "3302/3302 [==============================] - 1101s 333ms/step - loss: 1.1403 - output_loss: 1.1192 - dense_35_loss: 0.0211 - output_f1: 0.0396 - dense_35_f1: 0.0000e+00 - val_loss: 1.1044 - val_output_loss: 1.0824 - val_dense_35_loss: 0.0220 - val_output_f1: 0.0480 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.14404 to 1.10437, saving model to ../cache/ae36.h5\n",
      "Epoch 3/120\n",
      "3302/3302 [==============================] - 1104s 334ms/step - loss: 1.1160 - output_loss: 1.0949 - dense_35_loss: 0.0211 - output_f1: 0.0570 - dense_35_f1: 0.0000e+00 - val_loss: 1.0857 - val_output_loss: 1.0637 - val_dense_35_loss: 0.0220 - val_output_f1: 0.0840 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.10437 to 1.08569, saving model to ../cache/ae36.h5\n",
      "Epoch 4/120\n",
      "3302/3302 [==============================] - 1095s 332ms/step - loss: 1.0977 - output_loss: 1.0765 - dense_35_loss: 0.0211 - output_f1: 0.0754 - dense_35_f1: 0.0000e+00 - val_loss: 1.0682 - val_output_loss: 1.0462 - val_dense_35_loss: 0.0220 - val_output_f1: 0.0976 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.08569 to 1.06815, saving model to ../cache/ae36.h5\n",
      "Epoch 5/120\n",
      "3302/3302 [==============================] - 1093s 331ms/step - loss: 1.0863 - output_loss: 1.0652 - dense_35_loss: 0.0211 - output_f1: 0.0840 - dense_35_f1: 0.0000e+00 - val_loss: 1.0548 - val_output_loss: 1.0328 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1061 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.06815 to 1.05477, saving model to ../cache/ae36.h5\n",
      "Epoch 6/120\n",
      "3302/3302 [==============================] - 1096s 332ms/step - loss: 1.0754 - output_loss: 1.0543 - dense_35_loss: 0.0211 - output_f1: 0.0922 - dense_35_f1: 0.0000e+00 - val_loss: 1.0467 - val_output_loss: 1.0247 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1182 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.05477 to 1.04674, saving model to ../cache/ae36.h5\n",
      "Epoch 7/120\n",
      "3302/3302 [==============================] - 1096s 332ms/step - loss: 1.0667 - output_loss: 1.0456 - dense_35_loss: 0.0211 - output_f1: 0.0990 - dense_35_f1: 0.0000e+00 - val_loss: 1.0435 - val_output_loss: 1.0215 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1246 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.04674 to 1.04347, saving model to ../cache/ae36.h5\n",
      "Epoch 8/120\n",
      "3302/3302 [==============================] - 1107s 335ms/step - loss: 1.0572 - output_loss: 1.0361 - dense_35_loss: 0.0211 - output_f1: 0.1067 - dense_35_f1: 0.0000e+00 - val_loss: 1.0269 - val_output_loss: 1.0049 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1427 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.04347 to 1.02694, saving model to ../cache/ae36.h5\n",
      "Epoch 9/120\n",
      "3302/3302 [==============================] - 1106s 335ms/step - loss: 1.0468 - output_loss: 1.0257 - dense_35_loss: 0.0211 - output_f1: 0.1148 - dense_35_f1: 0.0000e+00 - val_loss: 1.0207 - val_output_loss: 0.9987 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1466 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.02694 to 1.02074, saving model to ../cache/ae36.h5\n",
      "Epoch 10/120\n",
      "3302/3302 [==============================] - 1104s 334ms/step - loss: 1.0356 - output_loss: 1.0145 - dense_35_loss: 0.0211 - output_f1: 0.1232 - dense_35_f1: 0.0000e+00 - val_loss: 1.0212 - val_output_loss: 0.9992 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1434 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.02074\n",
      "Epoch 11/120\n",
      "3302/3302 [==============================] - 1110s 336ms/step - loss: 1.0256 - output_loss: 1.0045 - dense_35_loss: 0.0211 - output_f1: 0.1298 - dense_35_f1: 0.0000e+00 - val_loss: 1.0057 - val_output_loss: 0.9837 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1545 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.02074 to 1.00567, saving model to ../cache/ae36.h5\n",
      "Epoch 12/120\n",
      "3302/3302 [==============================] - 1103s 334ms/step - loss: 1.0141 - output_loss: 0.9930 - dense_35_loss: 0.0211 - output_f1: 0.1370 - dense_35_f1: 0.0000e+00 - val_loss: 1.0023 - val_output_loss: 0.9803 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1599 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.00567 to 1.00225, saving model to ../cache/ae36.h5\n",
      "Epoch 13/120\n",
      "3302/3302 [==============================] - 1103s 334ms/step - loss: 1.0046 - output_loss: 0.9835 - dense_35_loss: 0.0211 - output_f1: 0.1437 - dense_35_f1: 0.0000e+00 - val_loss: 0.9994 - val_output_loss: 0.9774 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1644 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.00225 to 0.99936, saving model to ../cache/ae36.h5\n",
      "Epoch 14/120\n",
      "3302/3302 [==============================] - 1105s 335ms/step - loss: 0.9938 - output_loss: 0.9727 - dense_35_loss: 0.0211 - output_f1: 0.1507 - dense_35_f1: 0.0000e+00 - val_loss: 0.9926 - val_output_loss: 0.9706 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1698 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.99936 to 0.99259, saving model to ../cache/ae36.h5\n",
      "Epoch 15/120\n",
      "3302/3302 [==============================] - 1105s 335ms/step - loss: 0.9835 - output_loss: 0.9624 - dense_35_loss: 0.0211 - output_f1: 0.1562 - dense_35_f1: 0.0000e+00 - val_loss: 0.9889 - val_output_loss: 0.9669 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1775 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.99259 to 0.98886, saving model to ../cache/ae36.h5\n",
      "Epoch 16/120\n",
      "3302/3302 [==============================] - 1098s 333ms/step - loss: 0.9743 - output_loss: 0.9532 - dense_35_loss: 0.0211 - output_f1: 0.1615 - dense_35_f1: 0.0000e+00 - val_loss: 0.9860 - val_output_loss: 0.9640 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1794 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.98886 to 0.98597, saving model to ../cache/ae36.h5\n",
      "Epoch 17/120\n",
      "3302/3302 [==============================] - 1106s 335ms/step - loss: 0.9657 - output_loss: 0.9446 - dense_35_loss: 0.0211 - output_f1: 0.1675 - dense_35_f1: 0.0000e+00 - val_loss: 0.9865 - val_output_loss: 0.9646 - val_dense_35_loss: 0.0219 - val_output_f1: 0.1757 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.98597\n",
      "Epoch 18/120\n",
      "3302/3302 [==============================] - 1106s 335ms/step - loss: 0.9560 - output_loss: 0.9349 - dense_35_loss: 0.0211 - output_f1: 0.1720 - dense_35_f1: 0.0000e+00 - val_loss: 0.9931 - val_output_loss: 0.9711 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1733 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.98597\n",
      "Epoch 19/120\n",
      "3302/3302 [==============================] - 1108s 336ms/step - loss: 0.9488 - output_loss: 0.9278 - dense_35_loss: 0.0211 - output_f1: 0.1760 - dense_35_f1: 0.0000e+00 - val_loss: 0.9833 - val_output_loss: 0.9612 - val_dense_35_loss: 0.0221 - val_output_f1: 0.1846 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.98597 to 0.98325, saving model to ../cache/ae36.h5\n",
      "Epoch 20/120\n",
      "3302/3302 [==============================] - 1103s 334ms/step - loss: 0.9410 - output_loss: 0.9199 - dense_35_loss: 0.0211 - output_f1: 0.1819 - dense_35_f1: 0.0000e+00 - val_loss: 0.9819 - val_output_loss: 0.9600 - val_dense_35_loss: 0.0219 - val_output_f1: 0.1820 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.98325 to 0.98188, saving model to ../cache/ae36.h5\n",
      "Epoch 21/120\n",
      "3302/3302 [==============================] - 1106s 335ms/step - loss: 0.9343 - output_loss: 0.9132 - dense_35_loss: 0.0211 - output_f1: 0.1854 - dense_35_f1: 0.0000e+00 - val_loss: 0.9862 - val_output_loss: 0.9642 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1806 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.98188\n",
      "Epoch 22/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3302/3302 [==============================] - 1107s 335ms/step - loss: 0.9264 - output_loss: 0.9053 - dense_35_loss: 0.0211 - output_f1: 0.1892 - dense_35_f1: 0.0000e+00 - val_loss: 0.9838 - val_output_loss: 0.9618 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1879 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.98188\n",
      "Epoch 23/120\n",
      "3302/3302 [==============================] - 1100s 333ms/step - loss: 0.9185 - output_loss: 0.8974 - dense_35_loss: 0.0211 - output_f1: 0.1944 - dense_35_f1: 0.0000e+00 - val_loss: 0.9836 - val_output_loss: 0.9617 - val_dense_35_loss: 0.0219 - val_output_f1: 0.1845 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.98188\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 24/120\n",
      "3302/3302 [==============================] - 1105s 335ms/step - loss: 0.9019 - output_loss: 0.8809 - dense_35_loss: 0.0211 - output_f1: 0.2020 - dense_35_f1: 0.0000e+00 - val_loss: 0.9703 - val_output_loss: 0.9482 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1984 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.98188 to 0.97025, saving model to ../cache/ae36.h5\n",
      "Epoch 25/120\n",
      "3302/3302 [==============================] - 1094s 331ms/step - loss: 0.8958 - output_loss: 0.8747 - dense_35_loss: 0.0211 - output_f1: 0.2060 - dense_35_f1: 0.0000e+00 - val_loss: 0.9699 - val_output_loss: 0.9479 - val_dense_35_loss: 0.0221 - val_output_f1: 0.1980 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.97025 to 0.96994, saving model to ../cache/ae36.h5\n",
      "Epoch 26/120\n",
      "3302/3302 [==============================] - 1104s 334ms/step - loss: 0.8923 - output_loss: 0.8712 - dense_35_loss: 0.0211 - output_f1: 0.2080 - dense_35_f1: 0.0000e+00 - val_loss: 0.9713 - val_output_loss: 0.9494 - val_dense_35_loss: 0.0219 - val_output_f1: 0.1994 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.96994\n",
      "Epoch 27/120\n",
      "3302/3302 [==============================] - 1101s 333ms/step - loss: 0.8916 - output_loss: 0.8706 - dense_35_loss: 0.0211 - output_f1: 0.2079 - dense_35_f1: 0.0000e+00 - val_loss: 0.9735 - val_output_loss: 0.9515 - val_dense_35_loss: 0.0221 - val_output_f1: 0.1976 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.96994\n",
      "Epoch 28/120\n",
      "3302/3302 [==============================] - 1103s 334ms/step - loss: 0.8882 - output_loss: 0.8671 - dense_35_loss: 0.0211 - output_f1: 0.2099 - dense_35_f1: 0.0000e+00 - val_loss: 0.9747 - val_output_loss: 0.9528 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1984 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.96994\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 29/120\n",
      "3302/3302 [==============================] - 1108s 335ms/step - loss: 0.8851 - output_loss: 0.8640 - dense_35_loss: 0.0211 - output_f1: 0.2109 - dense_35_f1: 0.0000e+00 - val_loss: 0.9731 - val_output_loss: 0.9511 - val_dense_35_loss: 0.0220 - val_output_f1: 0.1975 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.96994\n",
      "Epoch 30/120\n",
      "3302/3302 [==============================] - 1116s 338ms/step - loss: 0.8850 - output_loss: 0.8639 - dense_35_loss: 0.0211 - output_f1: 0.2117 - dense_35_f1: 0.0000e+00 - val_loss: 0.9718 - val_output_loss: 0.9498 - val_dense_35_loss: 0.0219 - val_output_f1: 0.1986 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.96994\n",
      "Epoch 31/120\n",
      "3302/3302 [==============================] - 1113s 337ms/step - loss: 0.8844 - output_loss: 0.8633 - dense_35_loss: 0.0211 - output_f1: 0.2119 - dense_35_f1: 0.0000e+00 - val_loss: 0.9713 - val_output_loss: 0.9493 - val_dense_35_loss: 0.0220 - val_output_f1: 0.2008 - val_dense_35_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.96994\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9775b4b400>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train all layers\n",
    "batch_size=8\n",
    "\n",
    "train_generator = data_generator.create_train(\n",
    "    train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "validation_generator = data_generator.create_train(\n",
    "    train_dataset_info[valid_indexes], 16, (SIZE,SIZE,3), augument=False)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=epochs, \n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for ii in tqdm(np.arange(len(train_dataset_info))):\n",
    "#     img1 = cv2.imread(train_dataset_info[ii]['path']+'_red.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img2 = cv2.imread(train_dataset_info[ii]['path']+'_green.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img3 = cv2.imread(train_dataset_info[ii]['path']+'_blue.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img1 = np.stack((img1, img2, img3), -1)\n",
    "#     name = '../cache/RGB/img-{}.png'.format(ii)\n",
    "#     cv2.imwrite(name,img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# submit = pd.read_csv('../data/sample_submission.csv')\n",
    "# for name in tqdm(submit['Id']):\n",
    "#     path = os.path.join('../data/test/', name)\n",
    "#     img1 = cv2.imread(path+'_red.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img2 = cv2.imread(path+'_green.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img3 = cv2.imread(path+'_blue.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img1 = np.stack((img1, img2, img3), -1)\n",
    "#     name1 = '../cache/RGB/test/img-{}.png'.format(name)\n",
    "#     cv2.imwrite(name1,img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [14:00<00:00, 13.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create submit\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "predicted = []\n",
    "draw_predict = []\n",
    "# model = create_model(\n",
    "#     input_shape=(SIZE,SIZE,3), \n",
    "#     n_out=28)\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = True\n",
    "# model.compile(loss=f1_loss,\n",
    "#             optimizer=Adam(lr=1e-4),\n",
    "#             metrics=[f1])\n",
    "model.load_weights('../cache/ae36.h5')\n",
    "for name in tqdm(submit['Id']):\n",
    "    path = os.path.join('../data/test/', name)\n",
    "    image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "    score_predict= model.predict(image[np.newaxis])[0]\n",
    "    \n",
    "    draw_predict.append(score_predict[0])\n",
    "    label_predict = np.arange(28)[score_predict[0]>=0.2]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted\n",
    "# np.save('../cache/draw_predict_InceptionV3-8.npy', score_predict)\n",
    "# submit.to_csv('../submissions/submit_InceptionV3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv('../submissions/sub36-a.csv', index=False)\n",
    "\n",
    "# sub36-a.csv   2018-11-23 10:12:45               complete  0.287        None          \n",
    "# sub35b-c.csv  2018-11-22 08:00:23               complete  0.417        None          \n",
    "# sub35b-b.csv  2018-11-22 07:59:44               complete  0.415        None          \n",
    "# sub35b-a.csv  2018-11-22 07:58:57               complete  0.432        None          \n",
    "# sub35-a.csv   2018-11-20 06:54:01               complete  0.315        None          \n",
    "# sub8i1-e.csv  2018-11-19 07:12:37               complete  0.457        None          \n",
    "# sub8i1-d.csv  2018-11-19 07:09:46               complete  0.459        None          \n",
    "# sub8i1-c.csv  2018-11-19 07:08:49               complete  0.462        None          \n",
    "# sub8i1-b.csv  2018-11-19 07:08:05               complete  0.462        None          \n",
    "# sub8i1-a.csv  2018-11-19 07:07:14               complete  0.460        None          \n",
    "# sub32-c.csv   2018-11-15 11:47:19               complete  0.463        None          \n",
    "# sub32-bb.csv  2018-11-15 11:46:01               complete  0.465        None          \n",
    "# sub32-b.csv   2018-11-15 11:44:43               complete  0.456        None          \n",
    "# sub32-a.csv   2018-11-15 11:43:12               complete  0.449        None          \n",
    "# sub8f-1.csv   2018-11-14 12:36:46               complete  0.398        None          \n",
    "# sub8j.csv     2018-11-14 08:48:31               complete  0.063        None          \n",
    "# sub8i.csv     2018-11-14 04:28:35               complete  0.443        None          \n",
    "# sub8h.csv     2018-11-13 23:48:00               complete  0.434        None          \n",
    "# sub8g.csv     2018-11-13 07:17:59               complete  0.389        None          \n",
    "# sub8c.csv     2018-11-11 14:31:02               complete  0.429        None          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 49615.81it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "# for name in tqdm(submit['Id']):\n",
    "for score_predict in tqdm(draw_predict):\n",
    "#     path = os.path.join('../data/test/', name)\n",
    "#     image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "#     score_predict = model.predict(image[np.newaxis])[0]\n",
    "#     draw_predict.append(score_predict)\n",
    "    label_predict = np.arange(28)[score_predict>=0.25]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub35b-b.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 50116.60it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "# for name in tqdm(submit['Id']):\n",
    "for score_predict in tqdm(draw_predict):\n",
    "#     path = os.path.join('../data/test/', name)\n",
    "#     image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "#     score_predict = model.predict(image[np.newaxis])[0]\n",
    "#     draw_predict.append(score_predict)\n",
    "    label_predict = np.arange(28)[score_predict>=0.3]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub35b-c.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 53176.84it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "# for name in tqdm(submit['Id']):\n",
    "for score_predict in tqdm(draw_predict):\n",
    "#     path = os.path.join('../data/test/', name)\n",
    "#     image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "#     score_predict = model.predict(image[np.newaxis])[0]\n",
    "#     draw_predict.append(score_predict)\n",
    "    label_predict = np.arange(28)[score_predict>=0.35]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub35b-d.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/1855095/how-to-create-a-zip-archive-of-a-directory\n",
    "def backup_project_as_zip(project_dir, zip_file):\n",
    "    assert(os.path.isdir(project_dir))\n",
    "    assert(os.path.isdir(os.path.dirname(zip_file)))\n",
    "    shutil.make_archive(zip_file.replace('.zip',''), 'zip', project_dir)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-19 12:33:39.384598\n"
     ]
    }
   ],
   "source": [
    "import datetime, shutil\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "PROJECT_PATH = '/home/watts/lal/Kaggle/kagglehp/scripts_nbs'\n",
    "backup_project_as_zip(PROJECT_PATH, '../cache/code.scripts_nbs.%s.zip'%now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 480k/480k [00:08<00:00, 51.3kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 332 ms, sys: 134 ms, total: 467 ms\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# !kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub8i1.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub8i.csv     2018-11-14 04:28:35               complete  0.443        None          \r\n",
      "sub8h.csv     2018-11-13 23:48:00               complete  0.434        None          \r\n",
      "sub8g.csv     2018-11-13 07:17:59               complete  0.389        None          \r\n",
      "sub8c.csv     2018-11-11 14:31:02               complete  0.429        None          \r\n",
      "sub30.csv     2018-11-09 07:02:56               complete  0.033        None          \r\n",
      "sub29.csv     2018-11-08 22:07:11               complete  0.389        None          \r\n",
      "sub28-c.csv   2018-11-08 15:47:08               complete  0.457        None          \r\n",
      "sub28-bb.csv  2018-11-08 15:46:13               complete  0.458        None          \r\n",
      "sub28-b.csv   2018-11-08 15:45:28               complete  0.454        None          \r\n",
      "sub28-a.csv   2018-11-08 15:44:27               complete  0.454        None          \r\n",
      "sub25.csv     2018-11-07 10:45:12               complete  0.421        None          \r\n",
      "sub25.csv     2018-11-07 10:26:20               complete  0.421        None          \r\n",
      "sub8a.csv     2018-11-07 05:53:08               complete  0.425        None          \r\n",
      "sub24.csv     2018-11-07 05:09:26               complete  0.410        None          \r\n",
      "sub23-d.csv   2018-11-07 00:21:06               complete  0.469        None          \r\n",
      "sub23-c.csv   2018-11-06 14:34:37               complete  0.472        None          \r\n",
      "sub23-bb.csv  2018-11-06 14:33:35               complete  0.466        None          \r\n",
      "sub23-b.csv   2018-11-06 14:32:47               complete  0.463        None          \r\n",
      "sub23-a.csv   2018-11-06 14:31:57               complete  0.461        None          \r\n",
      "sub22-a.csv   2018-11-06 14:30:53               complete  0.467        None          \r\n"
     ]
    }
   ],
   "source": [
    "# from time import sleep\n",
    "# sleep(60)\n",
    "# !kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.3.8)\r\n",
      "fileName  date                 description  status    publicScore  privateScore  \r\n",
      "--------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub8.csv  2018-10-20 20:08:45               complete  0.422        None          \r\n",
      "sub7.csv  2018-10-20 17:06:09               complete  0.389        None          \r\n",
      "sub5.csv  2018-10-19 18:27:33               complete  0.387        None          \r\n",
      "sub4.csv  2018-10-19 14:45:15               complete  0.411        None          \r\n",
      "sub3.csv  2018-10-19 10:19:26               complete  0.377        None          \r\n",
      "sub2.csv  2018-10-19 08:07:30               complete  0.135        None          \r\n",
      "sub1.csv  2018-10-19 06:28:57               complete  0.374        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(60)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpg",
   "language": "python",
   "name": "hpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

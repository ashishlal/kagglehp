{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/mathormad/inceptionv3-baseline-lb-0-379/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 299\n",
    "SIZE=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) + (1 - K.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "path_to_train = '../data/train/'\n",
    "data = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_info = []\n",
    "for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "    train_dataset_info.append({\n",
    "        'path':os.path.join(path_to_train, name),\n",
    "        'labels':np.array([int(label) for label in labels])})\n",
    "train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'path': '../data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0', 'labels': array([16,  0])},\n",
       "       {'path': '../data/train/000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', 'labels': array([7, 1, 2, 0])},\n",
       "       {'path': '../data/train/000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([5])},\n",
       "       ...,\n",
       "       {'path': '../data/train/fff189d8-bbab-11e8-b2ba-ac1f6b6435d0', 'labels': array([7])},\n",
       "       {'path': '../data/train/fffdf7e0-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([25,  2, 21])},\n",
       "       {'path': '../data/train/fffe0ffe-bbc0-11e8-b2bb-ac1f6b6435d0', 'labels': array([2, 0])}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rgb_arr = np.memmap('../cache/tmp_rgb_arr', dtype='uint8', mode='r+', \n",
    "#                    shape=(len(train_dataset_info),299,299,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            dataset_info = shuffle(dataset_info)\n",
    "            for start in range(0, len(dataset_info), batch_size):\n",
    "                end = min(start + batch_size, len(dataset_info))\n",
    "                batch_images = []\n",
    "                X_train_batch = dataset_info[start:end]\n",
    "                batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "                for i in range(len(X_train_batch)):\n",
    "                    image = data_generator.load_image(\n",
    "                        X_train_batch[i]['path'], shape) \n",
    "#                     image = data_generator.load_image(\n",
    "#                         i, shape) \n",
    "                    if augument:\n",
    "                        image = data_generator.augment(image)\n",
    "                    batch_images.append(image/255.)\n",
    "                    batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "                yield np.array(batch_images, np.float32), batch_labels\n",
    "    def load_image(path, shape):\n",
    "        img1 = cv2.imread(path+'_red.png', cv2.IMREAD_GRAYSCALE)\n",
    "        img2 = cv2.imread(path+'_green.png', cv2.IMREAD_GRAYSCALE)\n",
    "        img3 = cv2.imread(path+'_blue.png', cv2.IMREAD_GRAYSCALE)\n",
    "        image = np.stack((img1,img2,img3), -1)\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    def load_image3(idx, shape):\n",
    "#         print(idx)\n",
    "        name = '../cache/RGB/img-{}.png'.format(idx)\n",
    "        image = cv2.imread(name)\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    def load_image2(path, shape):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image = np.stack((\n",
    "            np.array(image_red_ch),\n",
    "            np.array(image_green_ch), \n",
    "            np.array(image_blue_ch)), -1)\n",
    "        w, h = 512, 512\n",
    "        zero_data = np.zeros((h, w), dtype=np.uint8)\n",
    "#         image2 = np.stack((\n",
    "#             np.array(image_yellow_ch),\n",
    "#             zero_data, zero_data), -1)\n",
    "#         print(image1.shape, image2.shape)\n",
    "#         image = np.vstack((image1, image2))\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    \n",
    "    def augment(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=0),\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "            ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization \n",
    "from keras.layers import Input, Conv2D, Lambda, add, Add, UpSampling2D, UpSampling1D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_average_pooling(x):\n",
    "    return K.mean(x, axis = (2, 3))\n",
    "    \n",
    "def global_average_pooling_shape(input_shape):\n",
    "    return input_shape[0:2]\n",
    "\n",
    "def expand_dims(x):\n",
    "    return K.expand_dims(x, 1)\n",
    "\n",
    "def expand_dims_output_shape(input_shape):\n",
    "    return (input_shape[0], 1, input_shape[1])\n",
    "\n",
    "def expand_dims_output_shape2(input_shape):\n",
    "    return (input_shape[0], 1, input_shape[1], input_shape[2])\n",
    "\n",
    "def resize_layer(x):\n",
    "    return K.resize_images(x, 299,299, 'channels_last')\n",
    "\n",
    "def resize_output_shape(input_shape):\n",
    "    return (input_shape[0], 299, 299, 3)\n",
    "\n",
    "def resize_layer_vgg(x):\n",
    "    return K.resize_images(x, 224,224, 'channels_last')\n",
    "\n",
    "def resize_vgg_output_shape(input_shape):\n",
    "    return (input_shape[0], 224, 224, 3)\n",
    "\n",
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = MobileNet(include_top=False,\n",
    "                             weights='imagenet',\n",
    "                             input_shape=input_shape)\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = base_model(bn)\n",
    "#     x = Conv2D(256, kernel_size=(1,1), activation='relu', name='conv2')(x)\n",
    "#     x = Conv2D(32, kernel_size=(1,1), activation='relu', name='conv1')(x)\n",
    "    x = Lambda(global_average_pooling, output_shape=global_average_pooling_shape)(x)\n",
    "    x = Lambda(expand_dims, output_shape=expand_dims_output_shape)(x)\n",
    "    x = Lambda(expand_dims, output_shape=expand_dims_output_shape2)(x)\n",
    "    x = Conv2D(3, kernel_size=(1,1), activation='relu', name='conv3')(x)\n",
    "#     x = UpSampling2D(size=(2,2))(x)\n",
    "#     for i in range(8):\n",
    "#         x = UpSampling2D(size=(2,2))(x)\n",
    "#     x = Lambda(resize_layer, output_shape=resize_output_shape)(x)\n",
    "    x = Lambda(lambda a: a[0] + a[1])([x, bn])\n",
    "#     x = Add()([x, bn])\n",
    "#     x = Lambda(resize_layer_vgg, output_shape=resize_vgg_output_shape)(x)\n",
    "    base_model2 = VGG16(include_top=False,\n",
    "                             weights='imagenet',\n",
    "                             input_shape=input_shape)\n",
    "    x = base_model2(x)\n",
    "    x = Flatten()(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_model2(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=input_shape)\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = base_model(bn)\n",
    "#     x = Conv2D(299, kernel_size=(1,1), activation='relu', name='conv2')(x)\n",
    "#     x = Conv2D(32, kernel_size=(1,1), activation='relu', name='conv1')(x)\n",
    "    \n",
    "#     x = Conv2D(256, kernel_size=(1,1), activation='relu', name='conv3')(x)\n",
    "#     x = UpSampling1D(size=2)(x)\n",
    "#     x = add([K.expand_dims(x), input_tensor])\n",
    "    x = Flatten()(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 224, 224, 3)  12          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mobilenet_1.00_224 (Model)      (None, 7, 7, 1024)   3228864     batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 7)            0           mobilenet_1.00_224[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1, 7)         0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 1, 1, 7)      0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 1, 1, 3)      24          lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 224, 224, 3)  0           conv3[0][0]                      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "vgg16 (Model)                   (None, 7, 7, 512)    14714688    lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 25088)        0           vgg16[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          12845568    flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 28)           14364       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 30,803,520\n",
      "Trainable params: 30,781,626\n",
      "Non-trainable params: 21,894\n",
      "__________________________________________________________________________________________________\n",
      "CPU times: user 7.31 s, sys: 200 ms, total: 7.52 s\n",
      "Wall time: 7.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3), \n",
    "    n_out=28)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "# https://github.com/ailias/Focal-Loss-implement-on-Tensorflow/blob/master/focal_loss.py\n",
    "def focal_loss_org(prediction_tensor, target_tensor, weights=None, alpha=0.25, gamma=2):\n",
    "    r\"\"\"Compute focal loss for predictions.\n",
    "        Multi-labels Focal loss formula:\n",
    "            FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                 ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "    Args:\n",
    "     prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing the predicted logits for each class\n",
    "     target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing one-hot encoded classification targets\n",
    "     weights: A float tensor of shape [batch_size, num_anchors]\n",
    "     alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "     gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "    Returns:\n",
    "        loss: A (scalar) tensor representing the value of the loss function\n",
    "    \"\"\"\n",
    "    sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                          - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(weights=None, alpha=0.25, gamma=2):\n",
    "    def focal_loss_my(target_tensor, prediction_tensor, ):\n",
    "        r\"\"\"Compute focal loss for predictions.\n",
    "            Multi-labels Focal loss formula:\n",
    "                FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                     ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "        Args:\n",
    "         prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "            num_classes] representing the predicted logits for each class\n",
    "         target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "            num_classes] representing one-hot encoded classification targets\n",
    "         weights: A float tensor of shape [batch_size, num_anchors]\n",
    "         alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "         gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "        Returns:\n",
    "            loss: A (scalar) tensor representing the value of the loss function\n",
    "        \"\"\"\n",
    "        sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "        zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "\n",
    "        # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "        # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "        pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "\n",
    "        # For negative prediction, only need consider back part loss, front part is 0;\n",
    "        # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "        neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "        per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                              - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "        return tf.reduce_sum(per_entry_cross_ent)\n",
    "#         return K.mean(K.binary_crossentropy(target_tensor, prediction_tensor), axis=-1) + tf.reduce_sum(per_entry_cross_ent)\n",
    "    return focal_loss_my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_fixed(y_true, y_pred):\n",
    "    gamma = 2.\n",
    "    alpha = 0.25\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "#     pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "#     pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def focal_loss(gamma=2., alpha=.25):\n",
    "#     def focal_loss_fixed(y_true, y_pred):\n",
    "#         pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "#         pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "#         pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "#         pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "#         return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "#     return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epochs = 10; batch_size = 4\n",
    "checkpoint = ModelCheckpoint('../cache/InceptionV3-8g.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
    "                                   verbose=1, mode='auto', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=6)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split data into train, valid\n",
    "indexes = np.arange(train_dataset_info.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "train_indexes, valid_indexes = train_test_split(indexes, test_size=0.15, random_state=8)\n",
    "\n",
    "# create train and valid datagens\n",
    "train_generator = data_generator.create_train(\n",
    "    train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "validation_generator = data_generator.create_train(\n",
    "    train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm up model\n",
    "# model = create_model(\n",
    "#     input_shape=(SIZE,SIZE,3), \n",
    "#     n_out=28)\n",
    "# model.summary()\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "model.layers[-1].trainable = True\n",
    "model.layers[-2].trainable = True\n",
    "model.layers[-3].trainable = True\n",
    "model.layers[-4].trainable = True\n",
    "# model.layers[-5].trainable = True\n",
    "# model.layers[-6].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=f1_loss, \n",
    "    optimizer=Adam(1e-03),\n",
    "    metrics=[f1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "6603/6603 [==============================] - 508s 77ms/step - loss: 1.1586 - f1: 0.0255 - val_loss: 1.1189 - val_f1: 0.0204\n",
      "Epoch 2/2\n",
      "6603/6603 [==============================] - 510s 77ms/step - loss: 1.1344 - f1: 0.0281 - val_loss: 1.1113 - val_f1: 0.0184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f36332ba2e8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=2, \n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "6603/6603 [==============================] - 985s 149ms/step - loss: 1.1186 - f1: 0.0313 - val_loss: 1.0618 - val_f1: 0.0562\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.06179, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 2/120\n",
      "6603/6603 [==============================] - 970s 147ms/step - loss: 1.0979 - f1: 0.0465 - val_loss: 1.0359 - val_f1: 0.0948\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.06179 to 1.03591, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 3/120\n",
      "6603/6603 [==============================] - 968s 147ms/step - loss: 1.0857 - f1: 0.0556 - val_loss: 1.0142 - val_f1: 0.1075\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.03591 to 1.01415, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 4/120\n",
      "6603/6603 [==============================] - 967s 146ms/step - loss: 1.0736 - f1: 0.0637 - val_loss: 0.9932 - val_f1: 0.1503\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.01415 to 0.99319, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 5/120\n",
      "6603/6603 [==============================] - 965s 146ms/step - loss: 1.0605 - f1: 0.0720 - val_loss: 0.9588 - val_f1: 0.1805\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.99319 to 0.95877, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 6/120\n",
      "6603/6603 [==============================] - 967s 146ms/step - loss: 1.0472 - f1: 0.0802 - val_loss: 0.9249 - val_f1: 0.2121\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.95877 to 0.92495, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 7/120\n",
      "6603/6603 [==============================] - 970s 147ms/step - loss: 1.0358 - f1: 0.0871 - val_loss: 0.9142 - val_f1: 0.2220\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.92495 to 0.91425, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 8/120\n",
      "6603/6603 [==============================] - 970s 147ms/step - loss: 1.0282 - f1: 0.0911 - val_loss: 0.9020 - val_f1: 0.2359\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.91425 to 0.90204, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 9/120\n",
      "6603/6603 [==============================] - 969s 147ms/step - loss: 1.0216 - f1: 0.0947 - val_loss: 0.8937 - val_f1: 0.2390\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.90204 to 0.89372, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 10/120\n",
      "6603/6603 [==============================] - 968s 147ms/step - loss: 1.0173 - f1: 0.0972 - val_loss: 0.8825 - val_f1: 0.2516\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.89372 to 0.88250, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 11/120\n",
      "6603/6603 [==============================] - 968s 147ms/step - loss: 1.0122 - f1: 0.0999 - val_loss: 0.8716 - val_f1: 0.2577\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.88250 to 0.87155, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 12/120\n",
      "6603/6603 [==============================] - 966s 146ms/step - loss: 1.0092 - f1: 0.1013 - val_loss: 0.8553 - val_f1: 0.2730\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.87155 to 0.85527, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 13/120\n",
      "6603/6603 [==============================] - 971s 147ms/step - loss: 1.0049 - f1: 0.1038 - val_loss: 0.8569 - val_f1: 0.2738\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.85527\n",
      "Epoch 14/120\n",
      "6603/6603 [==============================] - 969s 147ms/step - loss: 1.0020 - f1: 0.1054 - val_loss: 0.8512 - val_f1: 0.2768\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.85527 to 0.85117, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 15/120\n",
      "6603/6603 [==============================] - 970s 147ms/step - loss: 0.9985 - f1: 0.1063 - val_loss: 0.8620 - val_f1: 0.2689\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.85117\n",
      "Epoch 16/120\n",
      "6603/6603 [==============================] - 970s 147ms/step - loss: 0.9957 - f1: 0.1082 - val_loss: 0.8431 - val_f1: 0.2849\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.85117 to 0.84308, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 17/120\n",
      "6603/6603 [==============================] - 969s 147ms/step - loss: 0.9928 - f1: 0.1096 - val_loss: 0.8327 - val_f1: 0.2897\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.84308 to 0.83274, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 18/120\n",
      "6603/6603 [==============================] - 966s 146ms/step - loss: 0.9904 - f1: 0.1106 - val_loss: 0.8285 - val_f1: 0.2937\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.83274 to 0.82850, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 19/120\n",
      "6603/6603 [==============================] - 969s 147ms/step - loss: 0.9878 - f1: 0.1120 - val_loss: 0.8295 - val_f1: 0.2951\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.82850\n",
      "Epoch 20/120\n",
      "6603/6603 [==============================] - 968s 147ms/step - loss: 0.9853 - f1: 0.1135 - val_loss: 0.8230 - val_f1: 0.3052\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.82850 to 0.82295, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 21/120\n",
      "6603/6603 [==============================] - 967s 146ms/step - loss: 0.9831 - f1: 0.1141 - val_loss: 0.8235 - val_f1: 0.2979\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.82295\n",
      "Epoch 22/120\n",
      "6603/6603 [==============================] - 968s 147ms/step - loss: 0.9812 - f1: 0.1153 - val_loss: 0.8234 - val_f1: 0.3037\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.82295\n",
      "Epoch 23/120\n",
      "6603/6603 [==============================] - 938s 142ms/step - loss: 0.9789 - f1: 0.1156 - val_loss: 0.8170 - val_f1: 0.3051\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.82295 to 0.81704, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 24/120\n",
      "6603/6603 [==============================] - 961s 146ms/step - loss: 0.9763 - f1: 0.1174 - val_loss: 0.8179 - val_f1: 0.3060\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.81704\n",
      "Epoch 25/120\n",
      "6603/6603 [==============================] - 956s 145ms/step - loss: 0.9757 - f1: 0.1181 - val_loss: 0.8114 - val_f1: 0.3111\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.81704 to 0.81139, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 26/120\n",
      "6603/6603 [==============================] - 958s 145ms/step - loss: 0.9729 - f1: 0.1191 - val_loss: 0.8139 - val_f1: 0.3080\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.81139\n",
      "Epoch 27/120\n",
      "6603/6603 [==============================] - 958s 145ms/step - loss: 0.9712 - f1: 0.1198 - val_loss: 0.8250 - val_f1: 0.2971\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.81139\n",
      "Epoch 28/120\n",
      "6603/6603 [==============================] - 957s 145ms/step - loss: 0.9699 - f1: 0.1203 - val_loss: 0.8185 - val_f1: 0.3053\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.81139\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 29/120\n",
      "6603/6603 [==============================] - 958s 145ms/step - loss: 0.9516 - f1: 0.1281 - val_loss: 0.7829 - val_f1: 0.3311\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.81139 to 0.78293, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 30/120\n",
      "6603/6603 [==============================] - 955s 145ms/step - loss: 0.9465 - f1: 0.1304 - val_loss: 0.7802 - val_f1: 0.3345\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.78293 to 0.78015, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 31/120\n",
      "6603/6603 [==============================] - 956s 145ms/step - loss: 0.9443 - f1: 0.1313 - val_loss: 0.7785 - val_f1: 0.3369\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.78015 to 0.77849, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 32/120\n",
      "6603/6603 [==============================] - 958s 145ms/step - loss: 0.9423 - f1: 0.1316 - val_loss: 0.7829 - val_f1: 0.3309\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.77849\n",
      "Epoch 33/120\n",
      "6603/6603 [==============================] - 957s 145ms/step - loss: 0.9410 - f1: 0.1326 - val_loss: 0.7781 - val_f1: 0.3340\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.77849 to 0.77810, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 34/120\n",
      "6603/6603 [==============================] - 958s 145ms/step - loss: 0.9394 - f1: 0.1329 - val_loss: 0.7770 - val_f1: 0.3388\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.77810 to 0.77704, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 35/120\n",
      "6603/6603 [==============================] - 958s 145ms/step - loss: 0.9370 - f1: 0.1341 - val_loss: 0.7771 - val_f1: 0.3368\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.77704\n",
      "Epoch 36/120\n",
      "6603/6603 [==============================] - 956s 145ms/step - loss: 0.9365 - f1: 0.1343 - val_loss: 0.7756 - val_f1: 0.3385\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.77704 to 0.77563, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 37/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6603/6603 [==============================] - 930s 141ms/step - loss: 0.9341 - f1: 0.1351 - val_loss: 0.7753 - val_f1: 0.3411\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.77563 to 0.77534, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 38/120\n",
      "6603/6603 [==============================] - 930s 141ms/step - loss: 0.9336 - f1: 0.1351 - val_loss: 0.7778 - val_f1: 0.3374\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.77534\n",
      "Epoch 39/120\n",
      "6603/6603 [==============================] - 931s 141ms/step - loss: 0.9329 - f1: 0.1354 - val_loss: 0.7775 - val_f1: 0.3381\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.77534\n",
      "Epoch 40/120\n",
      "6603/6603 [==============================] - 932s 141ms/step - loss: 0.9311 - f1: 0.1365 - val_loss: 0.7752 - val_f1: 0.3395\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.77534 to 0.77522, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 41/120\n",
      "6603/6603 [==============================] - 932s 141ms/step - loss: 0.9302 - f1: 0.1371 - val_loss: 0.7778 - val_f1: 0.3380\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.77522\n",
      "Epoch 42/120\n",
      "6603/6603 [==============================] - 932s 141ms/step - loss: 0.9294 - f1: 0.1369 - val_loss: 0.7747 - val_f1: 0.3395\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.77522 to 0.77467, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 43/120\n",
      "6603/6603 [==============================] - 951s 144ms/step - loss: 0.9270 - f1: 0.1381 - val_loss: 0.7748 - val_f1: 0.3409\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.77467\n",
      "Epoch 44/120\n",
      "6603/6603 [==============================] - 942s 143ms/step - loss: 0.9262 - f1: 0.1380 - val_loss: 0.7730 - val_f1: 0.3423\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.77467 to 0.77302, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 45/120\n",
      "6603/6603 [==============================] - 933s 141ms/step - loss: 0.9245 - f1: 0.1395 - val_loss: 0.7720 - val_f1: 0.3434\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.77302 to 0.77195, saving model to ../cache/InceptionV3-8g.h5\n",
      "Epoch 46/120\n",
      "6603/6603 [==============================] - 942s 143ms/step - loss: 0.9241 - f1: 0.1390 - val_loss: 0.7764 - val_f1: 0.3401\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.77195\n",
      "Epoch 47/120\n",
      "6603/6603 [==============================] - 956s 145ms/step - loss: 0.9229 - f1: 0.1398 - val_loss: 0.7742 - val_f1: 0.3420\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.77195\n",
      "Epoch 48/120\n",
      "6603/6603 [==============================] - 973s 147ms/step - loss: 0.9212 - f1: 0.1404 - val_loss: 0.7749 - val_f1: 0.3402\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.77195\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 49/120\n",
      "6603/6603 [==============================] - 966s 146ms/step - loss: 0.9188 - f1: 0.1414 - val_loss: 0.7726 - val_f1: 0.3422\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.77195\n",
      "Epoch 50/120\n",
      "6603/6603 [==============================] - 967s 146ms/step - loss: 0.9185 - f1: 0.1414 - val_loss: 0.7733 - val_f1: 0.3421\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.77195\n",
      "Epoch 51/120\n",
      "6603/6603 [==============================] - 962s 146ms/step - loss: 0.9182 - f1: 0.1412 - val_loss: 0.7735 - val_f1: 0.3412\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.77195\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "CPU times: user 16h 40min 19s, sys: 2h 16min 1s, total: 18h 56min 20s\n",
      "Wall time: 13h 35min\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# train all layers\n",
    "epochs=120\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.compile(loss=f1_loss,\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=[f1])\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=epochs, \n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ii in tqdm(np.arange(len(train_dataset_info))):\n",
    "#     img1 = cv2.imread(train_dataset_info[ii]['path']+'_red.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img2 = cv2.imread(train_dataset_info[ii]['path']+'_green.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img3 = cv2.imread(train_dataset_info[ii]['path']+'_blue.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img1 = np.stack((img1, img2, img3), -1)\n",
    "#     name = '../cache/RGB/img-{}.png'.format(ii)\n",
    "#     cv2.imwrite(name,img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit = pd.read_csv('../data/sample_submission.csv')\n",
    "# for name in tqdm(submit['Id']):\n",
    "#     path = os.path.join('../data/test/', name)\n",
    "#     img1 = cv2.imread(path+'_red.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img2 = cv2.imread(path+'_green.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img3 = cv2.imread(path+'_blue.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     img1 = np.stack((img1, img2, img3), -1)\n",
    "#     name1 = '../cache/RGB/test/img-{}.png'.format(name)\n",
    "#     cv2.imwrite(name1,img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 354/11702 [00:08<04:38, 40.75it/s]"
     ]
    }
   ],
   "source": [
    "# Create submit\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "predicted = []\n",
    "draw_predict = []\n",
    "# model = create_model(\n",
    "#     input_shape=(SIZE,SIZE,3), \n",
    "#     n_out=28)\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = True\n",
    "# model.compile(loss=f1_loss,\n",
    "#             optimizer=Adam(lr=1e-4),\n",
    "#             metrics=[f1])\n",
    "model.load_weights('../cache/InceptionV3-8g.h5')\n",
    "for name in tqdm(submit['Id']):\n",
    "    path = os.path.join('../data/test/', name)\n",
    "    image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "    score_predict = model.predict(image[np.newaxis])[0]\n",
    "    draw_predict.append(score_predict)\n",
    "    label_predict = np.arange(28)[score_predict>=0.2]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted\n",
    "# np.save('../cache/draw_predict_InceptionV3-8.npy', score_predict)\n",
    "# submit.to_csv('../submissions/submit_InceptionV3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('../submissions/sub8g.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/1855095/how-to-create-a-zip-archive-of-a-directory\n",
    "def backup_project_as_zip(project_dir, zip_file):\n",
    "    assert(os.path.isdir(project_dir))\n",
    "    assert(os.path.isdir(os.path.dirname(zip_file)))\n",
    "    shutil.make_archive(zip_file.replace('.zip',''), 'zip', project_dir)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-11 20:00:34.000711\n"
     ]
    }
   ],
   "source": [
    "import datetime, shutil\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "PROJECT_PATH = '/home/watts/lal/Kaggle/kagglehp/scripts_nbs'\n",
    "backup_project_as_zip(PROJECT_PATH, '../cache/code.scripts_nbs.%s.zip'%now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 483k/483k [00:08<00:00, 53.2kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 361 ms, sys: 168 ms, total: 530 ms\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub8c.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub8c.csv     2018-11-11 14:31:02               complete  0.429        None          \r\n",
      "sub30.csv     2018-11-09 07:02:56               complete  0.033        None          \r\n",
      "sub29.csv     2018-11-08 22:07:11               complete  0.389        None          \r\n",
      "sub28-c.csv   2018-11-08 15:47:08               complete  0.457        None          \r\n",
      "sub28-bb.csv  2018-11-08 15:46:13               complete  0.458        None          \r\n",
      "sub28-b.csv   2018-11-08 15:45:28               complete  0.454        None          \r\n",
      "sub28-a.csv   2018-11-08 15:44:27               complete  0.454        None          \r\n",
      "sub25.csv     2018-11-07 10:45:12               complete  0.421        None          \r\n",
      "sub25.csv     2018-11-07 10:26:20               complete  0.421        None          \r\n",
      "sub8a.csv     2018-11-07 05:53:08               complete  0.425        None          \r\n",
      "sub24.csv     2018-11-07 05:09:26               complete  0.410        None          \r\n",
      "sub23-d.csv   2018-11-07 00:21:06               complete  0.469        None          \r\n",
      "sub23-c.csv   2018-11-06 14:34:37               complete  0.472        None          \r\n",
      "sub23-bb.csv  2018-11-06 14:33:35               complete  0.466        None          \r\n",
      "sub23-b.csv   2018-11-06 14:32:47               complete  0.463        None          \r\n",
      "sub23-a.csv   2018-11-06 14:31:57               complete  0.461        None          \r\n",
      "sub22-a.csv   2018-11-06 14:30:53               complete  0.467        None          \r\n",
      "sub22-b.csv   2018-11-05 16:50:03               complete  0.467        None          \r\n",
      "sub22-a.csv   2018-11-05 16:48:26               complete  0.467        None          \r\n",
      "sub21-g.csv   2018-11-05 00:29:22               complete  0.462        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(60)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.3.8)\r\n",
      "fileName  date                 description  status    publicScore  privateScore  \r\n",
      "--------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub8.csv  2018-10-20 20:08:45               complete  0.422        None          \r\n",
      "sub7.csv  2018-10-20 17:06:09               complete  0.389        None          \r\n",
      "sub5.csv  2018-10-19 18:27:33               complete  0.387        None          \r\n",
      "sub4.csv  2018-10-19 14:45:15               complete  0.411        None          \r\n",
      "sub3.csv  2018-10-19 10:19:26               complete  0.377        None          \r\n",
      "sub2.csv  2018-10-19 08:07:30               complete  0.135        None          \r\n",
      "sub1.csv  2018-10-19 06:28:57               complete  0.374        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(60)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpg",
   "language": "python",
   "name": "hpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

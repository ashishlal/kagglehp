{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/mathormad/inceptionv3-baseline-lb-0-379/code\n",
    "# validation augmentation is True, TTA, load_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) + (1 - K.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "path_to_train = '../data/train/'\n",
    "data = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_info = []\n",
    "for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "    train_dataset_info.append({\n",
    "        'path':os.path.join(path_to_train, name),\n",
    "        'labels':np.array([int(label) for label in labels])})\n",
    "train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'path': '../data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0', 'labels': array([16,  0])},\n",
       "       {'path': '../data/train/000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', 'labels': array([7, 1, 2, 0])},\n",
       "       {'path': '../data/train/000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([5])},\n",
       "       ...,\n",
       "       {'path': '../data/train/fff189d8-bbab-11e8-b2ba-ac1f6b6435d0', 'labels': array([7])},\n",
       "       {'path': '../data/train/fffdf7e0-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([25,  2, 21])},\n",
       "       {'path': '../data/train/fffe0ffe-bbc0-11e8-b2bb-ac1f6b6435d0', 'labels': array([2, 0])}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            dataset_info = shuffle(dataset_info)\n",
    "            for start in range(0, len(dataset_info), batch_size):\n",
    "                end = min(start + batch_size, len(dataset_info))\n",
    "                batch_images = []\n",
    "                X_train_batch = dataset_info[start:end]\n",
    "                batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "                for i in range(len(X_train_batch)):\n",
    "                    image = data_generator.load_image(\n",
    "                        X_train_batch[i]['path'], shape)   \n",
    "                    if augument:\n",
    "                        image = data_generator.augment(image)\n",
    "                    batch_images.append(image/255.)\n",
    "                    batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "                yield np.array(batch_images, np.float32), batch_labels\n",
    "\n",
    "    def load_image(path, shape):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image = np.stack((\n",
    "            np.array(image_red_ch),\n",
    "            np.array(image_green_ch), \n",
    "            np.array(image_blue_ch)), -1)\n",
    "        w, h = 512, 512\n",
    "        zero_data = np.zeros((h, w), dtype=np.uint8)\n",
    "#         image2 = np.stack((\n",
    "#             np.array(image_yellow_ch),\n",
    "#             zero_data, zero_data), -1)\n",
    "#         print(image1.shape, image2.shape)\n",
    "#         image = np.vstack((image1, image2))\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "\n",
    "    def augment(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=0),\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "            ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=input_shape)\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = base_model(bn)\n",
    "    x = Conv2D(32, kernel_size=(1,1), activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "# https://github.com/ailias/Focal-Loss-implement-on-Tensorflow/blob/master/focal_loss.py\n",
    "def focal_loss_org(prediction_tensor, target_tensor, weights=None, alpha=0.25, gamma=2):\n",
    "    r\"\"\"Compute focal loss for predictions.\n",
    "        Multi-labels Focal loss formula:\n",
    "            FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                 ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "    Args:\n",
    "     prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing the predicted logits for each class\n",
    "     target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing one-hot encoded classification targets\n",
    "     weights: A float tensor of shape [batch_size, num_anchors]\n",
    "     alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "     gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "    Returns:\n",
    "        loss: A (scalar) tensor representing the value of the loss function\n",
    "    \"\"\"\n",
    "    sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                          - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(weights=None, alpha=0.25, gamma=2):\n",
    "    def focal_loss_my(target_tensor, prediction_tensor, ):\n",
    "        r\"\"\"Compute focal loss for predictions.\n",
    "            Multi-labels Focal loss formula:\n",
    "                FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                     ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "        Args:\n",
    "         prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "            num_classes] representing the predicted logits for each class\n",
    "         target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "            num_classes] representing one-hot encoded classification targets\n",
    "         weights: A float tensor of shape [batch_size, num_anchors]\n",
    "         alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "         gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "        Returns:\n",
    "            loss: A (scalar) tensor representing the value of the loss function\n",
    "        \"\"\"\n",
    "        sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "        zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "\n",
    "        # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "        # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "        pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "\n",
    "        # For negative prediction, only need consider back part loss, front part is 0;\n",
    "        # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "        neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "        per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                              - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "        return tf.reduce_sum(per_entry_cross_ent)\n",
    "#         return K.mean(K.binary_crossentropy(target_tensor, prediction_tensor), axis=-1) + tf.reduce_sum(per_entry_cross_ent)\n",
    "    return focal_loss_my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_fixed(y_true, y_pred):\n",
    "    gamma = 2.\n",
    "    alpha = 0.25\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "#     pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "#     pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def focal_loss(gamma=2., alpha=.25):\n",
    "#     def focal_loss_fixed(y_true, y_pred):\n",
    "#         pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "#         pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "#         pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "#         pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "#         return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "#     return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31072,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1554/1554 [==============================] - 372s 239ms/step - loss: 1.1202 - f1: 0.0372 - val_loss: 1.1772 - val_f1: 0.0321\n",
      "Epoch 2/2\n",
      "1554/1554 [==============================] - 368s 237ms/step - loss: 1.1055 - f1: 0.0493 - val_loss: 1.1746 - val_f1: 0.0345\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 444s 286ms/step - loss: 1.0633 - f1: 0.0898 - val_loss: 1.0045 - val_f1: 0.1592\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00447, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 424s 273ms/step - loss: 0.9965 - f1: 0.1532 - val_loss: 0.9405 - val_f1: 0.2185\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.00447 to 0.94047, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 426s 274ms/step - loss: 0.9474 - f1: 0.1957 - val_loss: 0.8935 - val_f1: 0.2628\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.94047 to 0.89347, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 412s 265ms/step - loss: 0.9170 - f1: 0.2224 - val_loss: 0.8867 - val_f1: 0.2723\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.89347 to 0.88669, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 412s 265ms/step - loss: 0.8970 - f1: 0.2362 - val_loss: 0.8536 - val_f1: 0.2909\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.88669 to 0.85361, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.8832 - f1: 0.2448 - val_loss: 0.8347 - val_f1: 0.3079\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.85361 to 0.83473, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 416s 268ms/step - loss: 0.8698 - f1: 0.2533 - val_loss: 0.8608 - val_f1: 0.2955\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.83473\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 417s 268ms/step - loss: 0.8585 - f1: 0.2610 - val_loss: 0.8018 - val_f1: 0.3292\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.83473 to 0.80178, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 431s 278ms/step - loss: 0.8506 - f1: 0.2663 - val_loss: 0.8147 - val_f1: 0.3216\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.80178\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 429s 276ms/step - loss: 0.8421 - f1: 0.2722 - val_loss: 0.7961 - val_f1: 0.3339\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.80178 to 0.79609, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 429s 276ms/step - loss: 0.8341 - f1: 0.2784 - val_loss: 0.7957 - val_f1: 0.3354\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.79609 to 0.79568, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 429s 276ms/step - loss: 0.8270 - f1: 0.2822 - val_loss: 0.7961 - val_f1: 0.3408\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.79568\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 431s 277ms/step - loss: 0.8227 - f1: 0.2847 - val_loss: 0.7960 - val_f1: 0.3407\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.79568\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 430s 276ms/step - loss: 0.8161 - f1: 0.2900 - val_loss: 0.7971 - val_f1: 0.3394\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.79568\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 430s 276ms/step - loss: 0.7893 - f1: 0.3057 - val_loss: 0.7514 - val_f1: 0.3690\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.79568 to 0.75144, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 429s 276ms/step - loss: 0.7821 - f1: 0.3101 - val_loss: 0.7486 - val_f1: 0.3694\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.75144 to 0.74856, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 423s 272ms/step - loss: 0.7762 - f1: 0.3138 - val_loss: 0.7472 - val_f1: 0.3735\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.74856 to 0.74724, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 423s 272ms/step - loss: 0.7736 - f1: 0.3150 - val_loss: 0.7466 - val_f1: 0.3718\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.74724 to 0.74662, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 423s 272ms/step - loss: 0.7712 - f1: 0.3165 - val_loss: 0.7449 - val_f1: 0.3739\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.74662 to 0.74486, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 429s 276ms/step - loss: 0.7656 - f1: 0.3198 - val_loss: 0.7463 - val_f1: 0.3740\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.74486\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 426s 274ms/step - loss: 0.7640 - f1: 0.3205 - val_loss: 0.7435 - val_f1: 0.3766\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.74486 to 0.74345, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 423s 272ms/step - loss: 0.7621 - f1: 0.3218 - val_loss: 0.7452 - val_f1: 0.3734\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.74345\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 425s 274ms/step - loss: 0.7572 - f1: 0.3265 - val_loss: 0.7449 - val_f1: 0.3746\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.74345\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 426s 274ms/step - loss: 0.7557 - f1: 0.3266 - val_loss: 0.7437 - val_f1: 0.3758\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.74345\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 25/120\n",
      "1554/1554 [==============================] - 430s 277ms/step - loss: 0.7503 - f1: 0.3293 - val_loss: 0.7462 - val_f1: 0.3728\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.74345\n",
      "Epoch 26/120\n",
      "1554/1554 [==============================] - 431s 277ms/step - loss: 0.7513 - f1: 0.3284 - val_loss: 0.7431 - val_f1: 0.3755\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.74345 to 0.74310, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 27/120\n",
      "1554/1554 [==============================] - 426s 274ms/step - loss: 0.7507 - f1: 0.3289 - val_loss: 0.7446 - val_f1: 0.3747\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.74310\n",
      "Epoch 28/120\n",
      "1554/1554 [==============================] - 425s 274ms/step - loss: 0.7521 - f1: 0.3272 - val_loss: 0.7442 - val_f1: 0.3746\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.74310\n",
      "Epoch 29/120\n",
      "1554/1554 [==============================] - 425s 273ms/step - loss: 0.7490 - f1: 0.3299 - val_loss: 0.7459 - val_f1: 0.3741\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.74310\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 30/120\n",
      "1554/1554 [==============================] - 425s 274ms/step - loss: 0.7486 - f1: 0.3300 - val_loss: 0.7450 - val_f1: 0.3754\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.74310\n",
      "Epoch 31/120\n",
      "1554/1554 [==============================] - 425s 273ms/step - loss: 0.7505 - f1: 0.3285 - val_loss: 0.7435 - val_f1: 0.3756\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.74310\n",
      "Epoch 32/120\n",
      "1554/1554 [==============================] - 425s 274ms/step - loss: 0.7494 - f1: 0.3290 - val_loss: 0.7420 - val_f1: 0.3766\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.74310 to 0.74196, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 33/120\n",
      "1554/1554 [==============================] - 421s 271ms/step - loss: 0.7470 - f1: 0.3313 - val_loss: 0.7448 - val_f1: 0.3745\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.74196\n",
      "Epoch 34/120\n",
      "1554/1554 [==============================] - 423s 272ms/step - loss: 0.7498 - f1: 0.3298 - val_loss: 0.7422 - val_f1: 0.3761\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.74196\n",
      "Epoch 35/120\n",
      "1554/1554 [==============================] - 419s 270ms/step - loss: 0.7492 - f1: 0.3297 - val_loss: 0.7473 - val_f1: 0.3722\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.74196\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "Epoch 36/120\n",
      "1554/1554 [==============================] - 422s 271ms/step - loss: 0.7490 - f1: 0.3297 - val_loss: 0.7421 - val_f1: 0.3791\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.74196\n",
      "Epoch 37/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.7502 - f1: 0.3290 - val_loss: 0.7431 - val_f1: 0.3765\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.74196\n",
      "Epoch 38/120\n",
      "1554/1554 [==============================] - 415s 267ms/step - loss: 0.7485 - f1: 0.3306 - val_loss: 0.7443 - val_f1: 0.3742\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.74196\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6215/6215 [05:27<00:00, 18.95it/s]\n",
      "11702it [08:01, 24.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1554/1554 [==============================] - 366s 236ms/step - loss: 1.1202 - f1: 0.0371 - val_loss: 1.1411 - val_f1: 0.0182\n",
      "Epoch 2/2\n",
      "1554/1554 [==============================] - 356s 229ms/step - loss: 1.1071 - f1: 0.0469 - val_loss: 1.1600 - val_f1: 0.0321\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 437s 281ms/step - loss: 1.0628 - f1: 0.0890 - val_loss: 0.9983 - val_f1: 0.1650\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.99829, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.9865 - f1: 0.1636 - val_loss: 0.9219 - val_f1: 0.2535\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.99829 to 0.92189, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.9350 - f1: 0.2089 - val_loss: 0.8784 - val_f1: 0.2752\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.92189 to 0.87842, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 414s 267ms/step - loss: 0.9090 - f1: 0.2276 - val_loss: 0.8999 - val_f1: 0.2691\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.87842\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 414s 267ms/step - loss: 0.8909 - f1: 0.2390 - val_loss: 0.8330 - val_f1: 0.3083\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.87842 to 0.83296, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.8758 - f1: 0.2498 - val_loss: 0.8239 - val_f1: 0.3165\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.83296 to 0.82386, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 415s 267ms/step - loss: 0.8641 - f1: 0.2571 - val_loss: 0.8338 - val_f1: 0.3131\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.82386\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.8532 - f1: 0.2646 - val_loss: 0.8166 - val_f1: 0.3197\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.82386 to 0.81662, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 414s 267ms/step - loss: 0.8463 - f1: 0.2682 - val_loss: 0.7971 - val_f1: 0.3382\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.81662 to 0.79707, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 414s 267ms/step - loss: 0.8375 - f1: 0.2742 - val_loss: 0.7999 - val_f1: 0.3283\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.79707\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.8278 - f1: 0.2816 - val_loss: 0.8025 - val_f1: 0.3379\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.79707\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.8233 - f1: 0.2842 - val_loss: 0.7788 - val_f1: 0.3520\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.79707 to 0.77877, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.8138 - f1: 0.2915 - val_loss: 0.7931 - val_f1: 0.3403\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.77877\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.8080 - f1: 0.2940 - val_loss: 0.7742 - val_f1: 0.3551\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.77877 to 0.77420, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 414s 267ms/step - loss: 0.8035 - f1: 0.2973 - val_loss: 0.7736 - val_f1: 0.3568\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.77420 to 0.77362, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.7968 - f1: 0.3017 - val_loss: 0.7655 - val_f1: 0.3613\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.77362 to 0.76555, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.7932 - f1: 0.3034 - val_loss: 0.7624 - val_f1: 0.3614\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.76555 to 0.76242, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.7868 - f1: 0.3077 - val_loss: 0.7737 - val_f1: 0.3560\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.76242\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 414s 267ms/step - loss: 0.7803 - f1: 0.3115 - val_loss: 0.7633 - val_f1: 0.3646\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.76242\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.7772 - f1: 0.3136 - val_loss: 0.7735 - val_f1: 0.3587\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.76242\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.7525 - f1: 0.3274 - val_loss: 0.7294 - val_f1: 0.3865\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.76242 to 0.72943, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.7427 - f1: 0.3328 - val_loss: 0.7334 - val_f1: 0.3843\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.72943\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.7365 - f1: 0.3376 - val_loss: 0.7299 - val_f1: 0.3867\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.72943\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.7335 - f1: 0.3392 - val_loss: 0.7337 - val_f1: 0.3853\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.72943\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 25/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.7294 - f1: 0.3415 - val_loss: 0.7315 - val_f1: 0.3851\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.72943\n",
      "Epoch 26/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.7293 - f1: 0.3407 - val_loss: 0.7271 - val_f1: 0.3892\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.72943 to 0.72707, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 27/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.7294 - f1: 0.3417 - val_loss: 0.7320 - val_f1: 0.3833\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.72707\n",
      "Epoch 28/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.7281 - f1: 0.3422 - val_loss: 0.7281 - val_f1: 0.3874\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.72707\n",
      "Epoch 29/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.7273 - f1: 0.3427 - val_loss: 0.7314 - val_f1: 0.3851\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.72707\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 30/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.7285 - f1: 0.3408 - val_loss: 0.7299 - val_f1: 0.3855\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.72707\n",
      "Epoch 31/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.7300 - f1: 0.3398 - val_loss: 0.7266 - val_f1: 0.3893\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.72707 to 0.72664, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 32/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.7278 - f1: 0.3422 - val_loss: 0.7305 - val_f1: 0.3869\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.72664\n",
      "Epoch 33/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.7270 - f1: 0.3423 - val_loss: 0.7313 - val_f1: 0.3853\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.72664\n",
      "Epoch 34/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.7264 - f1: 0.3427 - val_loss: 0.7296 - val_f1: 0.3869\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.72664\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "Epoch 35/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.7276 - f1: 0.3418 - val_loss: 0.7301 - val_f1: 0.3854\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.72664\n",
      "Epoch 36/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.7271 - f1: 0.3423 - val_loss: 0.7333 - val_f1: 0.3832\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.72664\n",
      "Epoch 37/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.7268 - f1: 0.3431 - val_loss: 0.7269 - val_f1: 0.3900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00037: val_loss did not improve from 0.72664\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6215/6215 [05:24<00:00, 19.14it/s]\n",
      "11702it [08:16, 23.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1554/1554 [==============================] - 372s 239ms/step - loss: 1.1210 - f1: 0.0362 - val_loss: 1.2001 - val_f1: 0.0287\n",
      "Epoch 2/2\n",
      "1554/1554 [==============================] - 349s 225ms/step - loss: 1.1053 - f1: 0.0496 - val_loss: 1.1484 - val_f1: 0.0283\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 436s 280ms/step - loss: 1.0561 - f1: 0.0977 - val_loss: 1.0155 - val_f1: 0.1534\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01549, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.9831 - f1: 0.1667 - val_loss: 0.9532 - val_f1: 0.2099\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01549 to 0.95320, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.9347 - f1: 0.2079 - val_loss: 0.8635 - val_f1: 0.2850\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.95320 to 0.86353, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.9068 - f1: 0.2292 - val_loss: 0.8670 - val_f1: 0.2812\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.86353\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.8887 - f1: 0.2404 - val_loss: 0.8301 - val_f1: 0.3103\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.86353 to 0.83007, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.8757 - f1: 0.2498 - val_loss: 0.9051 - val_f1: 0.2582\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.83007\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.8624 - f1: 0.2591 - val_loss: 0.8211 - val_f1: 0.3217\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.83007 to 0.82107, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 411s 264ms/step - loss: 0.8543 - f1: 0.2645 - val_loss: 0.8100 - val_f1: 0.3261\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.82107 to 0.81003, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.8440 - f1: 0.2710 - val_loss: 0.8255 - val_f1: 0.3244\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.81003\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 411s 265ms/step - loss: 0.8347 - f1: 0.2774 - val_loss: 0.7879 - val_f1: 0.3389\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.81003 to 0.78786, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.8242 - f1: 0.2850 - val_loss: 0.7849 - val_f1: 0.3484\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.78786 to 0.78492, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 411s 265ms/step - loss: 0.8228 - f1: 0.2843 - val_loss: 0.8107 - val_f1: 0.3361\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.78492\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 419s 270ms/step - loss: 0.8129 - f1: 0.2924 - val_loss: 0.7999 - val_f1: 0.3413\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.78492\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 420s 271ms/step - loss: 0.8088 - f1: 0.2947 - val_loss: 0.7856 - val_f1: 0.3479\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.78492\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 416s 268ms/step - loss: 0.7825 - f1: 0.3107 - val_loss: 0.7434 - val_f1: 0.3730\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.78492 to 0.74340, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 416s 268ms/step - loss: 0.7725 - f1: 0.3167 - val_loss: 0.7422 - val_f1: 0.3750\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.74340 to 0.74220, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 416s 268ms/step - loss: 0.7677 - f1: 0.3190 - val_loss: 0.7374 - val_f1: 0.3786\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.74220 to 0.73735, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 421s 271ms/step - loss: 0.7664 - f1: 0.3196 - val_loss: 0.7375 - val_f1: 0.3768\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.73735\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 420s 270ms/step - loss: 0.7627 - f1: 0.3211 - val_loss: 0.7370 - val_f1: 0.3766\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.73735 to 0.73702, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 421s 271ms/step - loss: 0.7591 - f1: 0.3242 - val_loss: 0.7349 - val_f1: 0.3786\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.73702 to 0.73492, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 420s 270ms/step - loss: 0.7557 - f1: 0.3260 - val_loss: 0.7373 - val_f1: 0.3779\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.73492\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 420s 270ms/step - loss: 0.7518 - f1: 0.3285 - val_loss: 0.7360 - val_f1: 0.3783\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.73492\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 421s 271ms/step - loss: 0.7493 - f1: 0.3309 - val_loss: 0.7361 - val_f1: 0.3783\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.73492\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 420s 270ms/step - loss: 0.7458 - f1: 0.3327 - val_loss: 0.7364 - val_f1: 0.3784\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.73492\n",
      "Epoch 25/120\n",
      "1554/1554 [==============================] - 416s 267ms/step - loss: 0.7464 - f1: 0.3316 - val_loss: 0.7371 - val_f1: 0.3776\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.73492\n",
      "Epoch 26/120\n",
      "1554/1554 [==============================] - 420s 271ms/step - loss: 0.7444 - f1: 0.3334 - val_loss: 0.7357 - val_f1: 0.3785\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.73492\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6214/6214 [05:44<00:00, 19.00it/s]\n",
      "11702it [08:28, 23.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1554/1554 [==============================] - 371s 239ms/step - loss: 1.1204 - f1: 0.0375 - val_loss: 1.1442 - val_f1: 0.0282\n",
      "Epoch 2/2\n",
      "1554/1554 [==============================] - 356s 229ms/step - loss: 1.1064 - f1: 0.0477 - val_loss: 1.1722 - val_f1: 0.0333\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 446s 287ms/step - loss: 1.0603 - f1: 0.0944 - val_loss: 0.9821 - val_f1: 0.1705\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.98207, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 415s 267ms/step - loss: 0.9818 - f1: 0.1687 - val_loss: 0.9100 - val_f1: 0.2592\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.98207 to 0.91002, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.9346 - f1: 0.2094 - val_loss: 0.8905 - val_f1: 0.2757\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.91002 to 0.89046, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 419s 270ms/step - loss: 0.9049 - f1: 0.2310 - val_loss: 0.8528 - val_f1: 0.2940\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.89046 to 0.85278, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 420s 270ms/step - loss: 0.8905 - f1: 0.2397 - val_loss: 0.8262 - val_f1: 0.3085\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.85278 to 0.82625, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 420s 271ms/step - loss: 0.8766 - f1: 0.2495 - val_loss: 0.8227 - val_f1: 0.3226\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.82625 to 0.82265, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 421s 271ms/step - loss: 0.8643 - f1: 0.2573 - val_loss: 0.8215 - val_f1: 0.3169\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.82265 to 0.82146, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 419s 270ms/step - loss: 0.8551 - f1: 0.2636 - val_loss: 0.8152 - val_f1: 0.3220\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.82146 to 0.81522, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 421s 271ms/step - loss: 0.8459 - f1: 0.2690 - val_loss: 0.7911 - val_f1: 0.3483\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.81522 to 0.79111, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 419s 269ms/step - loss: 0.8352 - f1: 0.2772 - val_loss: 0.7938 - val_f1: 0.3405\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.79111\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 421s 271ms/step - loss: 0.8308 - f1: 0.2802 - val_loss: 0.7832 - val_f1: 0.3487\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.79111 to 0.78317, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 421s 271ms/step - loss: 0.8204 - f1: 0.2862 - val_loss: 0.7813 - val_f1: 0.3465\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.78317 to 0.78130, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 422s 271ms/step - loss: 0.8146 - f1: 0.2908 - val_loss: 0.7881 - val_f1: 0.3455\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.78130\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 421s 271ms/step - loss: 0.8081 - f1: 0.2938 - val_loss: 0.7789 - val_f1: 0.3441\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.78130 to 0.77894, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 420s 270ms/step - loss: 0.8002 - f1: 0.2992 - val_loss: 0.8306 - val_f1: 0.3269\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.77894\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 425s 273ms/step - loss: 0.7961 - f1: 0.3021 - val_loss: 0.8124 - val_f1: 0.3281\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.77894\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 420s 270ms/step - loss: 0.7879 - f1: 0.3079 - val_loss: 0.7733 - val_f1: 0.3575\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.77894 to 0.77325, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 419s 270ms/step - loss: 0.7848 - f1: 0.3089 - val_loss: 0.7698 - val_f1: 0.3607\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.77325 to 0.76976, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 419s 270ms/step - loss: 0.7783 - f1: 0.3128 - val_loss: 0.7667 - val_f1: 0.3608\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.76976 to 0.76671, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 422s 272ms/step - loss: 0.7736 - f1: 0.3159 - val_loss: 0.7979 - val_f1: 0.3410\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.76671\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 422s 271ms/step - loss: 0.7692 - f1: 0.3180 - val_loss: 0.7910 - val_f1: 0.3471\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.76671\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 424s 273ms/step - loss: 0.7652 - f1: 0.3212 - val_loss: 0.7635 - val_f1: 0.3630\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.76671 to 0.76354, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 421s 271ms/step - loss: 0.7599 - f1: 0.3240 - val_loss: 0.7858 - val_f1: 0.3570\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.76354\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 422s 271ms/step - loss: 0.7561 - f1: 0.3268 - val_loss: 0.7682 - val_f1: 0.3624\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.76354\n",
      "Epoch 25/120\n",
      "1554/1554 [==============================] - 419s 270ms/step - loss: 0.7507 - f1: 0.3297 - val_loss: 0.7886 - val_f1: 0.3547\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.76354\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 26/120\n",
      "1554/1554 [==============================] - 421s 271ms/step - loss: 0.7267 - f1: 0.3426 - val_loss: 0.7390 - val_f1: 0.3783\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.76354 to 0.73898, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 27/120\n",
      "1554/1554 [==============================] - 419s 270ms/step - loss: 0.7177 - f1: 0.3480 - val_loss: 0.7318 - val_f1: 0.3863\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.73898 to 0.73177, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 28/120\n",
      "1554/1554 [==============================] - 420s 270ms/step - loss: 0.7125 - f1: 0.3514 - val_loss: 0.7377 - val_f1: 0.3842\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.73177\n",
      "Epoch 29/120\n",
      "1554/1554 [==============================] - 420s 271ms/step - loss: 0.7070 - f1: 0.3549 - val_loss: 0.7317 - val_f1: 0.3867\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.73177 to 0.73168, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 30/120\n",
      "1554/1554 [==============================] - 423s 272ms/step - loss: 0.7059 - f1: 0.3550 - val_loss: 0.7368 - val_f1: 0.3810\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.73168\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 31/120\n",
      "1554/1554 [==============================] - 419s 270ms/step - loss: 0.7015 - f1: 0.3576 - val_loss: 0.7354 - val_f1: 0.3827\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.73168\n",
      "Epoch 32/120\n",
      "1554/1554 [==============================] - 422s 271ms/step - loss: 0.7034 - f1: 0.3554 - val_loss: 0.7355 - val_f1: 0.3839\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.73168\n",
      "Epoch 33/120\n",
      "1554/1554 [==============================] - 416s 268ms/step - loss: 0.6987 - f1: 0.3592 - val_loss: 0.7350 - val_f1: 0.3846\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.73168\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 34/120\n",
      "1554/1554 [==============================] - 416s 268ms/step - loss: 0.6989 - f1: 0.3591 - val_loss: 0.7352 - val_f1: 0.3835\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.73168\n",
      "Epoch 35/120\n",
      "1554/1554 [==============================] - 415s 267ms/step - loss: 0.7020 - f1: 0.3562 - val_loss: 0.7336 - val_f1: 0.3837\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.73168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6214/6214 [05:36<00:00, 18.47it/s]\n",
      "11702it [08:45, 22.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1554/1554 [==============================] - 377s 243ms/step - loss: 1.1201 - f1: 0.0377 - val_loss: 1.1234 - val_f1: 0.0250\n",
      "Epoch 2/2\n",
      "1554/1554 [==============================] - 361s 232ms/step - loss: 1.1078 - f1: 0.0459 - val_loss: 1.1323 - val_f1: 0.0327\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 440s 283ms/step - loss: 1.0632 - f1: 0.0918 - val_loss: 1.0085 - val_f1: 0.1473\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00852, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.9865 - f1: 0.1643 - val_loss: 0.9201 - val_f1: 0.2414\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.00852 to 0.92009, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 424s 273ms/step - loss: 0.9371 - f1: 0.2048 - val_loss: 0.9056 - val_f1: 0.2543\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.92009 to 0.90559, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 0.9066 - f1: 0.2285 - val_loss: 0.8648 - val_f1: 0.2840\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.90559 to 0.86479, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 411s 264ms/step - loss: 0.8879 - f1: 0.2419 - val_loss: 0.8468 - val_f1: 0.3029\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.86479 to 0.84677, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 412s 265ms/step - loss: 0.8730 - f1: 0.2515 - val_loss: 0.8205 - val_f1: 0.3191\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.84677 to 0.82053, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 411s 264ms/step - loss: 0.8612 - f1: 0.2589 - val_loss: 0.7958 - val_f1: 0.3320\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.82053 to 0.79577, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.8510 - f1: 0.2661 - val_loss: 0.7962 - val_f1: 0.3372\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.79577\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 412s 265ms/step - loss: 0.8428 - f1: 0.2716 - val_loss: 0.8000 - val_f1: 0.3406\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.79577\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.8337 - f1: 0.2783 - val_loss: 0.7890 - val_f1: 0.3387\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.79577 to 0.78898, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 413s 265ms/step - loss: 0.8269 - f1: 0.2822 - val_loss: 0.7877 - val_f1: 0.3435\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.78898 to 0.78766, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 412s 265ms/step - loss: 0.8196 - f1: 0.2867 - val_loss: 0.8127 - val_f1: 0.3297\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.78766\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 411s 265ms/step - loss: 0.8121 - f1: 0.2904 - val_loss: 0.7687 - val_f1: 0.3582\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.78766 to 0.76868, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 412s 265ms/step - loss: 0.8059 - f1: 0.2961 - val_loss: 0.7910 - val_f1: 0.3469\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.76868\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 411s 265ms/step - loss: 0.7987 - f1: 0.3006 - val_loss: 0.8022 - val_f1: 0.3372\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.76868\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 412s 265ms/step - loss: 0.7930 - f1: 0.3045 - val_loss: 0.7865 - val_f1: 0.3420\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.76868\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 418s 269ms/step - loss: 0.7689 - f1: 0.3183 - val_loss: 0.7352 - val_f1: 0.3802\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.76868 to 0.73521, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 417s 268ms/step - loss: 0.7578 - f1: 0.3256 - val_loss: 0.7323 - val_f1: 0.3844\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.73521 to 0.73234, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 416s 268ms/step - loss: 0.7532 - f1: 0.3276 - val_loss: 0.7342 - val_f1: 0.3786\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.73234\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 416s 268ms/step - loss: 0.7494 - f1: 0.3301 - val_loss: 0.7286 - val_f1: 0.3882\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.73234 to 0.72856, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 415s 267ms/step - loss: 0.7440 - f1: 0.3334 - val_loss: 0.7329 - val_f1: 0.3806\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.72856\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.7420 - f1: 0.3337 - val_loss: 0.7329 - val_f1: 0.3846\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.72856\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 409s 263ms/step - loss: 0.7408 - f1: 0.3339 - val_loss: 0.7352 - val_f1: 0.3833\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.72856\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 409s 263ms/step - loss: 0.7339 - f1: 0.3390 - val_loss: 0.7310 - val_f1: 0.3851\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.72856\n",
      "Epoch 25/120\n",
      "1554/1554 [==============================] - 409s 263ms/step - loss: 0.7347 - f1: 0.3386 - val_loss: 0.7289 - val_f1: 0.3886\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.72856\n",
      "Epoch 26/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.7356 - f1: 0.3380 - val_loss: 0.7291 - val_f1: 0.3872\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.72856\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6214/6214 [07:24<00:00, 12.64it/s]\n",
      "11702it [09:31, 20.49it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split data into train, valid\n",
    "indexes = np.arange(train_dataset_info.shape[0])\n",
    "# np.random.shuffle(indexes)\n",
    "# train_indexes, valid_indexes = train_test_split(indexes, test_size=0.15, random_state=8)\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "# train_generator = data_generator.create_train(\n",
    "#     train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "# validation_generator = data_generator.create_train(\n",
    "#     train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)\n",
    "\n",
    "oof_class_preds = np.zeros((train_dataset_info.shape[0], 28))\n",
    "sub_class_preds = np.zeros((submit.shape[0], 28))\n",
    "\n",
    "fold_ = 0\n",
    "epochs = 10; batch_size = 16\n",
    "for train_indexes, valid_indexes in kf.split(indexes):\n",
    "    \n",
    "    checkpoint = ModelCheckpoint('../cache/InceptionV3.h5', monitor='val_loss', verbose=1, \n",
    "                                 save_best_only=True, mode='min', save_weights_only = True)\n",
    "    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
    "                                       verbose=1, mode='auto', epsilon=0.0001)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", \n",
    "                          mode=\"min\", \n",
    "                          patience=6)\n",
    "    callbacks_list = [checkpoint, early, reduceLROnPlat]\n",
    "\n",
    "\n",
    "    _preds = []\n",
    "    # create train and valid datagens\n",
    "    train_generator = data_generator.create_train(\n",
    "        train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "    validation_generator = data_generator.create_train(\n",
    "        train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=True)\n",
    "\n",
    "    # warm up model\n",
    "    model = create_model(\n",
    "        input_shape=(SIZE,SIZE,3), \n",
    "        n_out=28)\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.layers[-1].trainable = True\n",
    "    model.layers[-2].trainable = True\n",
    "    model.layers[-3].trainable = True\n",
    "    model.layers[-4].trainable = True\n",
    "    model.layers[-5].trainable = True\n",
    "    model.layers[-6].trainable = True\n",
    "    \n",
    "    model.compile(\n",
    "        loss=f1_loss, \n",
    "        optimizer=Adam(1e-03),\n",
    "        metrics=[f1])\n",
    "#     model.summary()\n",
    "\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "        epochs=2, \n",
    "        verbose=1)\n",
    "    \n",
    "    # train all layers\n",
    "    epochs=120\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    model.compile(loss=f1_loss,\n",
    "                optimizer=Adam(lr=1e-4),\n",
    "                metrics=[f1])\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "        epochs=epochs, \n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list)\n",
    "    \n",
    "    model.save_weights('../cache/my_model_weights-21.h5')\n",
    "    model.load_weights('../cache/InceptionV3.h5')\n",
    "    for idx in tqdm(valid_indexes):\n",
    "        item = train_dataset_info[idx]\n",
    "        path = item['path']\n",
    "        labels = item['labels']\n",
    "        image = data_generator.load_image(path, (SIZE,SIZE,3))\n",
    "        image = data_generator.augment(image)\n",
    "        image = image / 255.\n",
    "        score_predict = model.predict(image[np.newaxis])[0]\n",
    "        oof_class_preds[idx] = score_predict\n",
    "        np.save('../cache/oof_class_preds-21.npy', oof_class_preds)\n",
    "        \n",
    "    for idx, name in tqdm(enumerate(submit['Id'])):\n",
    "        path = os.path.join('../data/test/', name)\n",
    "        image = data_generator.load_image(path, (SIZE,SIZE,3))\n",
    "        image = data_generator.augment(image)\n",
    "        image = image / 255.\n",
    "        score_predict = model.predict(image[np.newaxis])[0]\n",
    "        sub_class_preds[idx] += score_predict\n",
    "        np.save('../cache/sub_class_preds-21.npy', sub_class_preds)\n",
    "    fold_ += 1\n",
    "sub_class_preds /= n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in tqdm(valid_indexes):\n",
    "#     item = train_dataset_info[idx]\n",
    "#     path = item['path']\n",
    "#     labels = item['labels']\n",
    "#     image = data_generator.load_image(path, (SIZE,SIZE,3))\n",
    "#     image = data_generator.augment(image)\n",
    "#     image = image / 255.\n",
    "#     score_predict = model.predict(image[np.newaxis])[0]\n",
    "#     oof_class_preds[idx] = score_predict\n",
    "#     np.save('../cache/oof_class_preds-20.npy', oof_class_preds)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../cache/oof_class_preds-21-1.npy', oof_class_preds)\n",
    "np.save('../cache/sub_class_preds-21-1.npy', sub_class_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof_class_preds = np.zeros((train_dataset_info.shape[0], 28))\n",
    "# sub_class_preds = np.zeros((submit.shape[0], 28))\n",
    "# score_predict = model.predict(image[np.newaxis])[0]\n",
    "# oof_class_preds[idx] = score_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train_index, test_index in kf.split(indexes):\n",
    "# ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11702, 28)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_class_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 94235.74it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for line in tqdm(sub_class_preds):\n",
    "    label_predict = np.arange(28)[line>=0.2]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11702"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " '5 25',\n",
       " '0 5 25',\n",
       " '0 25',\n",
       " '0 7 25',\n",
       " '4 21',\n",
       " '0 4 25',\n",
       " '0 23 25',\n",
       " '0',\n",
       " '25',\n",
       " '17 18 25',\n",
       " '3 5',\n",
       " '0 25',\n",
       " '7 20 25',\n",
       " '23',\n",
       " '0 4 18 25',\n",
       " '2 14',\n",
       " '0 5',\n",
       " '14 21',\n",
       " '0 5',\n",
       " '6',\n",
       " '3 5 24',\n",
       " '0 11 16 25',\n",
       " '0 7 21',\n",
       " '0',\n",
       " '0 12 21 25 26',\n",
       " '0',\n",
       " '0',\n",
       " '0 2 5 25',\n",
       " '0',\n",
       " '13 21',\n",
       " '0 7',\n",
       " '14 16 17 18 21 25',\n",
       " '5 25',\n",
       " '0 7 25',\n",
       " '13',\n",
       " '0 25',\n",
       " '0 3',\n",
       " '0 5 21 25',\n",
       " '1',\n",
       " '0 16 17 25',\n",
       " '6 25',\n",
       " '0 21 25',\n",
       " '18 19 25',\n",
       " '16 22 25',\n",
       " '6',\n",
       " '0',\n",
       " '0',\n",
       " '0 23 25',\n",
       " '0',\n",
       " '0 16 17 25',\n",
       " '0 5',\n",
       " '20 23 24',\n",
       " '25',\n",
       " '3',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '11 23',\n",
       " '0 25',\n",
       " '11 21 25',\n",
       " '2 21 22',\n",
       " '0 5 21 22',\n",
       " '14 16 25',\n",
       " '6 7 21 25',\n",
       " '23',\n",
       " '0 19 25',\n",
       " '3 21 25',\n",
       " '0 21 25',\n",
       " '0',\n",
       " '11 21 25',\n",
       " '2 3',\n",
       " '0 2',\n",
       " '14',\n",
       " '4',\n",
       " '21',\n",
       " '0',\n",
       " '0 4',\n",
       " '0 1 21',\n",
       " '0 25',\n",
       " '0 16 25',\n",
       " '6 25',\n",
       " '0 25',\n",
       " '0 21',\n",
       " '0 2 16 17 21 25',\n",
       " '17 18',\n",
       " '0 23 24 25',\n",
       " '20 23',\n",
       " '0 21',\n",
       " '14 25',\n",
       " '11 14',\n",
       " '25',\n",
       " '11 14 21',\n",
       " '23',\n",
       " '12 13',\n",
       " '0 16 17 25',\n",
       " '0 25',\n",
       " '7 17 18 25',\n",
       " '0 7 19 25',\n",
       " '24',\n",
       " '0 23 25',\n",
       " '0 19 25',\n",
       " '23',\n",
       " '21',\n",
       " '0 23',\n",
       " '7 11 25',\n",
       " '19 21 25',\n",
       " '0 14 16',\n",
       " '0 11 16 24',\n",
       " '20 25 26',\n",
       " '0 25',\n",
       " '0 2 5',\n",
       " '1',\n",
       " '14 16 17 18 25',\n",
       " '0 22 25',\n",
       " '25',\n",
       " '21 23',\n",
       " '0 2 25',\n",
       " '4 25',\n",
       " '14 16 17 25',\n",
       " '26',\n",
       " '0 18 19 25',\n",
       " '21 25',\n",
       " '2 21 25',\n",
       " '8 20',\n",
       " '0 4',\n",
       " '0',\n",
       " '0 25',\n",
       " '25',\n",
       " '0 3 4',\n",
       " '19',\n",
       " '25',\n",
       " '0 21',\n",
       " '0',\n",
       " '0 7',\n",
       " '0 11',\n",
       " '5',\n",
       " '0 14 16',\n",
       " '0',\n",
       " '5 21',\n",
       " '6 25',\n",
       " '0 19',\n",
       " '2 6 21 25',\n",
       " '0 1',\n",
       " '0 5 25',\n",
       " '0 25',\n",
       " '26',\n",
       " '0 21 25',\n",
       " '0 6 25',\n",
       " '0',\n",
       " '0 6 25',\n",
       " '0 23',\n",
       " '7 14 16 17 18',\n",
       " '0 7',\n",
       " '0 11 25',\n",
       " '6 7 25',\n",
       " '6',\n",
       " '0',\n",
       " '0 25',\n",
       " '1 25',\n",
       " '25',\n",
       " '0 19 25',\n",
       " '0 21 25',\n",
       " '0 11 19 25',\n",
       " '4 25',\n",
       " '5',\n",
       " '23',\n",
       " '0 25',\n",
       " '19',\n",
       " '11 21 25',\n",
       " '7 16 17 25',\n",
       " '5 25',\n",
       " '0 6 25',\n",
       " '0 14 16 25',\n",
       " '21 23',\n",
       " '0 2',\n",
       " '0 11',\n",
       " '21',\n",
       " '0 7 25',\n",
       " '0 12 21',\n",
       " '0 2 25',\n",
       " '0',\n",
       " '23',\n",
       " '0',\n",
       " '0 19 24',\n",
       " '14',\n",
       " '0 14 16 17 21 25',\n",
       " '0 11 21 25',\n",
       " '0 5 7 18 25',\n",
       " '23',\n",
       " '0 14 16 17 21 25',\n",
       " '17 25',\n",
       " '0 25',\n",
       " '5 25 26',\n",
       " '25 26',\n",
       " '0 5 21 25',\n",
       " '0 13 22',\n",
       " '0 25',\n",
       " '14 16 19 25',\n",
       " '23 25',\n",
       " '0 4 25',\n",
       " '2 21',\n",
       " '0 2 3 25',\n",
       " '0 2 5 25',\n",
       " '0 25',\n",
       " '0 3 19 25',\n",
       " '21',\n",
       " '0 25',\n",
       " '0 5',\n",
       " '0',\n",
       " '0 21',\n",
       " '0 14 16 21',\n",
       " '0 5',\n",
       " '7',\n",
       " '0 5',\n",
       " '18 19 25',\n",
       " '0 25',\n",
       " '24 26',\n",
       " '7 21 25',\n",
       " '21 22',\n",
       " '2 3',\n",
       " '0 3 22',\n",
       " '14 16 17 25',\n",
       " '21',\n",
       " '2 3 12 21',\n",
       " '6 8 20 23',\n",
       " '0 23',\n",
       " '14 21',\n",
       " '12 25',\n",
       " '0 19',\n",
       " '0 5',\n",
       " '2 25',\n",
       " '0 11',\n",
       " '0 2',\n",
       " '23',\n",
       " '0 23 25',\n",
       " '11',\n",
       " '13 20 26',\n",
       " '0 12',\n",
       " '0 18 21 25',\n",
       " '17 18 21 25',\n",
       " '21',\n",
       " '2 7',\n",
       " '21',\n",
       " '0 21 25',\n",
       " '2 4 11 14 16 17 25',\n",
       " '12 23',\n",
       " '25',\n",
       " '4',\n",
       " '13 22',\n",
       " '0 1',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '2 6 23 25',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '11',\n",
       " '23',\n",
       " '3 5',\n",
       " '19 25',\n",
       " '12 13 21 25',\n",
       " '14 17 21 25',\n",
       " '19 26',\n",
       " '3 4 26',\n",
       " '0 2 5',\n",
       " '13 21 22',\n",
       " '0',\n",
       " '12',\n",
       " '0 2 5',\n",
       " '23 25',\n",
       " '0 25',\n",
       " '0 19',\n",
       " '0 21 25',\n",
       " '0 12',\n",
       " '0 2 25',\n",
       " '2',\n",
       " '0 18 19 25',\n",
       " '0 2 19',\n",
       " '11',\n",
       " '0 25',\n",
       " '0',\n",
       " '0 21 25',\n",
       " '2',\n",
       " '3',\n",
       " '8 9 10 20',\n",
       " '0 19',\n",
       " '0 12 21',\n",
       " '0 2',\n",
       " '7',\n",
       " '1 2',\n",
       " '7 11',\n",
       " '0 5 7',\n",
       " '0 14 16',\n",
       " '0 18 21 25',\n",
       " '7',\n",
       " '0 19',\n",
       " '0 21 25',\n",
       " '25',\n",
       " '0 2 25',\n",
       " '0 5 19',\n",
       " '0 2 21 25',\n",
       " '0 26',\n",
       " '0 5 19 24',\n",
       " '0 11 25',\n",
       " '0 12 14',\n",
       " '25',\n",
       " '0',\n",
       " '1',\n",
       " '14 16 17 25',\n",
       " '0 21',\n",
       " '0',\n",
       " '11 25',\n",
       " '6 11',\n",
       " '0 7 25',\n",
       " '0',\n",
       " '0 25',\n",
       " '21 25',\n",
       " '14 16 17 18 25',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '0 2 16 17 25',\n",
       " '19',\n",
       " '6 25',\n",
       " '0 25',\n",
       " '0 2',\n",
       " '0',\n",
       " '0 2 7 18 25',\n",
       " '21 22',\n",
       " '0 2',\n",
       " '0 2 3',\n",
       " '14',\n",
       " '5',\n",
       " '9 10 18 19 25',\n",
       " '25',\n",
       " '0 1 21',\n",
       " '11 21 25',\n",
       " '7',\n",
       " '24',\n",
       " '14 16 17 25',\n",
       " '0 25',\n",
       " '6',\n",
       " '21 25',\n",
       " '0 7 18 25',\n",
       " '0 2 4',\n",
       " '14 16',\n",
       " '4 12 21 22 25',\n",
       " '0 2 3 5 19 25',\n",
       " '18 19',\n",
       " '2 3 7',\n",
       " '0 2 6 21 25',\n",
       " '0 25',\n",
       " '0 7',\n",
       " '20 26',\n",
       " '0 25',\n",
       " '5 19',\n",
       " '0 2 25',\n",
       " '3',\n",
       " '0 7 24',\n",
       " '0 5 21',\n",
       " '25',\n",
       " '2 4',\n",
       " '11 24 25',\n",
       " '2 6 25',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '7 25',\n",
       " '11',\n",
       " '11',\n",
       " '0',\n",
       " '0 5 18 19 25',\n",
       " '0 5',\n",
       " '16 17 18',\n",
       " '0 1',\n",
       " '0 21 25',\n",
       " '17 25',\n",
       " '0 23',\n",
       " '0 21',\n",
       " '0 3 26',\n",
       " '0 3 25',\n",
       " '12 21',\n",
       " '4 13 21 25',\n",
       " '0 21',\n",
       " '2 7',\n",
       " '0',\n",
       " '2 25',\n",
       " '0 4 5',\n",
       " '14',\n",
       " '0',\n",
       " '3',\n",
       " '18 19',\n",
       " '0 25',\n",
       " '16 17 18',\n",
       " '0 25',\n",
       " '0 14',\n",
       " '21 25',\n",
       " '0 5 19',\n",
       " '0',\n",
       " '0 21',\n",
       " '0 11 25',\n",
       " '14 21',\n",
       " '0 25',\n",
       " '12 21 23 25',\n",
       " '0 5',\n",
       " '21 23',\n",
       " '0 14 21',\n",
       " '0 21 22 25',\n",
       " '0 2 23',\n",
       " '0 20 23',\n",
       " '7 23',\n",
       " '0 21 25',\n",
       " '1',\n",
       " '6 25',\n",
       " '0 3',\n",
       " '20 23',\n",
       " '7 25',\n",
       " '0',\n",
       " '0',\n",
       " '21 25',\n",
       " '17 21 25',\n",
       " '0 2 21 25',\n",
       " '0 7 24',\n",
       " '0 25',\n",
       " '0 2 5',\n",
       " '18 21 25',\n",
       " '0 3 4 25',\n",
       " '0 25',\n",
       " '0 7 25',\n",
       " '18 19 25',\n",
       " '0 18 19',\n",
       " '0 3 5 24',\n",
       " '4 21 25',\n",
       " '0 12 21 25',\n",
       " '0 23 25',\n",
       " '23',\n",
       " '0 14 16',\n",
       " '0 18 25',\n",
       " '0 18 19 21 23 25',\n",
       " '0 4',\n",
       " '0 12 21 23 25',\n",
       " '2',\n",
       " '7',\n",
       " '1 14',\n",
       " '21 23',\n",
       " '7 11 25',\n",
       " '0 2',\n",
       " '0 25',\n",
       " '0 13 21 22 25',\n",
       " '0',\n",
       " '21 25',\n",
       " '14 16 17 18 25',\n",
       " '16 17 18 21',\n",
       " '12 21',\n",
       " '0 2',\n",
       " '0 4 21 25',\n",
       " '0 2',\n",
       " '0',\n",
       " '0',\n",
       " '3 5 25',\n",
       " '25',\n",
       " '6 23',\n",
       " '0 4',\n",
       " '0 25',\n",
       " '2 21 25',\n",
       " '0 2',\n",
       " '0 2 25',\n",
       " '7 20',\n",
       " '21',\n",
       " '23',\n",
       " '23',\n",
       " '0 2 11',\n",
       " '7 11',\n",
       " '3',\n",
       " '18 23',\n",
       " '0 2',\n",
       " '0 25',\n",
       " '14 25',\n",
       " '3 4',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '6',\n",
       " '4',\n",
       " '14 16',\n",
       " '0 5 21 25',\n",
       " '4',\n",
       " '0',\n",
       " '0 22',\n",
       " '0 21 25',\n",
       " '6 25',\n",
       " '0 13',\n",
       " '4',\n",
       " '0 1 25',\n",
       " '0 21 25',\n",
       " '21 25',\n",
       " '0 25',\n",
       " '2 25',\n",
       " '0 25',\n",
       " '7',\n",
       " '21 25',\n",
       " '0 7',\n",
       " '0 23',\n",
       " '0 18 19 23 25',\n",
       " '21 25',\n",
       " '2',\n",
       " '0',\n",
       " '21 25',\n",
       " '21',\n",
       " '0',\n",
       " '0 21 25',\n",
       " '2 7 25',\n",
       " '0 16 25',\n",
       " '0 7',\n",
       " '0 16 21 25',\n",
       " '0 7 21 25',\n",
       " '12 13',\n",
       " '0 2 3 25',\n",
       " '0 13 21 22 25',\n",
       " '21 25',\n",
       " '7',\n",
       " '0 21',\n",
       " '16 25',\n",
       " '3',\n",
       " '0 19',\n",
       " '0 3 25',\n",
       " '0 1 25',\n",
       " '0 11 25',\n",
       " '0 22',\n",
       " '19 25',\n",
       " '0 4',\n",
       " '7 23',\n",
       " '0 25',\n",
       " '21 23',\n",
       " '18 19',\n",
       " '0 5 21 25',\n",
       " '0 16 17 25',\n",
       " '0 16 17 18 21',\n",
       " '4 25',\n",
       " '0 14',\n",
       " '2 3 25',\n",
       " '19',\n",
       " '0 25',\n",
       " '18 19',\n",
       " '12',\n",
       " '0 21',\n",
       " '5 26',\n",
       " '0',\n",
       " '0',\n",
       " '7 11',\n",
       " '4',\n",
       " '0',\n",
       " '2 7 21',\n",
       " '2',\n",
       " '23',\n",
       " '14',\n",
       " '0 2 25',\n",
       " '0 11 12',\n",
       " '2 7 21 25',\n",
       " '21',\n",
       " '12 21 25',\n",
       " '0 25',\n",
       " '0 21 25',\n",
       " '0 25',\n",
       " '7',\n",
       " '0',\n",
       " '21 23',\n",
       " '2 7',\n",
       " '25',\n",
       " '0 21 25',\n",
       " '2 18 25',\n",
       " '0 25',\n",
       " '6 25',\n",
       " '0 25',\n",
       " '25',\n",
       " '0 25',\n",
       " '14 16 25',\n",
       " '0 5',\n",
       " '7 21 25',\n",
       " '24',\n",
       " '0 16 17 25',\n",
       " '7',\n",
       " '0',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '20 21 23 25 26',\n",
       " '11 23 25',\n",
       " '5 18 19 25',\n",
       " '21',\n",
       " '0 25',\n",
       " '21',\n",
       " '4',\n",
       " '23',\n",
       " '21',\n",
       " '0 7 19',\n",
       " '5 23',\n",
       " '0 13 22',\n",
       " '6 25',\n",
       " '21 22 25',\n",
       " '0 25',\n",
       " '5 19 25',\n",
       " '2 3',\n",
       " '2 14',\n",
       " '1 6 14',\n",
       " '0 25',\n",
       " '1 2',\n",
       " '23',\n",
       " '0 25',\n",
       " '6 21 25',\n",
       " '0 5 21',\n",
       " '0',\n",
       " '19',\n",
       " '0 4 7',\n",
       " '0 12 21',\n",
       " '0 25',\n",
       " '7 25',\n",
       " '14',\n",
       " '12 21',\n",
       " '5 19',\n",
       " '21',\n",
       " '23',\n",
       " '1 25',\n",
       " '0 25',\n",
       " '7',\n",
       " '0 21 23 25',\n",
       " '3 5',\n",
       " '0 12',\n",
       " '14',\n",
       " '23 25',\n",
       " '0 21',\n",
       " '5 21 22',\n",
       " '21 22',\n",
       " '0 25',\n",
       " '25',\n",
       " '0 12 21',\n",
       " '25',\n",
       " '0 5 7 18 21',\n",
       " '0 25',\n",
       " '3 5 25',\n",
       " '14 16 17',\n",
       " '0 7 18 21',\n",
       " '0 19',\n",
       " '5',\n",
       " '',\n",
       " '21 22',\n",
       " '13',\n",
       " '0',\n",
       " '2 23 25',\n",
       " '0 7',\n",
       " '11 12 21 23 25',\n",
       " '0 1 25',\n",
       " '7 25',\n",
       " '0 5 25',\n",
       " '25 26',\n",
       " '21 25',\n",
       " '14 21 25',\n",
       " '0 21 25',\n",
       " '0 2 3',\n",
       " '2',\n",
       " '5 21',\n",
       " '0 21 25',\n",
       " '1 4',\n",
       " '0 1 18 19 21 25',\n",
       " '0 25',\n",
       " '0 18 19',\n",
       " '0 4 5',\n",
       " '0 7 19',\n",
       " '0 16 22 25',\n",
       " '0 18 25',\n",
       " '4 26',\n",
       " '7',\n",
       " '5 25',\n",
       " '5 25',\n",
       " '21 22',\n",
       " '14 16 17 25',\n",
       " '0 21 22',\n",
       " '0 5 25',\n",
       " '7',\n",
       " '5 25',\n",
       " '7 18 19',\n",
       " '0 1 25',\n",
       " '0 21 25',\n",
       " '19 26',\n",
       " '4 18 19 25',\n",
       " '23 25',\n",
       " '1 2 25',\n",
       " '0 14 16',\n",
       " '22',\n",
       " '0 21 25',\n",
       " '0 25',\n",
       " '0 22',\n",
       " '0 21',\n",
       " '0 25',\n",
       " '0 16 25',\n",
       " '0 25',\n",
       " '6 23',\n",
       " '0 2 21',\n",
       " '4',\n",
       " '13',\n",
       " '0 25',\n",
       " '25',\n",
       " '0 12 21 23',\n",
       " '1 6 21 25',\n",
       " '0 5 14 16 25',\n",
       " '5',\n",
       " '0 23',\n",
       " '23',\n",
       " '14',\n",
       " '0 2 11 25',\n",
       " '5 25',\n",
       " '11',\n",
       " '0 7',\n",
       " '0 24',\n",
       " '3 24',\n",
       " '0 21',\n",
       " '0 25',\n",
       " '21',\n",
       " '1 21 25',\n",
       " '25',\n",
       " '0 21 25',\n",
       " '11 12 21 22 24',\n",
       " '0 2',\n",
       " '2 5',\n",
       " '0 21',\n",
       " '0 18',\n",
       " '21',\n",
       " '21 25',\n",
       " '0 25',\n",
       " '19',\n",
       " '0 3',\n",
       " '0 2 25',\n",
       " '0 23',\n",
       " '0 1 2',\n",
       " '0',\n",
       " '0 25',\n",
       " '0 13 22 25',\n",
       " '5',\n",
       " '0 18 21',\n",
       " '0 2 7',\n",
       " '0 1 2',\n",
       " '0 7 19 21',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '23',\n",
       " '7 13 19 25',\n",
       " '0 19 21 25',\n",
       " '25',\n",
       " '23 25',\n",
       " '21',\n",
       " '21 24 25',\n",
       " '0 13',\n",
       " '0 21 25',\n",
       " '0 25',\n",
       " '0 21 25',\n",
       " '0',\n",
       " '23',\n",
       " '0 2 21',\n",
       " '0 2',\n",
       " '0 25',\n",
       " '0 3 5',\n",
       " '21 25',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '23',\n",
       " '13 21 25',\n",
       " '0 25',\n",
       " '0 2 21',\n",
       " '14 25',\n",
       " '5 7',\n",
       " '11 12 14 21 25',\n",
       " '0 7 21 25',\n",
       " '18 19 21 22 25',\n",
       " '7',\n",
       " '7',\n",
       " '7',\n",
       " '18',\n",
       " '23',\n",
       " '0 5 21 25',\n",
       " '0 25',\n",
       " '0 16 17 25',\n",
       " '14 16 17 25',\n",
       " '25',\n",
       " '0 2 16',\n",
       " '21 25',\n",
       " '0 25',\n",
       " '11 23',\n",
       " '14 16 17 25',\n",
       " '0 16 21 22',\n",
       " '14 16 25',\n",
       " '14 25',\n",
       " '0 18 19 25',\n",
       " '11 23',\n",
       " '5',\n",
       " '2 7',\n",
       " '7 23',\n",
       " '0',\n",
       " '0 1',\n",
       " '0',\n",
       " '0 25',\n",
       " '25',\n",
       " '23 25',\n",
       " '0 16 25',\n",
       " '21 22 25 26',\n",
       " '0 2 3 25',\n",
       " '0 18 19 25',\n",
       " '12',\n",
       " '0 21',\n",
       " '0 16 25',\n",
       " '0 4 21',\n",
       " '0 5 18 21',\n",
       " '23',\n",
       " '4 18 19 25',\n",
       " '0 25',\n",
       " '0 16 23',\n",
       " '1 4 23',\n",
       " '11',\n",
       " '2',\n",
       " '0 23 25',\n",
       " '0 7',\n",
       " '5',\n",
       " '0',\n",
       " '14 16 17 18',\n",
       " '0 5 21 25',\n",
       " '0 11 21 23 25',\n",
       " '21 25',\n",
       " '0 13 14',\n",
       " '0 25',\n",
       " '25',\n",
       " '0 21 25',\n",
       " '0 11 16 21 25',\n",
       " '23 25',\n",
       " '25',\n",
       " '6 25',\n",
       " '0 21',\n",
       " '21 25',\n",
       " '6 21 25',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '0 1',\n",
       " '0 25',\n",
       " '2 3',\n",
       " '0 19',\n",
       " '0 5',\n",
       " '0 2 21',\n",
       " '0 25',\n",
       " '0',\n",
       " '0 19',\n",
       " '0 3 5',\n",
       " '7 9 10',\n",
       " '0 26',\n",
       " '0 25',\n",
       " '0 21 25',\n",
       " '19 21 25',\n",
       " '0 12 21',\n",
       " '0 5 25',\n",
       " '0 25',\n",
       " '14 16 17',\n",
       " '14 17 21 25',\n",
       " '0 2 21 25',\n",
       " '0 19',\n",
       " '6 25',\n",
       " '0 2',\n",
       " '21',\n",
       " '0 18 19',\n",
       " '21',\n",
       " '0 23',\n",
       " '0 17 18',\n",
       " '0 1 16',\n",
       " '19 25',\n",
       " '0 25',\n",
       " '0 5 12 21',\n",
       " '0 21 25',\n",
       " '0 11 14 16 17 25',\n",
       " '0 5',\n",
       " '11',\n",
       " '3 5 22 25',\n",
       " '0 2 3',\n",
       " '0 5',\n",
       " '6 11 25',\n",
       " '0',\n",
       " '0 25',\n",
       " '0 21',\n",
       " '23',\n",
       " '6 25',\n",
       " '0 6 23 25',\n",
       " '0 2 21 25',\n",
       " '16 21 22',\n",
       " '0 25',\n",
       " '0 26',\n",
       " '1 21',\n",
       " '0 23',\n",
       " '0 20 23 25',\n",
       " '0 25',\n",
       " '6',\n",
       " '0 2 18',\n",
       " '5',\n",
       " '23',\n",
       " '11 12 21 25',\n",
       " '2 7 25',\n",
       " '0 1 7',\n",
       " '6',\n",
       " '26',\n",
       " '0 21 24 25',\n",
       " '0 21 25',\n",
       " '0 7',\n",
       " '0 21 25',\n",
       " '0 13 22',\n",
       " '23',\n",
       " '25',\n",
       " '0 14 21 25',\n",
       " '4',\n",
       " '0 16 17 25',\n",
       " '0 5 7',\n",
       " '19 21 25',\n",
       " '0 3',\n",
       " '19 25',\n",
       " '0 21 25',\n",
       " '0 5',\n",
       " '22 26',\n",
       " '0 7',\n",
       " '14',\n",
       " '21 25',\n",
       " '7',\n",
       " '0 2 25',\n",
       " '7',\n",
       " '23 25',\n",
       " '0',\n",
       " '0 23 25',\n",
       " '0 21 25',\n",
       " '0 3 5',\n",
       " '0 13 21 25',\n",
       " '0 17 18 21 25',\n",
       " '0 2 7 25',\n",
       " '0 25',\n",
       " '0 6 25',\n",
       " '11 18 19 25',\n",
       " '0 2 4',\n",
       " '4',\n",
       " '17 18 21',\n",
       " '14 16 17',\n",
       " '0 21',\n",
       " '0 13 21 22 25',\n",
       " '19',\n",
       " '21 23',\n",
       " '6 21 25',\n",
       " '0 21 25',\n",
       " '21',\n",
       " '0 21',\n",
       " '1 21 25',\n",
       " '25',\n",
       " '0 2',\n",
       " '7',\n",
       " '0 1 25',\n",
       " '16 21',\n",
       " '2 4 26',\n",
       " '0 25',\n",
       " '14 16 25',\n",
       " '0 2 3',\n",
       " '0 17 21 25',\n",
       " '4 21 25',\n",
       " '4 21',\n",
       " '0 25',\n",
       " '0 19 25',\n",
       " '0',\n",
       " '0 18 19',\n",
       " '0 11',\n",
       " '25',\n",
       " '7 16 18',\n",
       " '12',\n",
       " '7',\n",
       " '0 18 23',\n",
       " '0 25',\n",
       " '0 7',\n",
       " '7',\n",
       " '13 21 25',\n",
       " '0 11 12 25',\n",
       " '0 4 22 25',\n",
       " '0 4',\n",
       " '0 5 21 22',\n",
       " '0',\n",
       " '0 1 4',\n",
       " '0 16 17 18 25',\n",
       " '0 25',\n",
       " '0 21',\n",
       " '11 21',\n",
       " '0 7 14 16',\n",
       " '21',\n",
       " '3',\n",
       " '4 6 21 25',\n",
       " '0 3 5',\n",
       " '2 3',\n",
       " '7 18 25',\n",
       " '23',\n",
       " '5',\n",
       " '0 25',\n",
       " '0 2',\n",
       " '0 1',\n",
       " '0 2',\n",
       " '0 2',\n",
       " '5 25',\n",
       " '0 1 25 26',\n",
       " '0 21 25',\n",
       " '0 25',\n",
       " '23 25',\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['Predicted'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submit\n",
    "# submit = pd.read_csv('../data/sample_submission.csv')\n",
    "# predicted = []\n",
    "# draw_predict = []\n",
    "# # model.load_weights('../cache/InceptionV3.h5')\n",
    "# for name in tqdm(submit['Id']):\n",
    "#     path = os.path.join('../data/test/', name)\n",
    "#     image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "#     score_predict = model.predict(image[np.newaxis])[0]\n",
    "#     draw_predict.append(score_predict)\n",
    "#     label_predict = np.arange(28)[score_predict>=0.2]\n",
    "#     str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "#     predicted.append(str_predict_label)\n",
    "\n",
    "# submit['Predicted'] = predicted\n",
    "# np.save('../cache/draw_predict_InceptionV3-8.npy', score_predict)\n",
    "# submit.to_csv('../submissions/submit_InceptionV3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('../submissions/sub21-a.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/1855095/how-to-create-a-zip-archive-of-a-directory\n",
    "def backup_project_as_zip(project_dir, zip_file):\n",
    "    assert(os.path.isdir(project_dir))\n",
    "    assert(os.path.isdir(os.path.dirname(zip_file)))\n",
    "    shutil.make_archive(zip_file.replace('.zip',''), 'zip', project_dir)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-04 19:10:02.025329\n"
     ]
    }
   ],
   "source": [
    "import datetime, shutil\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "PROJECT_PATH = '/home/watts/lal/Kaggle/kagglehp/scripts_nbs'\n",
    "backup_project_as_zip(PROJECT_PATH, '../cache/code.scripts_nbs.%s.zip'%now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 492k/492k [00:16<00:00, 31.0kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 421 ms, sys: 242 ms, total: 663 ms\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub21-a.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub21-a.csv   2018-11-04 13:40:31               complete  0.447        None          \r\n",
      "sub20-d.csv   2018-11-03 16:29:16               complete  0.463        None          \r\n",
      "sub20-c.csv   2018-11-03 16:28:02               complete  0.466        None          \r\n",
      "sub20-bb.csv  2018-11-03 16:27:02               complete  0.464        None          \r\n",
      "sub20-b.csv   2018-11-03 16:26:10               complete  0.459        None          \r\n",
      "sub20-a.csv   2018-11-03 16:24:53               complete  0.450        None          \r\n",
      "sub17-c.csv   2018-11-02 05:37:12               complete  0.463        None          \r\n",
      "sub17-bb.csv  2018-11-02 05:36:24               complete  0.464        None          \r\n",
      "sub17-b.csv   2018-11-02 05:35:30               complete  0.456        None          \r\n",
      "sub17-a.csv   2018-11-02 05:34:31               complete  0.448        None          \r\n",
      "sub15-e.csv   2018-10-31 17:17:32               complete  0.453        None          \r\n",
      "sub15-d.csv   2018-10-31 17:16:24               complete  0.461        None          \r\n",
      "sub15-c.csv   2018-10-31 17:15:49               complete  0.457        None          \r\n",
      "sub15-b.csv   2018-10-31 17:14:03               complete  0.454        None          \r\n",
      "sub15-a.csv   2018-10-31 17:13:02               complete  0.446        None          \r\n",
      "sub14-d.csv   2018-10-30 16:36:00               complete  0.464        None          \r\n",
      "sub14-c.csv   2018-10-30 16:35:09               complete  0.466        None          \r\n",
      "sub14-bb.csv  2018-10-30 16:34:35               complete  0.461        None          \r\n",
      "sub14-b.csv   2018-10-30 16:33:35               complete  0.459        None          \r\n",
      "sub14-a.csv   2018-10-30 16:32:37               complete  0.452        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(20)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 103129.81it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for line in tqdm(sub_class_preds):\n",
    "    label_predict = np.arange(28)[line>=0.25]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub21-b.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-04 19:10:55.093484\n"
     ]
    }
   ],
   "source": [
    "import datetime, shutil\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "PROJECT_PATH = '/home/watts/lal/Kaggle/kagglehp/scripts_nbs'\n",
    "backup_project_as_zip(PROJECT_PATH, '../cache/code.scripts_nbs.%s.zip'%now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 485k/485k [00:13<00:00, 37.2kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 324 ms, sys: 170 ms, total: 494 ms\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub21-b.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub21-b.csv   2018-11-04 13:41:21               complete  0.456        None          \r\n",
      "sub21-a.csv   2018-11-04 13:40:31               complete  0.447        None          \r\n",
      "sub20-d.csv   2018-11-03 16:29:16               complete  0.463        None          \r\n",
      "sub20-c.csv   2018-11-03 16:28:02               complete  0.466        None          \r\n",
      "sub20-bb.csv  2018-11-03 16:27:02               complete  0.464        None          \r\n",
      "sub20-b.csv   2018-11-03 16:26:10               complete  0.459        None          \r\n",
      "sub20-a.csv   2018-11-03 16:24:53               complete  0.450        None          \r\n",
      "sub17-c.csv   2018-11-02 05:37:12               complete  0.463        None          \r\n",
      "sub17-bb.csv  2018-11-02 05:36:24               complete  0.464        None          \r\n",
      "sub17-b.csv   2018-11-02 05:35:30               complete  0.456        None          \r\n",
      "sub17-a.csv   2018-11-02 05:34:31               complete  0.448        None          \r\n",
      "sub15-e.csv   2018-10-31 17:17:32               complete  0.453        None          \r\n",
      "sub15-d.csv   2018-10-31 17:16:24               complete  0.461        None          \r\n",
      "sub15-c.csv   2018-10-31 17:15:49               complete  0.457        None          \r\n",
      "sub15-b.csv   2018-10-31 17:14:03               complete  0.454        None          \r\n",
      "sub15-a.csv   2018-10-31 17:13:02               complete  0.446        None          \r\n",
      "sub14-d.csv   2018-10-30 16:36:00               complete  0.464        None          \r\n",
      "sub14-c.csv   2018-10-30 16:35:09               complete  0.466        None          \r\n",
      "sub14-bb.csv  2018-10-30 16:34:35               complete  0.461        None          \r\n",
      "sub14-b.csv   2018-10-30 16:33:35               complete  0.459        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(20)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {0.3:'bb', 0.35:'c', 0.4:'d', 0.45:'e', 0.5:'f'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 102201.68it/s]\n",
      "100%|██████████| 11702/11702 [00:00<00:00, 107528.82it/s]\n",
      "  0%|          | 0/11702 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub21-bb.csv\n",
      "../submissions/sub21-c.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 123287.51it/s]\n",
      "100%|██████████| 11702/11702 [00:00<00:00, 126258.22it/s]\n",
      "  0%|          | 0/11702 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub21-d.csv\n",
      "../submissions/sub21-e.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 11702/11702 [00:00<00:00, 129069.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub21-f.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for alpha in [0.3, 0.35, 0.4, 0.45, 0.5]:\n",
    "    predicted = []\n",
    "    for line in tqdm(sub_class_preds):\n",
    "        label_predict = np.arange(28)[line>=alpha]\n",
    "        str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "        predicted.append(str_predict_label)\n",
    "    submit['Predicted'] = predicted\n",
    "    name = '../submissions/sub21-' + d[alpha] + '.csv'\n",
    "    print(name)\n",
    "    submit.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 481k/481k [00:12<00:00, 39.5kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 298 ms, sys: 167 ms, total: 465 ms\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub21-bb.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub21-bb.csv  2018-11-04 13:41:58               complete  0.462        None          \r\n",
      "sub21-b.csv   2018-11-04 13:41:21               complete  0.456        None          \r\n",
      "sub21-a.csv   2018-11-04 13:40:31               complete  0.447        None          \r\n",
      "sub20-d.csv   2018-11-03 16:29:16               complete  0.463        None          \r\n",
      "sub20-c.csv   2018-11-03 16:28:02               complete  0.466        None          \r\n",
      "sub20-bb.csv  2018-11-03 16:27:02               complete  0.464        None          \r\n",
      "sub20-b.csv   2018-11-03 16:26:10               complete  0.459        None          \r\n",
      "sub20-a.csv   2018-11-03 16:24:53               complete  0.450        None          \r\n",
      "sub17-c.csv   2018-11-02 05:37:12               complete  0.463        None          \r\n",
      "sub17-bb.csv  2018-11-02 05:36:24               complete  0.464        None          \r\n",
      "sub17-b.csv   2018-11-02 05:35:30               complete  0.456        None          \r\n",
      "sub17-a.csv   2018-11-02 05:34:31               complete  0.448        None          \r\n",
      "sub15-e.csv   2018-10-31 17:17:32               complete  0.453        None          \r\n",
      "sub15-d.csv   2018-10-31 17:16:24               complete  0.461        None          \r\n",
      "sub15-c.csv   2018-10-31 17:15:49               complete  0.457        None          \r\n",
      "sub15-b.csv   2018-10-31 17:14:03               complete  0.454        None          \r\n",
      "sub15-a.csv   2018-10-31 17:13:02               complete  0.446        None          \r\n",
      "sub14-d.csv   2018-10-30 16:36:00               complete  0.464        None          \r\n",
      "sub14-c.csv   2018-10-30 16:35:09               complete  0.466        None          \r\n",
      "sub14-bb.csv  2018-10-30 16:34:35               complete  0.461        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(20)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 477k/477k [00:12<00:00, 37.7kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationfileName      date                 description  status    publicScore  privateScore  \n",
      "------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub21-c.csv   2018-11-04 13:42:36               complete  0.465        None          \n",
      "sub21-bb.csv  2018-11-04 13:41:58               complete  0.462        None          \n",
      "sub21-b.csv   2018-11-04 13:41:21               complete  0.456        None          \n",
      "sub21-a.csv   2018-11-04 13:40:31               complete  0.447        None          \n",
      "sub20-d.csv   2018-11-03 16:29:16               complete  0.463        None          \n",
      "sub20-c.csv   2018-11-03 16:28:02               complete  0.466        None          \n",
      "sub20-bb.csv  2018-11-03 16:27:02               complete  0.464        None          \n",
      "sub20-b.csv   2018-11-03 16:26:10               complete  0.459        None          \n",
      "sub20-a.csv   2018-11-03 16:24:53               complete  0.450        None          \n",
      "sub17-c.csv   2018-11-02 05:37:12               complete  0.463        None          \n",
      "sub17-bb.csv  2018-11-02 05:36:24               complete  0.464        None          \n",
      "sub17-b.csv   2018-11-02 05:35:30               complete  0.456        None          \n",
      "sub17-a.csv   2018-11-02 05:34:31               complete  0.448        None          \n",
      "sub15-e.csv   2018-10-31 17:17:32               complete  0.453        None          \n",
      "sub15-d.csv   2018-10-31 17:16:24               complete  0.461        None          \n",
      "sub15-c.csv   2018-10-31 17:15:49               complete  0.457        None          \n",
      "sub15-b.csv   2018-10-31 17:14:03               complete  0.454        None          \n",
      "sub15-a.csv   2018-10-31 17:13:02               complete  0.446        None          \n",
      "sub14-d.csv   2018-10-30 16:36:00               complete  0.464        None          \n",
      "sub14-c.csv   2018-10-30 16:35:09               complete  0.466        None          \n",
      "CPU times: user 389 ms, sys: 274 ms, total: 663 ms\n",
      "Wall time: 38.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub21-c.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(20)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 473k/473k [00:12<00:00, 39.8kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationfileName      date                 description  status    publicScore  privateScore  \n",
      "------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub21-d.csv   2018-11-04 13:43:13               complete  0.468        None          \n",
      "sub21-c.csv   2018-11-04 13:42:36               complete  0.465        None          \n",
      "sub21-bb.csv  2018-11-04 13:41:58               complete  0.462        None          \n",
      "sub21-b.csv   2018-11-04 13:41:21               complete  0.456        None          \n",
      "sub21-a.csv   2018-11-04 13:40:31               complete  0.447        None          \n",
      "sub20-d.csv   2018-11-03 16:29:16               complete  0.463        None          \n",
      "sub20-c.csv   2018-11-03 16:28:02               complete  0.466        None          \n",
      "sub20-bb.csv  2018-11-03 16:27:02               complete  0.464        None          \n",
      "sub20-b.csv   2018-11-03 16:26:10               complete  0.459        None          \n",
      "sub20-a.csv   2018-11-03 16:24:53               complete  0.450        None          \n",
      "sub17-c.csv   2018-11-02 05:37:12               complete  0.463        None          \n",
      "sub17-bb.csv  2018-11-02 05:36:24               complete  0.464        None          \n",
      "sub17-b.csv   2018-11-02 05:35:30               complete  0.456        None          \n",
      "sub17-a.csv   2018-11-02 05:34:31               complete  0.448        None          \n",
      "sub15-e.csv   2018-10-31 17:17:32               complete  0.453        None          \n",
      "sub15-d.csv   2018-10-31 17:16:24               complete  0.461        None          \n",
      "sub15-c.csv   2018-10-31 17:15:49               complete  0.457        None          \n",
      "sub15-b.csv   2018-10-31 17:14:03               complete  0.454        None          \n",
      "sub15-a.csv   2018-10-31 17:13:02               complete  0.446        None          \n",
      "sub14-d.csv   2018-10-30 16:36:00               complete  0.464        None          \n",
      "CPU times: user 358 ms, sys: 280 ms, total: 638 ms\n",
      "Wall time: 36.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub21-d.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(20)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub21-e.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(20)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub21-f.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(20)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31072, 28)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_class_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31072, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31072, 28)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = 28\n",
    "y_train = np.zeros(oof_class_preds.shape)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31072/31072 [00:00<00:00, 741901.66it/s]\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for labels in tqdm(data['Target'].str.split(' ')):\n",
    "#     print(labels)\n",
    "    for label in labels:\n",
    "        y_train[idx][int(label)] = 1\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 1., 0., 0.],\n",
       "       [1., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.6174081518752018\n",
      "0.01 0.6174081494907646\n",
      "0.1 0.6174079419635514\n",
      "0.5 0.6174049197331472\n",
      "0.75 0.6174023910597577\n",
      "1.0 0.6173997536697944\n",
      "------------------\n",
      "0.001 0.7042131450176463\n",
      "0.01 0.7042131449021258\n",
      "0.1 0.7042131341504188\n",
      "0.5 0.7042129258072632\n",
      "0.75 0.7042126913081677\n",
      "1.0 0.7042123865546952\n",
      "------------------\n",
      "0.001 0.6405293650278552\n",
      "0.01 0.6405293643199262\n",
      "0.1 0.6405293035770623\n",
      "0.5 0.6405284696126301\n",
      "0.75 0.6405278129689744\n",
      "1.0 0.6405271539313317\n",
      "------------------\n",
      "0.001 0.5206868082561187\n",
      "0.01 0.5206868064937693\n",
      "0.1 0.5206866550090807\n",
      "0.5 0.5206845661735036\n",
      "0.75 0.5206829204240566\n",
      "1.0 0.5206812723936773\n",
      "------------------\n",
      "0.001 0.6225629182679313\n",
      "0.01 0.6225628870674288\n",
      "0.1 0.6225602493284829\n",
      "0.5 0.6225265347174933\n",
      "0.75 0.6225024258166685\n",
      "1.0 0.6224802007531789\n",
      "------------------\n",
      "0.001 0.47625033808763034\n",
      "0.01 0.4762503310360723\n",
      "0.1 0.47624973372408597\n",
      "0.5 0.4762420258070793\n",
      "0.75 0.4762364393569697\n",
      "1.0 0.47623122503579396\n",
      "------------------\n",
      "0.001 0.37015383088158105\n",
      "0.01 0.37015383051625794\n",
      "0.1 0.37015379589885844\n",
      "0.5 0.370153120483382\n",
      "0.75 0.3701523989904022\n",
      "1.0 0.3701515220047934\n",
      "------------------\n",
      "0.001 0.6278499957612576\n",
      "0.01 0.6278499933196181\n",
      "0.1 0.6278497675145762\n",
      "0.5 0.6278457789099899\n",
      "0.75 0.6278419420023433\n",
      "1.0 0.627837666925348\n",
      "------------------\n",
      "0.001 0.23972549325385814\n",
      "0.01 0.23972546015299\n",
      "0.1 0.23972230652354265\n",
      "0.5 0.2396593031542591\n",
      "0.75 0.23959034757832143\n",
      "1.0 0.23950497349498\n",
      "------------------\n",
      "0.001 0.31331172323399825\n",
      "0.01 0.3133113149919019\n",
      "0.1 0.3132732609784179\n",
      "0.5 0.3125863700441769\n",
      "0.75 0.3119170265340445\n",
      "1.0 0.31116713928425954\n",
      "------------------\n",
      "0.001 0.2558623247962669\n",
      "0.01 0.255861948626774\n",
      "0.1 0.2558268879333644\n",
      "0.5 0.2551944259969803\n",
      "0.75 0.254578676028139\n",
      "1.0 0.2538894622998168\n",
      "------------------\n",
      "0.001 0.5968983087672983\n",
      "0.01 0.5968983084398413\n",
      "0.1 0.5968982782239397\n",
      "0.5 0.5968977292368627\n",
      "0.75 0.5968971633273094\n",
      "1.0 0.5968964809073818\n",
      "------------------\n",
      "0.001 0.49290871329017727\n",
      "0.01 0.49290871298539585\n",
      "0.1 0.4929086845398815\n",
      "0.5 0.49290813804804207\n",
      "0.75 0.49290753815710786\n",
      "1.0 0.49290677775529046\n",
      "------------------\n",
      "0.001 0.3787539379466851\n",
      "0.01 0.37875392012227116\n",
      "0.1 0.37875241060554954\n",
      "0.5 0.3787329277123833\n",
      "0.75 0.37871876766614176\n",
      "1.0 0.37870547804838306\n",
      "------------------\n",
      "0.001 0.7506973147934757\n",
      "0.01 0.7506973136319354\n",
      "0.1 0.7506972086330781\n",
      "0.5 0.7506954269694857\n",
      "0.75 0.7506936901241192\n",
      "1.0 0.7506916736611128\n",
      "------------------\n",
      "0.001 0.01983763392161686\n",
      "0.01 0.019837535325928135\n",
      "0.1 0.019828450862805713\n",
      "0.5 0.019665777176268495\n",
      "0.75 0.01950321718332293\n",
      "1.0 0.019314882518966292\n",
      "------------------\n",
      "0.001 0.1751297622786967\n",
      "0.01 0.17512974759033484\n",
      "0.1 0.17512846022244855\n",
      "0.5 0.17510915339406974\n",
      "0.75 0.17509248192232651\n",
      "1.0 0.17507468614395694\n",
      "------------------\n",
      "0.001 0.1667931781498947\n",
      "0.01 0.16679317509693092\n",
      "0.1 0.1667928880710675\n",
      "0.5 0.1667873460571777\n",
      "0.75 0.1667814074139947\n",
      "1.0 0.16677415822966601\n",
      "------------------\n",
      "0.001 0.3087568657202341\n",
      "0.01 0.3087568642759011\n",
      "0.1 0.3087567290489941\n",
      "0.5 0.3087542387474458\n",
      "0.75 0.30875175512550346\n",
      "1.0 0.3087489153085835\n",
      "------------------\n",
      "0.001 0.38082130482961174\n",
      "0.01 0.380821304671176\n",
      "0.1 0.38082128970689655\n",
      "0.5 0.38082100099719085\n",
      "0.75 0.38082069536892904\n",
      "1.0 0.3808203259166289\n",
      "------------------\n",
      "0.001 0.1538312914148493\n",
      "0.01 0.15383122528024018\n",
      "0.1 0.1538250686849436\n",
      "0.5 0.15371444612700225\n",
      "0.75 0.15360709884995571\n",
      "1.0 0.15348721808287602\n",
      "------------------\n",
      "0.001 0.48557824579010384\n",
      "0.01 0.48557824528334803\n",
      "0.1 0.4855781985932974\n",
      "0.5 0.48557738349917456\n",
      "0.75 0.48557660601055197\n",
      "1.0 0.48557574340418097\n",
      "------------------\n",
      "0.001 0.3646582386899322\n",
      "0.01 0.3646582372378981\n",
      "0.1 0.364658112376793\n",
      "0.5 0.36465637881526125\n",
      "0.75 0.3646549894024714\n",
      "1.0 0.364653566661458\n",
      "------------------\n",
      "0.001 0.6577178864298491\n",
      "0.01 0.6577178855069651\n",
      "0.1 0.6577177994042775\n",
      "0.5 0.6577162388191071\n",
      "0.75 0.657714710592422\n",
      "1.0 0.6577129906824652\n",
      "------------------\n",
      "0.001 0.48347736718545864\n",
      "0.01 0.4834773624770783\n",
      "0.1 0.4834769405428032\n",
      "0.5 0.4834700745036658\n",
      "0.75 0.48346367708679355\n",
      "1.0 0.483456474479732\n",
      "------------------\n",
      "0.001 0.43786458026883995\n",
      "0.01 0.43786457972979137\n",
      "0.1 0.4378645337127519\n",
      "0.5 0.4378639170133599\n",
      "0.75 0.4378634463465395\n",
      "1.0 0.43786298621777675\n",
      "------------------\n",
      "0.001 0.21837253521077882\n",
      "0.01 0.2183724949425644\n",
      "0.1 0.21836905357883252\n",
      "0.5 0.21832290898991366\n",
      "0.75 0.21828791269733872\n",
      "1.0 0.21825408221249284\n",
      "------------------\n",
      "0.001 0.0019063946880274463\n",
      "0.01 0.0019063937044974109\n",
      "0.1 0.0019063073388934093\n",
      "0.5 0.0019050204660372438\n",
      "0.75 0.0019039365114243976\n",
      "1.0 0.0019028140324179876\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "sub_ridge_preds = np.zeros(sub_class_preds.shape)\n",
    "from sklearn.linear_model import Ridge\n",
    "for cls in np.arange(n_classes):\n",
    "    for alpha in [0.001, 0.01, 0.1, 0.5, 0.75, 1.0]:\n",
    "        y = y_train[:, cls]\n",
    "        X = oof_class_preds\n",
    "        clf = Ridge(alpha=alpha)\n",
    "        clf.fit(X,y)\n",
    "        score_ = clf.score(X, y)\n",
    "        print(alpha, score_)\n",
    "    print('------------------')\n",
    "#         X_test = sub_class_preds[:, cls]\n",
    "#         preds_ = clf.predict(X_test)\n",
    "#         sub_ridge_preds[:,cls] = preds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ridge_preds = np.zeros(sub_class_preds.shape)\n",
    "from sklearn.linear_model import Ridge\n",
    "for cls in np.arange(n_classes):\n",
    "    y = y_train[:, cls]\n",
    "    X = oof_class_preds\n",
    "    clf = Ridge(alpha=0.1)\n",
    "    clf.fit(X,y)\n",
    "    X_test = sub_class_preds\n",
    "    preds_ = clf.predict(X_test)\n",
    "    sub_ridge_preds[:,cls] = preds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.38001867e-02, 1.59430779e-04, 9.98871672e-01, ...,\n",
       "        1.77784316e-03, 1.12266340e-04, 5.20836841e-09],\n",
       "       [2.47062426e-02, 2.68127583e-04, 7.85995722e-04, ...,\n",
       "        6.51888692e-01, 8.71524611e-04, 3.26655725e-05],\n",
       "       [8.41529155e-01, 2.72278007e-04, 3.96186303e-03, ...,\n",
       "        9.23864961e-01, 2.01543609e-03, 1.92988443e-05],\n",
       "       ...,\n",
       "       [6.59056642e-04, 5.12143007e-05, 3.49444263e-05, ...,\n",
       "        1.77463120e-03, 5.20864920e-08, 5.91902866e-09],\n",
       "       [5.01914832e-01, 9.99162483e-01, 2.73049554e-03, ...,\n",
       "        1.16331837e-02, 1.53418808e-04, 1.20560289e-06],\n",
       "       [5.07521251e-01, 3.52088286e-04, 3.01849514e-03, ...,\n",
       "        6.92711103e-01, 1.99362053e-03, 1.05607675e-06]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_class_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11702,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_class_preds[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.26424064e-01,  8.84581981e-03,  8.85833838e-01, ...,\n",
       "         3.20592330e-02,  4.54385428e-03, -6.92529375e-05],\n",
       "       [ 5.51528260e-02,  2.13827355e-03,  1.63897473e-02, ...,\n",
       "         5.38178358e-01,  1.06812025e-03,  1.39571143e-03],\n",
       "       [ 7.77233272e-01, -1.67743788e-03,  1.87791802e-02, ...,\n",
       "         7.51411388e-01, -1.45306321e-03,  1.27131495e-03],\n",
       "       ...,\n",
       "       [ 2.62432560e-02,  4.66381707e-04,  7.97892029e-03, ...,\n",
       "         2.92185935e-02,  3.12140745e-03,  1.76712611e-04],\n",
       "       [ 4.95625470e-01,  8.71181686e-01,  1.99755452e-02, ...,\n",
       "         3.70837162e-02,  3.03441473e-03,  3.03600332e-05],\n",
       "       [ 4.96285545e-01,  1.04218405e-02,  2.67882221e-02, ...,\n",
       "         5.85205684e-01,  4.37290352e-03,  1.52204091e-06]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_ridge_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 88242.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub12-g.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "alpha = 0.35\n",
    "for line in tqdm(sub_ridge_preds):\n",
    "    label_predict = np.arange(28)[line>=alpha]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "name = '../submissions/sub12-g.csv'\n",
    "print(name)\n",
    "submit.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.3.8)\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 372 ms, sys: 201 ms, total: 573 ms\n",
      "Wall time: 17.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub12-g.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.3.8)\r\n",
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \r\n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \r\n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \r\n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \r\n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \r\n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \r\n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \r\n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \r\n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \r\n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \r\n",
      "sub11-g.csv   2018-10-23 09:13:19               complete  0.347        None          \r\n",
      "sub11-f.csv   2018-10-23 09:11:15               complete  0.358        None          \r\n",
      "sub11-b.csv   2018-10-23 05:25:32               complete  0.437        None          \r\n",
      "sub12.csv     2018-10-23 05:18:36               complete  0.436        None          \r\n",
      "sub11.csv     2018-10-23 01:07:18               complete  0.431        None          \r\n",
      "sub10.csv     2018-10-22 17:16:40               complete  0.336        None          \r\n",
      "sub9.csv      2018-10-21 20:04:09               complete  0.098        None          \r\n",
      "sub9.csv      2018-10-21 19:44:17               complete  0.073        None          \r\n",
      "sub9.csv      2018-10-21 19:37:30               complete  0.043        None          \r\n",
      "sub9.csv      2018-10-21 19:27:51               complete  0.000        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 79991.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub12-h.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "alpha = 0.4\n",
    "for line in tqdm(sub_ridge_preds):\n",
    "    label_predict = np.arange(28)[line>=alpha]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "name = '../submissions/sub12-h.csv'\n",
    "print(name)\n",
    "submit.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.0 / client 1.3.8)\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationWarning: Looks like you're using an outdated API Version, please consider updating (server 1.5.0 / client 1.3.8)\n",
      "fileName      date                 description  status    publicScore  privateScore  \n",
      "------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \n",
      "sub11-g.csv   2018-10-23 09:13:19               complete  0.347        None          \n",
      "sub11-f.csv   2018-10-23 09:11:15               complete  0.358        None          \n",
      "sub11-b.csv   2018-10-23 05:25:32               complete  0.437        None          \n",
      "sub12.csv     2018-10-23 05:18:36               complete  0.436        None          \n",
      "sub11.csv     2018-10-23 01:07:18               complete  0.431        None          \n",
      "sub10.csv     2018-10-22 17:16:40               complete  0.336        None          \n",
      "sub9.csv      2018-10-21 20:04:09               complete  0.098        None          \n",
      "sub9.csv      2018-10-21 19:44:17               complete  0.073        None          \n",
      "sub9.csv      2018-10-21 19:37:30               complete  0.043        None          \n",
      "CPU times: user 328 ms, sys: 282 ms, total: 610 ms\n",
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub12-h.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.0 / client 1.3.8)\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationWarning: Looks like you're using an outdated API Version, please consider updating (server 1.5.0 / client 1.3.8)\n",
      "fileName      date                 description  status    publicScore  privateScore  \n",
      "------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub12-d.csv   2018-10-26 02:09:32               complete  0.466        None          \n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \n",
      "sub11-g.csv   2018-10-23 09:13:19               complete  0.347        None          \n",
      "sub11-f.csv   2018-10-23 09:11:15               complete  0.358        None          \n",
      "sub11-b.csv   2018-10-23 05:25:32               complete  0.437        None          \n",
      "sub12.csv     2018-10-23 05:18:36               complete  0.436        None          \n",
      "sub11.csv     2018-10-23 01:07:18               complete  0.431        None          \n",
      "sub10.csv     2018-10-22 17:16:40               complete  0.336        None          \n",
      "sub9.csv      2018-10-21 20:04:09               complete  0.098        None          \n",
      "sub9.csv      2018-10-21 19:44:17               complete  0.073        None          \n",
      "CPU times: user 352 ms, sys: 328 ms, total: 680 ms\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub12-d.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpg",
   "language": "python",
   "name": "hpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

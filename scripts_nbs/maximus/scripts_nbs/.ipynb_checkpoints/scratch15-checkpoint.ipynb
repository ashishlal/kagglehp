{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/mathormad/inceptionv3-baseline-lb-0-379/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) + (1 - K.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "path_to_train = '../data/train/'\n",
    "data = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_info = []\n",
    "for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "    train_dataset_info.append({\n",
    "        'path':os.path.join(path_to_train, name),\n",
    "        'labels':np.array([int(label) for label in labels])})\n",
    "train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'path': '../data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0', 'labels': array([16,  0])},\n",
       "       {'path': '../data/train/000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', 'labels': array([7, 1, 2, 0])},\n",
       "       {'path': '../data/train/000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([5])},\n",
       "       ...,\n",
       "       {'path': '../data/train/fff189d8-bbab-11e8-b2ba-ac1f6b6435d0', 'labels': array([7])},\n",
       "       {'path': '../data/train/fffdf7e0-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([25,  2, 21])},\n",
       "       {'path': '../data/train/fffe0ffe-bbc0-11e8-b2bb-ac1f6b6435d0', 'labels': array([2, 0])}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            dataset_info = shuffle(dataset_info)\n",
    "            for start in range(0, len(dataset_info), batch_size):\n",
    "                end = min(start + batch_size, len(dataset_info))\n",
    "                batch_images = []\n",
    "                X_train_batch = dataset_info[start:end]\n",
    "                batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "                for i in range(len(X_train_batch)):\n",
    "                    image = data_generator.load_image(\n",
    "                        X_train_batch[i]['path'], shape)   \n",
    "                    if augument:\n",
    "                        image = data_generator.augment(image)\n",
    "                    batch_images.append(image/255.)\n",
    "                    batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "                yield np.array(batch_images, np.float32), batch_labels\n",
    "\n",
    "    def load_image(path, shape):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image = np.stack((\n",
    "            np.array(image_red_ch),\n",
    "            np.array(image_green_ch), \n",
    "            np.array(image_blue_ch)), -1)\n",
    "        w, h = 512, 512\n",
    "        zero_data = np.zeros((h, w), dtype=np.uint8)\n",
    "#         image2 = np.stack((\n",
    "#             np.array(image_yellow_ch),\n",
    "#             zero_data, zero_data), -1)\n",
    "#         print(image1.shape, image2.shape)\n",
    "#         image = np.vstack((image1, image2))\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "\n",
    "    def augment(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=0),\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "            ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=input_shape)\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = base_model(bn)\n",
    "    x = Conv2D(32, kernel_size=(1,1), activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "# https://github.com/ailias/Focal-Loss-implement-on-Tensorflow/blob/master/focal_loss.py\n",
    "def focal_loss_org(prediction_tensor, target_tensor, weights=None, alpha=0.25, gamma=2):\n",
    "    r\"\"\"Compute focal loss for predictions.\n",
    "        Multi-labels Focal loss formula:\n",
    "            FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                 ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "    Args:\n",
    "     prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing the predicted logits for each class\n",
    "     target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing one-hot encoded classification targets\n",
    "     weights: A float tensor of shape [batch_size, num_anchors]\n",
    "     alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "     gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "    Returns:\n",
    "        loss: A (scalar) tensor representing the value of the loss function\n",
    "    \"\"\"\n",
    "    sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                          - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(weights=None, alpha=0.25, gamma=2):\n",
    "    def focal_loss_my(target_tensor, prediction_tensor, ):\n",
    "        r\"\"\"Compute focal loss for predictions.\n",
    "            Multi-labels Focal loss formula:\n",
    "                FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                     ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "        Args:\n",
    "         prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "            num_classes] representing the predicted logits for each class\n",
    "         target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "            num_classes] representing one-hot encoded classification targets\n",
    "         weights: A float tensor of shape [batch_size, num_anchors]\n",
    "         alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "         gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "        Returns:\n",
    "            loss: A (scalar) tensor representing the value of the loss function\n",
    "        \"\"\"\n",
    "        sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "        zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "\n",
    "        # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "        # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "        pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "\n",
    "        # For negative prediction, only need consider back part loss, front part is 0;\n",
    "        # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "        neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "        per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                              - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "        return tf.reduce_sum(per_entry_cross_ent)\n",
    "#         return K.mean(K.binary_crossentropy(target_tensor, prediction_tensor), axis=-1) + tf.reduce_sum(per_entry_cross_ent)\n",
    "    return focal_loss_my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_fixed(y_true, y_pred):\n",
    "    gamma = 2.\n",
    "    alpha = 0.25\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "#     pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "#     pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def focal_loss(gamma=2., alpha=.25):\n",
    "#     def focal_loss_fixed(y_true, y_pred):\n",
    "#         pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "#         pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "#         pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "#         pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "#         return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "#     return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31072,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31072/31072 [00:00<00:00, 602399.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31072, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_classes = 28\n",
    "y_train = np.zeros((train_dataset_info.shape[0], n_classes))\n",
    "print(y_train.shape)\n",
    "\n",
    "idx = 0\n",
    "for labels in tqdm(data['Target'].str.split(' ')):\n",
    "#     print(labels)\n",
    "    for label in labels:\n",
    "        y_train[idx][int(label)] = 1\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27958 3114\n",
      "27973 3099\n",
      "27969 3103\n",
      "Epoch 1/2\n",
      "1749/1749 [==============================] - 1523s 871ms/step - loss: 1.1180 - f1: 0.0402 - val_loss: 1.1985 - val_f1: 0.0329\n",
      "Epoch 2/2\n",
      "1749/1749 [==============================] - 324s 186ms/step - loss: 1.1038 - f1: 0.0513 - val_loss: 1.1325 - val_f1: 0.0272\n",
      "Epoch 1/120\n",
      "1749/1749 [==============================] - 414s 236ms/step - loss: 1.0478 - f1: 0.1068 - val_loss: 0.9834 - val_f1: 0.1947\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.98341, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1749/1749 [==============================] - 399s 228ms/step - loss: 0.9747 - f1: 0.1745 - val_loss: 0.9182 - val_f1: 0.2328\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.98341 to 0.91816, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1749/1749 [==============================] - 397s 227ms/step - loss: 0.9331 - f1: 0.2088 - val_loss: 0.9284 - val_f1: 0.2466\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.91816\n",
      "Epoch 4/120\n",
      "1749/1749 [==============================] - 399s 228ms/step - loss: 0.9071 - f1: 0.2275 - val_loss: 0.8520 - val_f1: 0.2996\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.91816 to 0.85196, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 5/120\n",
      "1749/1749 [==============================] - 398s 228ms/step - loss: 0.8863 - f1: 0.2429 - val_loss: 0.8343 - val_f1: 0.3148\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.85196 to 0.83426, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1749/1749 [==============================] - 399s 228ms/step - loss: 0.8758 - f1: 0.2497 - val_loss: 0.8538 - val_f1: 0.2999\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.83426\n",
      "Epoch 7/120\n",
      "1749/1749 [==============================] - 399s 228ms/step - loss: 0.8620 - f1: 0.2588 - val_loss: 0.8085 - val_f1: 0.3309\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.83426 to 0.80847, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 8/120\n",
      "1749/1749 [==============================] - 399s 228ms/step - loss: 0.8528 - f1: 0.2651 - val_loss: 0.8548 - val_f1: 0.2898\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.80847\n",
      "Epoch 9/120\n",
      "1749/1749 [==============================] - 398s 228ms/step - loss: 0.8416 - f1: 0.2724 - val_loss: 0.7903 - val_f1: 0.3400\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.80847 to 0.79032, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 10/120\n",
      "1749/1749 [==============================] - 398s 228ms/step - loss: 0.8364 - f1: 0.2764 - val_loss: 0.8089 - val_f1: 0.3280\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.79032\n",
      "Epoch 11/120\n",
      "1749/1749 [==============================] - 400s 229ms/step - loss: 0.8283 - f1: 0.2813 - val_loss: 0.7762 - val_f1: 0.3523\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.79032 to 0.77619, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 12/120\n",
      "1749/1749 [==============================] - 401s 230ms/step - loss: 0.8214 - f1: 0.2858 - val_loss: 0.7712 - val_f1: 0.3570\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.77619 to 0.77118, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 13/120\n",
      "1749/1749 [==============================] - 399s 228ms/step - loss: 0.8138 - f1: 0.2906 - val_loss: 0.7726 - val_f1: 0.3523\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.77118\n",
      "Epoch 14/120\n",
      "1749/1749 [==============================] - 399s 228ms/step - loss: 0.8082 - f1: 0.2946 - val_loss: 0.8046 - val_f1: 0.3314\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.77118\n",
      "Epoch 15/120\n",
      "1749/1749 [==============================] - 399s 228ms/step - loss: 0.8034 - f1: 0.2975 - val_loss: 0.7806 - val_f1: 0.3456\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.77118\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 16/120\n",
      "1749/1749 [==============================] - 399s 228ms/step - loss: 0.7803 - f1: 0.3110 - val_loss: 0.7291 - val_f1: 0.3859\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.77118 to 0.72914, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 17/120\n",
      "1749/1749 [==============================] - 399s 228ms/step - loss: 0.7699 - f1: 0.3182 - val_loss: 0.7322 - val_f1: 0.3869\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.72914\n",
      "Epoch 18/120\n",
      "1749/1749 [==============================] - 399s 228ms/step - loss: 0.7643 - f1: 0.3213 - val_loss: 0.7332 - val_f1: 0.3853\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.72914\n",
      "Epoch 19/120\n",
      "1749/1749 [==============================] - 398s 227ms/step - loss: 0.7614 - f1: 0.3230 - val_loss: 0.7356 - val_f1: 0.3815\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.72914\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 20/120\n",
      "1749/1749 [==============================] - 390s 223ms/step - loss: 0.7581 - f1: 0.3246 - val_loss: 0.7321 - val_f1: 0.3850\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.72914\n",
      "Epoch 21/120\n",
      "1749/1749 [==============================] - 389s 222ms/step - loss: 0.7561 - f1: 0.3261 - val_loss: 0.7344 - val_f1: 0.3827\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.72914\n",
      "Epoch 22/120\n",
      "1749/1749 [==============================] - 390s 223ms/step - loss: 0.7566 - f1: 0.3248 - val_loss: 0.7306 - val_f1: 0.3871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3103 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00022: val_loss did not improve from 0.72914\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3103/3103 [02:45<00:00, 18.71it/s]\n",
      "11702it [20:02,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27995 3077\n",
      "Epoch 1/2\n",
      "1750/1750 [==============================] - 333s 190ms/step - loss: 1.1175 - f1: 0.0407 - val_loss: 1.1790 - val_f1: 0.0365\n",
      "Epoch 2/2\n",
      "1750/1750 [==============================] - 324s 185ms/step - loss: 1.1043 - f1: 0.0514 - val_loss: 1.1739 - val_f1: 0.0305\n",
      "Epoch 1/120\n",
      "1750/1750 [==============================] - 415s 237ms/step - loss: 1.0517 - f1: 0.1021 - val_loss: 1.0347 - val_f1: 0.1529\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03473, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1750/1750 [==============================] - 397s 227ms/step - loss: 0.9839 - f1: 0.1652 - val_loss: 0.9320 - val_f1: 0.2228\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03473 to 0.93204, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1750/1750 [==============================] - 395s 226ms/step - loss: 0.9394 - f1: 0.2026 - val_loss: 0.9169 - val_f1: 0.2498\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.93204 to 0.91691, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 4/120\n",
      "1750/1750 [==============================] - 393s 224ms/step - loss: 0.9117 - f1: 0.2244 - val_loss: 0.8814 - val_f1: 0.2711\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.91691 to 0.88138, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 5/120\n",
      "1750/1750 [==============================] - 391s 224ms/step - loss: 0.8948 - f1: 0.2365 - val_loss: 0.8510 - val_f1: 0.2961\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.88138 to 0.85098, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1750/1750 [==============================] - 391s 224ms/step - loss: 0.8814 - f1: 0.2453 - val_loss: 0.8799 - val_f1: 0.2762\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.85098\n",
      "Epoch 7/120\n",
      "1750/1750 [==============================] - 392s 224ms/step - loss: 0.8699 - f1: 0.2535 - val_loss: 0.8169 - val_f1: 0.3159\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.85098 to 0.81692, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 8/120\n",
      "1750/1750 [==============================] - 391s 224ms/step - loss: 0.8588 - f1: 0.2603 - val_loss: 0.8310 - val_f1: 0.3160\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.81692\n",
      "Epoch 9/120\n",
      "1750/1750 [==============================] - 391s 224ms/step - loss: 0.8488 - f1: 0.2666 - val_loss: 0.7981 - val_f1: 0.3431\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.81692 to 0.79812, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 10/120\n",
      "1750/1750 [==============================] - 392s 224ms/step - loss: 0.8429 - f1: 0.2707 - val_loss: 0.8146 - val_f1: 0.3278\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.79812\n",
      "Epoch 11/120\n",
      "1750/1750 [==============================] - 391s 223ms/step - loss: 0.8319 - f1: 0.2781 - val_loss: 0.7879 - val_f1: 0.3488\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.79812 to 0.78785, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 12/120\n",
      "1750/1750 [==============================] - 392s 224ms/step - loss: 0.8265 - f1: 0.2826 - val_loss: 0.7995 - val_f1: 0.3474\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.78785\n",
      "Epoch 13/120\n",
      "1750/1750 [==============================] - 391s 224ms/step - loss: 0.8181 - f1: 0.2885 - val_loss: 0.7682 - val_f1: 0.3542\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.78785 to 0.76816, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 14/120\n",
      "1750/1750 [==============================] - 392s 224ms/step - loss: 0.8127 - f1: 0.2913 - val_loss: 0.7665 - val_f1: 0.3655\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.76816 to 0.76654, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 15/120\n",
      "1750/1750 [==============================] - 391s 224ms/step - loss: 0.8081 - f1: 0.2941 - val_loss: 0.7794 - val_f1: 0.3550\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.76654\n",
      "Epoch 16/120\n",
      "1750/1750 [==============================] - 391s 224ms/step - loss: 0.8040 - f1: 0.2976 - val_loss: 0.7735 - val_f1: 0.3577\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.76654\n",
      "Epoch 17/120\n",
      "1750/1750 [==============================] - 391s 224ms/step - loss: 0.7979 - f1: 0.3007 - val_loss: 0.7775 - val_f1: 0.3568\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.76654\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 18/120\n",
      "1750/1750 [==============================] - 391s 223ms/step - loss: 0.7733 - f1: 0.3160 - val_loss: 0.7255 - val_f1: 0.3895\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.76654 to 0.72554, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 19/120\n",
      "1750/1750 [==============================] - 391s 224ms/step - loss: 0.7643 - f1: 0.3211 - val_loss: 0.7266 - val_f1: 0.3876\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.72554\n",
      "Epoch 20/120\n",
      "1750/1750 [==============================] - 388s 222ms/step - loss: 0.7616 - f1: 0.3225 - val_loss: 0.7204 - val_f1: 0.3941\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.72554 to 0.72037, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 21/120\n",
      "1750/1750 [==============================] - 398s 228ms/step - loss: 0.7567 - f1: 0.3259 - val_loss: 0.7253 - val_f1: 0.3870\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.72037\n",
      "Epoch 22/120\n",
      "1750/1750 [==============================] - 395s 226ms/step - loss: 0.7546 - f1: 0.3271 - val_loss: 0.7229 - val_f1: 0.3893\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.72037\n",
      "Epoch 23/120\n",
      "1750/1750 [==============================] - 394s 225ms/step - loss: 0.7520 - f1: 0.3281 - val_loss: 0.7228 - val_f1: 0.3898\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.72037\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 24/120\n",
      "1750/1750 [==============================] - 395s 226ms/step - loss: 0.7496 - f1: 0.3303 - val_loss: 0.7216 - val_f1: 0.3927\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.72037\n",
      "Epoch 25/120\n",
      "1750/1750 [==============================] - 394s 225ms/step - loss: 0.7475 - f1: 0.3314 - val_loss: 0.7242 - val_f1: 0.3904\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.72037\n",
      "Epoch 26/120\n",
      "1750/1750 [==============================] - 394s 225ms/step - loss: 0.7481 - f1: 0.3310 - val_loss: 0.7219 - val_f1: 0.3910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3077 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00026: val_loss did not improve from 0.72037\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3077/3077 [02:42<00:00, 18.90it/s]\n",
      "11702it [08:10, 24.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27928 3144\n",
      "Epoch 1/2\n",
      "1746/1746 [==============================] - 333s 190ms/step - loss: 1.1190 - f1: 0.0394 - val_loss: 1.1304 - val_f1: 0.0292\n",
      "Epoch 2/2\n",
      "1746/1746 [==============================] - 322s 185ms/step - loss: 1.1049 - f1: 0.0502 - val_loss: 1.1549 - val_f1: 0.0227\n",
      "Epoch 1/120\n",
      "1746/1746 [==============================] - 405s 232ms/step - loss: 1.0552 - f1: 0.0992 - val_loss: 1.0144 - val_f1: 0.1589\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01443, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1746/1746 [==============================] - 383s 219ms/step - loss: 0.9849 - f1: 0.1632 - val_loss: 0.9488 - val_f1: 0.2134\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01443 to 0.94883, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1746/1746 [==============================] - 382s 219ms/step - loss: 0.9362 - f1: 0.2067 - val_loss: 0.9174 - val_f1: 0.2545\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.94883 to 0.91744, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 4/120\n",
      "1746/1746 [==============================] - 382s 219ms/step - loss: 0.9111 - f1: 0.2249 - val_loss: 0.8462 - val_f1: 0.2932\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.91744 to 0.84617, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 5/120\n",
      "1746/1746 [==============================] - 382s 219ms/step - loss: 0.8890 - f1: 0.2410 - val_loss: 0.8366 - val_f1: 0.3097\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.84617 to 0.83660, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1746/1746 [==============================] - 393s 225ms/step - loss: 0.8734 - f1: 0.2510 - val_loss: 0.8437 - val_f1: 0.3179\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.83660\n",
      "Epoch 7/120\n",
      "1746/1746 [==============================] - 392s 225ms/step - loss: 0.8627 - f1: 0.2585 - val_loss: 0.8297 - val_f1: 0.3147\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.83660 to 0.82972, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 8/120\n",
      "1746/1746 [==============================] - 391s 224ms/step - loss: 0.8514 - f1: 0.2654 - val_loss: 0.8145 - val_f1: 0.3299\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.82972 to 0.81449, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 9/120\n",
      "1746/1746 [==============================] - 390s 224ms/step - loss: 0.8457 - f1: 0.2697 - val_loss: 0.8331 - val_f1: 0.3179\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.81449\n",
      "Epoch 10/120\n",
      "1746/1746 [==============================] - 390s 223ms/step - loss: 0.8384 - f1: 0.2746 - val_loss: 0.7808 - val_f1: 0.3517\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.81449 to 0.78078, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 11/120\n",
      "1746/1746 [==============================] - 391s 224ms/step - loss: 0.8295 - f1: 0.2814 - val_loss: 0.8217 - val_f1: 0.3121\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.78078\n",
      "Epoch 12/120\n",
      "1746/1746 [==============================] - 390s 223ms/step - loss: 0.8228 - f1: 0.2848 - val_loss: 0.7900 - val_f1: 0.3456\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.78078\n",
      "Epoch 13/120\n",
      "1746/1746 [==============================] - 391s 224ms/step - loss: 0.8164 - f1: 0.2897 - val_loss: 0.7848 - val_f1: 0.3458\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.78078\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 14/120\n",
      "1746/1746 [==============================] - 389s 223ms/step - loss: 0.7926 - f1: 0.3033 - val_loss: 0.7437 - val_f1: 0.3772\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.78078 to 0.74367, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 15/120\n",
      "1746/1746 [==============================] - 390s 223ms/step - loss: 0.7837 - f1: 0.3097 - val_loss: 0.7368 - val_f1: 0.3802\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.74367 to 0.73676, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 16/120\n",
      "1746/1746 [==============================] - 390s 223ms/step - loss: 0.7787 - f1: 0.3127 - val_loss: 0.7423 - val_f1: 0.3717\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.73676\n",
      "Epoch 17/120\n",
      "1746/1746 [==============================] - 390s 223ms/step - loss: 0.7750 - f1: 0.3149 - val_loss: 0.7403 - val_f1: 0.3774\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.73676\n",
      "Epoch 18/120\n",
      "1746/1746 [==============================] - 390s 224ms/step - loss: 0.7732 - f1: 0.3151 - val_loss: 0.7364 - val_f1: 0.3806\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.73676 to 0.73639, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 19/120\n",
      "1746/1746 [==============================] - 390s 224ms/step - loss: 0.7699 - f1: 0.3177 - val_loss: 0.7352 - val_f1: 0.3822\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.73639 to 0.73520, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 20/120\n",
      "1746/1746 [==============================] - 391s 224ms/step - loss: 0.7674 - f1: 0.3193 - val_loss: 0.7382 - val_f1: 0.3794\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.73520\n",
      "Epoch 21/120\n",
      "1746/1746 [==============================] - 390s 223ms/step - loss: 0.7655 - f1: 0.3199 - val_loss: 0.7393 - val_f1: 0.3799\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.73520\n",
      "Epoch 22/120\n",
      "1746/1746 [==============================] - 389s 223ms/step - loss: 0.7606 - f1: 0.3228 - val_loss: 0.7379 - val_f1: 0.3788\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.73520\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 23/120\n",
      "1746/1746 [==============================] - 390s 223ms/step - loss: 0.7584 - f1: 0.3246 - val_loss: 0.7321 - val_f1: 0.3845\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.73520 to 0.73207, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 24/120\n",
      "1746/1746 [==============================] - 391s 224ms/step - loss: 0.7586 - f1: 0.3241 - val_loss: 0.7359 - val_f1: 0.3802\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.73207\n",
      "Epoch 25/120\n",
      "1746/1746 [==============================] - 390s 223ms/step - loss: 0.7570 - f1: 0.3248 - val_loss: 0.7329 - val_f1: 0.3824\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.73207\n",
      "Epoch 26/120\n",
      "1746/1746 [==============================] - 389s 223ms/step - loss: 0.7587 - f1: 0.3235 - val_loss: 0.7333 - val_f1: 0.3836\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.73207\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 27/120\n",
      "1746/1746 [==============================] - 389s 223ms/step - loss: 0.7562 - f1: 0.3257 - val_loss: 0.7362 - val_f1: 0.3816\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.73207\n",
      "Epoch 28/120\n",
      "1746/1746 [==============================] - 389s 223ms/step - loss: 0.7543 - f1: 0.3267 - val_loss: 0.7388 - val_f1: 0.3786\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.73207\n",
      "Epoch 29/120\n",
      "1746/1746 [==============================] - 389s 223ms/step - loss: 0.7580 - f1: 0.3232 - val_loss: 0.7335 - val_f1: 0.3840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3144 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00029: val_loss did not improve from 0.73207\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3144/3144 [03:04<00:00, 20.41it/s]\n",
      "11702it [08:20, 23.40it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split data into train, valid\n",
    "indexes = np.arange(train_dataset_info.shape[0])\n",
    "# np.random.shuffle(indexes)\n",
    "# train_indexes, valid_indexes = train_test_split(indexes, test_size=0.15, random_state=8)\n",
    "n_splits = 5\n",
    "# kf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "kf = MultilabelStratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "# train_generator = data_generator.create_train(\n",
    "#     train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "# validation_generator = data_generator.create_train(\n",
    "#     train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)\n",
    "\n",
    "oof_class_preds = np.zeros((train_dataset_info.shape[0], 28))\n",
    "sub_class_preds = np.zeros((submit.shape[0], 28))\n",
    "\n",
    "# oof_class_preds = np.load('../cache/oof_class_preds-14.npy')\n",
    "# sub_class_preds = np.load('../cache/sub_class_preds-14.npy')\n",
    "                          \n",
    "fold_ = 0\n",
    "epochs = 10; batch_size = 16\n",
    "my_fold = 0\n",
    "for train_indexes, valid_indexes in kf.split(indexes, y_train):\n",
    "    print(len(train_indexes), len(valid_indexes))\n",
    "#     if my_fold < 2:\n",
    "#         my_fold += 1\n",
    "#         continue\n",
    "    checkpoint = ModelCheckpoint('../cache/InceptionV3.h5', monitor='val_loss', verbose=1, \n",
    "                                 save_best_only=True, mode='min', save_weights_only = True)\n",
    "    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
    "                                       verbose=1, mode='auto', epsilon=0.0001)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", \n",
    "                          mode=\"min\", \n",
    "                          patience=6)\n",
    "    callbacks_list = [checkpoint, early, reduceLROnPlat]\n",
    "\n",
    "\n",
    "    _preds = []\n",
    "    # create train and valid datagens\n",
    "    train_generator = data_generator.create_train(\n",
    "        train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "    validation_generator = data_generator.create_train(\n",
    "        train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)\n",
    "\n",
    "    # warm up model\n",
    "    model = create_model(\n",
    "        input_shape=(SIZE,SIZE,3), \n",
    "        n_out=28)\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.layers[-1].trainable = True\n",
    "    model.layers[-2].trainable = True\n",
    "    model.layers[-3].trainable = True\n",
    "    model.layers[-4].trainable = True\n",
    "    model.layers[-5].trainable = True\n",
    "    model.layers[-6].trainable = True\n",
    "    \n",
    "    model.compile(\n",
    "        loss=f1_loss, \n",
    "        optimizer=Adam(1e-03),\n",
    "        metrics=[f1])\n",
    "#     model.summary()\n",
    "\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "        epochs=2, \n",
    "        verbose=1)\n",
    "    \n",
    "    # train all layers\n",
    "    epochs=120\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    model.compile(loss=f1_loss,\n",
    "                optimizer=Adam(lr=1e-4),\n",
    "                metrics=[f1])\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "        epochs=epochs, \n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list)\n",
    "    \n",
    "    for idx in tqdm(valid_indexes):\n",
    "        item = train_dataset_info[idx]\n",
    "        path = item['path']\n",
    "        labels = item['labels']\n",
    "        image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "        score_predict = model.predict(image[np.newaxis])[0]\n",
    "        oof_class_preds[idx] = score_predict\n",
    "        np.save('../cache/oof_class_preds-15.npy', oof_class_preds)\n",
    "        \n",
    "    for idx, name in tqdm(enumerate(submit['Id'])):\n",
    "        path = os.path.join('../data/test/', name)\n",
    "        image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "        score_predict = model.predict(image[np.newaxis])[0]\n",
    "        sub_class_preds[idx] += score_predict\n",
    "        np.save('../cache/sub_class_preds-15.npy', sub_class_preds)\n",
    "    fold_ += 1\n",
    "sub_class_preds /= n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27958 3114\n",
    "# Epoch 1/2\n",
    "# 1748/1748 [==============================] - 1797s 1s/step - loss: 1.1195 - f1: 0.0376 - val_loss: 1.1698 - val_f1: 0.0320\n",
    "# Epoch 2/2\n",
    "# 1748/1748 [==============================] - 325s 186ms/step - loss: 1.1067 - f1: 0.0476 - val_loss: 1.1549 - val_f1: 0.0308\n",
    "# Epoch 1/120\n",
    "# 1748/1748 [==============================] - 413s 236ms/step - loss: 1.0568 - f1: 0.0977 - val_loss: 0.9852 - val_f1: 0.1725\n",
    "\n",
    "# Epoch 00001: val_loss improved from inf to 0.98515, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 2/120\n",
    "# 1748/1748 [==============================] - 396s 227ms/step - loss: 0.9804 - f1: 0.1686 - val_loss: 0.9657 - val_f1: 0.2039\n",
    "\n",
    "# Epoch 00002: val_loss improved from 0.98515 to 0.96568, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 3/120\n",
    "# 1748/1748 [==============================] - 401s 229ms/step - loss: 0.9326 - f1: 0.2086 - val_loss: 0.8926 - val_f1: 0.2645\n",
    "\n",
    "# Epoch 00003: val_loss improved from 0.96568 to 0.89257, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 4/120\n",
    "# 1748/1748 [==============================] - 403s 230ms/step - loss: 0.9055 - f1: 0.2302 - val_loss: 0.8653 - val_f1: 0.2878\n",
    "\n",
    "# Epoch 00004: val_loss improved from 0.89257 to 0.86533, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 5/120\n",
    "# 1748/1748 [==============================] - 406s 232ms/step - loss: 0.8883 - f1: 0.2408 - val_loss: 0.8512 - val_f1: 0.3025\n",
    "\n",
    "# Epoch 00005: val_loss improved from 0.86533 to 0.85124, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 6/120\n",
    "# 1748/1748 [==============================] - 407s 233ms/step - loss: 0.8755 - f1: 0.2495 - val_loss: 0.8227 - val_f1: 0.3191\n",
    "\n",
    "# Epoch 00006: val_loss improved from 0.85124 to 0.82271, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 7/120\n",
    "# 1748/1748 [==============================] - 404s 231ms/step - loss: 0.8642 - f1: 0.2577 - val_loss: 0.8060 - val_f1: 0.3284\n",
    "\n",
    "# Epoch 00007: val_loss improved from 0.82271 to 0.80597, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 8/120\n",
    "# 1748/1748 [==============================] - 404s 231ms/step - loss: 0.8521 - f1: 0.2657 - val_loss: 0.8056 - val_f1: 0.3273\n",
    "\n",
    "# Epoch 00008: val_loss improved from 0.80597 to 0.80562, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 9/120\n",
    "# 1748/1748 [==============================] - 402s 230ms/step - loss: 0.8428 - f1: 0.2711 - val_loss: 0.7919 - val_f1: 0.3408\n",
    "\n",
    "# Epoch 00009: val_loss improved from 0.80562 to 0.79187, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 10/120\n",
    "# 1748/1748 [==============================] - 399s 228ms/step - loss: 0.8353 - f1: 0.2757 - val_loss: 0.8004 - val_f1: 0.3347\n",
    "\n",
    "# Epoch 00010: val_loss did not improve from 0.79187\n",
    "# Epoch 11/120\n",
    "# 1748/1748 [==============================] - 400s 229ms/step - loss: 0.8286 - f1: 0.2815 - val_loss: 0.7983 - val_f1: 0.3329\n",
    "\n",
    "# Epoch 00011: val_loss did not improve from 0.79187\n",
    "# Epoch 12/120\n",
    "# 1748/1748 [==============================] - 396s 226ms/step - loss: 0.8188 - f1: 0.2875 - val_loss: 0.7782 - val_f1: 0.3509\n",
    "\n",
    "# Epoch 00012: val_loss improved from 0.79187 to 0.77825, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 13/120\n",
    "# 1748/1748 [==============================] - 396s 226ms/step - loss: 0.8137 - f1: 0.2905 - val_loss: 0.7710 - val_f1: 0.3602\n",
    "\n",
    "# Epoch 00013: val_loss improved from 0.77825 to 0.77102, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 14/120\n",
    "# 1748/1748 [==============================] - 396s 227ms/step - loss: 0.8078 - f1: 0.2949 - val_loss: 0.7753 - val_f1: 0.3552\n",
    "\n",
    "# Epoch 00014: val_loss did not improve from 0.77102\n",
    "# Epoch 15/120\n",
    "# 1748/1748 [==============================] - 396s 226ms/step - loss: 0.8001 - f1: 0.2995 - val_loss: 0.7773 - val_f1: 0.3505\n",
    "\n",
    "# Epoch 00015: val_loss did not improve from 0.77102\n",
    "# Epoch 16/120\n",
    "# 1748/1748 [==============================] - 395s 226ms/step - loss: 0.7972 - f1: 0.3004 - val_loss: 0.8064 - val_f1: 0.3335\n",
    "\n",
    "# Epoch 00016: val_loss did not improve from 0.77102\n",
    "\n",
    "# Epoch 00016: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
    "# Epoch 17/120\n",
    "# 1748/1748 [==============================] - 396s 226ms/step - loss: 0.7710 - f1: 0.3174 - val_loss: 0.7301 - val_f1: 0.3866\n",
    "\n",
    "# Epoch 00017: val_loss improved from 0.77102 to 0.73014, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 18/120\n",
    "# 1748/1748 [==============================] - 395s 226ms/step - loss: 0.7606 - f1: 0.3238 - val_loss: 0.7375 - val_f1: 0.3798\n",
    "\n",
    "# Epoch 00018: val_loss did not improve from 0.73014\n",
    "# Epoch 19/120\n",
    "# 1748/1748 [==============================] - 396s 226ms/step - loss: 0.7575 - f1: 0.3253 - val_loss: 0.7297 - val_f1: 0.3862\n",
    "\n",
    "# Epoch 00019: val_loss improved from 0.73014 to 0.72971, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 20/120\n",
    "# 1748/1748 [==============================] - 394s 225ms/step - loss: 0.7554 - f1: 0.3257 - val_loss: 0.7304 - val_f1: 0.3831\n",
    "\n",
    "# Epoch 00020: val_loss did not improve from 0.72971\n",
    "# Epoch 21/120\n",
    "# 1748/1748 [==============================] - 395s 226ms/step - loss: 0.7496 - f1: 0.3294 - val_loss: 0.7343 - val_f1: 0.3803\n",
    "\n",
    "# Epoch 00021: val_loss did not improve from 0.72971\n",
    "# Epoch 22/120\n",
    "# 1748/1748 [==============================] - 393s 225ms/step - loss: 0.7482 - f1: 0.3302 - val_loss: 0.7265 - val_f1: 0.3887\n",
    "\n",
    "# Epoch 00022: val_loss improved from 0.72971 to 0.72649, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 23/120\n",
    "# 1748/1748 [==============================] - 394s 226ms/step - loss: 0.7444 - f1: 0.3328 - val_loss: 0.7339 - val_f1: 0.3818\n",
    "\n",
    "# Epoch 00023: val_loss did not improve from 0.72649\n",
    "# Epoch 24/120\n",
    "# 1748/1748 [==============================] - 394s 225ms/step - loss: 0.7426 - f1: 0.3338 - val_loss: 0.7304 - val_f1: 0.3866\n",
    "\n",
    "# Epoch 00024: val_loss did not improve from 0.72649\n",
    "# Epoch 25/120\n",
    "# 1748/1748 [==============================] - 394s 226ms/step - loss: 0.7398 - f1: 0.3354 - val_loss: 0.7324 - val_f1: 0.3816\n",
    "\n",
    "# Epoch 00025: val_loss did not improve from 0.72649\n",
    "\n",
    "# Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
    "# Epoch 26/120\n",
    "# 1748/1748 [==============================] - 395s 226ms/step - loss: 0.7369 - f1: 0.3367 - val_loss: 0.7284 - val_f1: 0.3866\n",
    "\n",
    "# Epoch 00026: val_loss did not improve from 0.72649\n",
    "# Epoch 27/120\n",
    "# 1748/1748 [==============================] - 394s 225ms/step - loss: 0.7358 - f1: 0.3378 - val_loss: 0.7321 - val_f1: 0.3835\n",
    "\n",
    "# Epoch 00027: val_loss did not improve from 0.72649\n",
    "# Epoch 28/120\n",
    "# 1748/1748 [==============================] - 393s 225ms/step - loss: 0.7357 - f1: 0.3381 - val_loss: 0.7290 - val_f1: 0.3877\n",
    "#   0%|          | 0/3114 [00:00<?, ?it/s]\n",
    "\n",
    "# Epoch 00028: val_loss did not improve from 0.72649\n",
    "\n",
    "# Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
    "# 100%|██████████| 3114/3114 [02:42<00:00, 19.14it/s]\n",
    "# 11702it [21:06,  9.24it/s]\n",
    "# 27973 3099\n",
    "# Epoch 1/2\n",
    "# 1749/1749 [==============================] - 331s 190ms/step - loss: 1.1197 - f1: 0.0378 - val_loss: 1.1816 - val_f1: 0.0357\n",
    "# Epoch 2/2\n",
    "# 1749/1749 [==============================] - 319s 182ms/step - loss: 1.1067 - f1: 0.0478 - val_loss: 1.2044 - val_f1: 0.0347\n",
    "# Epoch 1/120\n",
    "# 1749/1749 [==============================] - 405s 232ms/step - loss: 1.0601 - f1: 0.0933 - val_loss: 0.9947 - val_f1: 0.1676\n",
    "\n",
    "# Epoch 00001: val_loss improved from inf to 0.99467, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 2/120\n",
    "# 1749/1749 [==============================] - 384s 220ms/step - loss: 0.9880 - f1: 0.1613 - val_loss: 0.9376 - val_f1: 0.2233\n",
    "\n",
    "# Epoch 00002: val_loss improved from 0.99467 to 0.93759, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 3/120\n",
    "# 1749/1749 [==============================] - 385s 220ms/step - loss: 0.9386 - f1: 0.2050 - val_loss: 0.8781 - val_f1: 0.2778\n",
    "\n",
    "# Epoch 00003: val_loss improved from 0.93759 to 0.87811, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 4/120\n",
    "# 1749/1749 [==============================] - 385s 220ms/step - loss: 0.9109 - f1: 0.2256 - val_loss: 0.8430 - val_f1: 0.2990\n",
    "\n",
    "# Epoch 00004: val_loss improved from 0.87811 to 0.84296, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 5/120\n",
    "# 1749/1749 [==============================] - 384s 220ms/step - loss: 0.8937 - f1: 0.2383 - val_loss: 0.8510 - val_f1: 0.2958\n",
    "\n",
    "# Epoch 00005: val_loss did not improve from 0.84296\n",
    "# Epoch 6/120\n",
    "# 1749/1749 [==============================] - 384s 219ms/step - loss: 0.8799 - f1: 0.2472 - val_loss: 0.8430 - val_f1: 0.3046\n",
    "\n",
    "# Epoch 00006: val_loss did not improve from 0.84296\n",
    "# Epoch 7/120\n",
    "# 1749/1749 [==============================] - 384s 219ms/step - loss: 0.8679 - f1: 0.2548 - val_loss: 0.8243 - val_f1: 0.3158\n",
    "\n",
    "# Epoch 00007: val_loss improved from 0.84296 to 0.82426, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 8/120\n",
    "# 1749/1749 [==============================] - 383s 219ms/step - loss: 0.8553 - f1: 0.2641 - val_loss: 0.8559 - val_f1: 0.2988\n",
    "\n",
    "# Epoch 00008: val_loss did not improve from 0.82426\n",
    "# Epoch 9/120\n",
    "# 1749/1749 [==============================] - 384s 219ms/step - loss: 0.8510 - f1: 0.2662 - val_loss: 0.8497 - val_f1: 0.3030\n",
    "\n",
    "# Epoch 00009: val_loss did not improve from 0.82426\n",
    "# Epoch 10/120\n",
    "# 1749/1749 [==============================] - 382s 218ms/step - loss: 0.8410 - f1: 0.2731 - val_loss: 0.8813 - val_f1: 0.2890\n",
    "\n",
    "# Epoch 00010: val_loss did not improve from 0.82426\n",
    "\n",
    "# Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
    "# Epoch 11/120\n",
    "# 1749/1749 [==============================] - 382s 219ms/step - loss: 0.8165 - f1: 0.2874 - val_loss: 0.7563 - val_f1: 0.3650\n",
    "\n",
    "# Epoch 00011: val_loss improved from 0.82426 to 0.75628, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 12/120\n",
    "# 1749/1749 [==============================] - 382s 218ms/step - loss: 0.8073 - f1: 0.2933 - val_loss: 0.7564 - val_f1: 0.3663\n",
    "\n",
    "# Epoch 00012: val_loss did not improve from 0.75628\n",
    "# Epoch 13/120\n",
    "# 1749/1749 [==============================] - 383s 219ms/step - loss: 0.8035 - f1: 0.2963 - val_loss: 0.7528 - val_f1: 0.3692\n",
    "\n",
    "# Epoch 00013: val_loss improved from 0.75628 to 0.75284, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 14/120\n",
    "# 1749/1749 [==============================] - 383s 219ms/step - loss: 0.7986 - f1: 0.2995 - val_loss: 0.7515 - val_f1: 0.3681\n",
    "\n",
    "# Epoch 00014: val_loss improved from 0.75284 to 0.75147, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 15/120\n",
    "# 1749/1749 [==============================] - 382s 219ms/step - loss: 0.7943 - f1: 0.3026 - val_loss: 0.7538 - val_f1: 0.3696\n",
    "\n",
    "# Epoch 00015: val_loss did not improve from 0.75147\n",
    "# Epoch 16/120\n",
    "# 1749/1749 [==============================] - 382s 219ms/step - loss: 0.7955 - f1: 0.3003 - val_loss: 0.7536 - val_f1: 0.3683\n",
    "\n",
    "# Epoch 00016: val_loss did not improve from 0.75147\n",
    "# Epoch 17/120\n",
    "# 1749/1749 [==============================] - 384s 219ms/step - loss: 0.7889 - f1: 0.3054 - val_loss: 0.7508 - val_f1: 0.3676\n",
    "\n",
    "# Epoch 00017: val_loss improved from 0.75147 to 0.75082, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 18/120\n",
    "# 1749/1749 [==============================] - 383s 219ms/step - loss: 0.7860 - f1: 0.3074 - val_loss: 0.7566 - val_f1: 0.3672\n",
    "\n",
    "# Epoch 00018: val_loss did not improve from 0.75082\n",
    "# Epoch 19/120\n",
    "# 1749/1749 [==============================] - 383s 219ms/step - loss: 0.7832 - f1: 0.3099 - val_loss: 0.7517 - val_f1: 0.3718\n",
    "\n",
    "# Epoch 00019: val_loss did not improve from 0.75082\n",
    "# Epoch 20/120\n",
    "# 1749/1749 [==============================] - 382s 219ms/step - loss: 0.7794 - f1: 0.3112 - val_loss: 0.7510 - val_f1: 0.3683\n",
    "\n",
    "# Epoch 00020: val_loss did not improve from 0.75082\n",
    "\n",
    "# Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
    "# Epoch 21/120\n",
    "# 1749/1749 [==============================] - 383s 219ms/step - loss: 0.7766 - f1: 0.3129 - val_loss: 0.7479 - val_f1: 0.3757\n",
    "\n",
    "# Epoch 00021: val_loss improved from 0.75082 to 0.74793, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 22/120\n",
    "# 1749/1749 [==============================] - 383s 219ms/step - loss: 0.7778 - f1: 0.3123 - val_loss: 0.7495 - val_f1: 0.3689\n",
    "\n",
    "# Epoch 00022: val_loss did not improve from 0.74793\n",
    "# Epoch 23/120\n",
    "# 1749/1749 [==============================] - 383s 219ms/step - loss: 0.7773 - f1: 0.3124 - val_loss: 0.7480 - val_f1: 0.3725\n",
    "\n",
    "# Epoch 00023: val_loss did not improve from 0.74793\n",
    "# Epoch 24/120\n",
    "# 1749/1749 [==============================] - 382s 219ms/step - loss: 0.7740 - f1: 0.3146 - val_loss: 0.7491 - val_f1: 0.3715\n",
    "\n",
    "# Epoch 00024: val_loss did not improve from 0.74793\n",
    "\n",
    "# Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
    "# Epoch 25/120\n",
    "# 1749/1749 [==============================] - 382s 219ms/step - loss: 0.7737 - f1: 0.3153 - val_loss: 0.7473 - val_f1: 0.3761\n",
    "\n",
    "# Epoch 00025: val_loss improved from 0.74793 to 0.74733, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 26/120\n",
    "# 1749/1749 [==============================] - 383s 219ms/step - loss: 0.7742 - f1: 0.3146 - val_loss: 0.7437 - val_f1: 0.3774\n",
    "\n",
    "# Epoch 00026: val_loss improved from 0.74733 to 0.74374, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 27/120\n",
    "# 1749/1749 [==============================] - 383s 219ms/step - loss: 0.7743 - f1: 0.3141 - val_loss: 0.7483 - val_f1: 0.3721\n",
    "\n",
    "# Epoch 00027: val_loss did not improve from 0.74374\n",
    "# Epoch 28/120\n",
    "# 1749/1749 [==============================] - 384s 219ms/step - loss: 0.7740 - f1: 0.3141 - val_loss: 0.7494 - val_f1: 0.3722\n",
    "\n",
    "# Epoch 00028: val_loss did not improve from 0.74374\n",
    "# Epoch 29/120\n",
    "# 1749/1749 [==============================] - 383s 219ms/step - loss: 0.7741 - f1: 0.3147 - val_loss: 0.7469 - val_f1: 0.3752\n",
    "\n",
    "# Epoch 00029: val_loss did not improve from 0.74374\n",
    "\n",
    "# Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
    "# Epoch 30/120\n",
    "# 1749/1749 [==============================] - 383s 219ms/step - loss: 0.7732 - f1: 0.3159 - val_loss: 0.7468 - val_f1: 0.3734\n",
    "\n",
    "# Epoch 00030: val_loss did not improve from 0.74374\n",
    "# Epoch 31/120\n",
    "# 1749/1749 [==============================] - 384s 220ms/step - loss: 0.7745 - f1: 0.3138 - val_loss: 0.7500 - val_f1: 0.3702\n",
    "\n",
    "# Epoch 00031: val_loss did not improve from 0.74374\n",
    "# Epoch 32/120\n",
    "# 1749/1749 [==============================] - 386s 221ms/step - loss: 0.7765 - f1: 0.3127 - val_loss: 0.7468 - val_f1: 0.3748\n",
    "#   0%|          | 0/3099 [00:00<?, ?it/s]\n",
    "\n",
    "# Epoch 00032: val_loss did not improve from 0.74374\n",
    "\n",
    "# Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
    "# 100%|██████████| 3099/3099 [02:47<00:00, 18.52it/s]\n",
    "# 11702it [08:03, 24.19it/s]\n",
    "# 27969 3103\n",
    "# Epoch 1/2\n",
    "# 1749/1749 [==============================] - 331s 189ms/step - loss: 1.1194 - f1: 0.0381 - val_loss: 1.1530 - val_f1: 0.0276\n",
    "# Epoch 2/2\n",
    "# 1749/1749 [==============================] - 328s 187ms/step - loss: 1.1066 - f1: 0.0477 - val_loss: 1.2052 - val_f1: 0.0364\n",
    "# Epoch 1/120\n",
    "# 1749/1749 [==============================] - 401s 229ms/step - loss: 1.0629 - f1: 0.0893 - val_loss: 1.0045 - val_f1: 0.1535\n",
    "\n",
    "# Epoch 00001: val_loss improved from inf to 1.00452, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 2/120\n",
    "# 1749/1749 [==============================] - 380s 218ms/step - loss: 0.9884 - f1: 0.1612 - val_loss: 0.9702 - val_f1: 0.1899\n",
    "\n",
    "# Epoch 00002: val_loss improved from 1.00452 to 0.97018, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 3/120\n",
    "# 1749/1749 [==============================] - 378s 216ms/step - loss: 0.9427 - f1: 0.2013 - val_loss: 0.8851 - val_f1: 0.2652\n",
    "\n",
    "# Epoch 00003: val_loss improved from 0.97018 to 0.88508, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 4/120\n",
    "# 1749/1749 [==============================] - 379s 217ms/step - loss: 0.9145 - f1: 0.2240 - val_loss: 0.8796 - val_f1: 0.2782\n",
    "\n",
    "# Epoch 00004: val_loss improved from 0.88508 to 0.87963, saving model to ../cache/InceptionV3.h5\n",
    "# Epoch 5/120\n",
    "#  978/1749 [===============>..............] - ETA: 2:24 - loss: 0.8989 - f1: 0.2330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../cache/oof_class_preds-14-1.npy', oof_class_preds)\n",
    "np.save('../cache/sub_class_preds-14-1.npy', sub_class_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof_class_preds = np.zeros((train_dataset_info.shape[0], 28))\n",
    "# sub_class_preds = np.zeros((submit.shape[0], 28))\n",
    "# score_predict = model.predict(image[np.newaxis])[0]\n",
    "# oof_class_preds[idx] = score_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train_index, test_index in kf.split(indexes):\n",
    "# ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11702, 28)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_class_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 99590.62it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for line in tqdm(sub_class_preds):\n",
    "    label_predict = np.arange(28)[line>=0.2]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11702"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " '5 25',\n",
       " '0 5 25',\n",
       " '0 25',\n",
       " '0 7 25',\n",
       " '4',\n",
       " '0 4 25',\n",
       " '0 23 25',\n",
       " '0',\n",
       " '25',\n",
       " '17 18 25',\n",
       " '3 5',\n",
       " '0 2 25',\n",
       " '7 9 10 18 25',\n",
       " '23',\n",
       " '0 4 18',\n",
       " '2 14 25',\n",
       " '0 5',\n",
       " '14 21',\n",
       " '0 5',\n",
       " '6',\n",
       " '3 5',\n",
       " '0 11 16 17 25',\n",
       " '0 7',\n",
       " '0 4',\n",
       " '0 12 21 25',\n",
       " '0',\n",
       " '0',\n",
       " '0 5 25',\n",
       " '0',\n",
       " '0 13 21',\n",
       " '0 25',\n",
       " '14 16 17 18 21 25',\n",
       " '0 5 25',\n",
       " '0 7 25',\n",
       " '13',\n",
       " '0 25',\n",
       " '0 3',\n",
       " '0 5 21 25',\n",
       " '1',\n",
       " '0 16 17 25',\n",
       " '6 25',\n",
       " '0 21',\n",
       " '18 19 25',\n",
       " '0 14 16 25',\n",
       " '6',\n",
       " '0',\n",
       " '0 23',\n",
       " '0 6 11 23 25',\n",
       " '0',\n",
       " '0 16 17 25',\n",
       " '0 5',\n",
       " '20 23 26',\n",
       " '0 25',\n",
       " '3',\n",
       " '0 25',\n",
       " '0 17 25',\n",
       " '11 23',\n",
       " '0 25',\n",
       " '21 25',\n",
       " '2 22',\n",
       " '0 5 21 25',\n",
       " '2 14 16 25',\n",
       " '7 21',\n",
       " '23',\n",
       " '0 18 19 25',\n",
       " '3 6 21 25',\n",
       " '0 25',\n",
       " '0 1',\n",
       " '21 25',\n",
       " '2 3',\n",
       " '0 2',\n",
       " '14',\n",
       " '4',\n",
       " '21',\n",
       " '0',\n",
       " '2 4',\n",
       " '0 1',\n",
       " '0 25',\n",
       " '0 16 17 25',\n",
       " '6 25',\n",
       " '0 25',\n",
       " '0 21',\n",
       " '0 17 21 25',\n",
       " '16 17 18 19',\n",
       " '0 23',\n",
       " '20 23',\n",
       " '0 21',\n",
       " '14 16 17 25',\n",
       " '14',\n",
       " '0 25',\n",
       " '11 14',\n",
       " '23',\n",
       " '13',\n",
       " '0 16 17 25',\n",
       " '0 25',\n",
       " '7 16 17 18 25',\n",
       " '0 7 19 25',\n",
       " '24',\n",
       " '0 23 25',\n",
       " '0 25',\n",
       " '23',\n",
       " '21 23',\n",
       " '0 23',\n",
       " '7 11',\n",
       " '19 21 25 26',\n",
       " '0 14 16',\n",
       " '0 11 24 25',\n",
       " '7 20 26',\n",
       " '0 25',\n",
       " '0 2',\n",
       " '1',\n",
       " '16 17 18 25',\n",
       " '0 21 22 25',\n",
       " '22 25',\n",
       " '21 23 25',\n",
       " '0 2',\n",
       " '4',\n",
       " '14 16 17 25',\n",
       " '26',\n",
       " '0 18 19 25',\n",
       " '0 21 25',\n",
       " '2 21 25',\n",
       " '8 20',\n",
       " '0 2 4',\n",
       " '0',\n",
       " '0 25',\n",
       " '25',\n",
       " '0 3 4',\n",
       " '19',\n",
       " '17 25',\n",
       " '0 21',\n",
       " '0',\n",
       " '0',\n",
       " '0 11',\n",
       " '5',\n",
       " '0 14 16 25',\n",
       " '0',\n",
       " '13 21',\n",
       " '6 21 25',\n",
       " '0 19 21',\n",
       " '21 25',\n",
       " '0 1',\n",
       " '0 5',\n",
       " '0 25',\n",
       " '5 26',\n",
       " '0 21 25',\n",
       " '0 6 7 25',\n",
       " '0',\n",
       " '0 25',\n",
       " '0 3 23',\n",
       " '7 16 17 18',\n",
       " '0 7',\n",
       " '0 11 25',\n",
       " '6 7 25',\n",
       " '6',\n",
       " '0',\n",
       " '0 16 25',\n",
       " '1 25',\n",
       " '25',\n",
       " '0 18 19 25',\n",
       " '0 21 25',\n",
       " '0 3 11 18 19 25',\n",
       " '4 25',\n",
       " '5',\n",
       " '21 23',\n",
       " '0 19 25',\n",
       " '19',\n",
       " '17 21 25',\n",
       " '7 16 25',\n",
       " '5 25',\n",
       " '0 6 19 21 25',\n",
       " '0 6 11 14 16 25',\n",
       " '21 23',\n",
       " '0 2',\n",
       " '0 11 23',\n",
       " '21',\n",
       " '0 7 18 25',\n",
       " '0 12 21',\n",
       " '0 2 25',\n",
       " '0',\n",
       " '23',\n",
       " '0 19',\n",
       " '0 20 26',\n",
       " '14',\n",
       " '0 16 17 21 25',\n",
       " '0 11 25',\n",
       " '0 5 18',\n",
       " '23',\n",
       " '14 16 17 21 25',\n",
       " '14 17 25',\n",
       " '0 25',\n",
       " '5 25 26',\n",
       " '25 26',\n",
       " '5 25',\n",
       " '0 13',\n",
       " '0 25',\n",
       " '16 17',\n",
       " '23 25',\n",
       " '0 25',\n",
       " '2 21',\n",
       " '0 2 3',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '0 3 25',\n",
       " '21',\n",
       " '0 21 25',\n",
       " '0 5',\n",
       " '0',\n",
       " '0 21',\n",
       " '0 14 16',\n",
       " '0 5',\n",
       " '7 25',\n",
       " '0 5',\n",
       " '18 19',\n",
       " '0 21 25',\n",
       " '24',\n",
       " '7',\n",
       " '21 22',\n",
       " '2 3',\n",
       " '0 3',\n",
       " '14 21 25',\n",
       " '7 21',\n",
       " '2 3 12 21',\n",
       " '6 8 20 23',\n",
       " '23',\n",
       " '21',\n",
       " '12 25',\n",
       " '0 19',\n",
       " '0 5',\n",
       " '11 25',\n",
       " '0 11',\n",
       " '0 2',\n",
       " '23',\n",
       " '0 23 25',\n",
       " '11',\n",
       " '13 20 22 26',\n",
       " '0 12',\n",
       " '0 18 21 25',\n",
       " '16 17 18 25',\n",
       " '0 21',\n",
       " '7',\n",
       " '0 7 21 25',\n",
       " '0 25',\n",
       " '2 4 11 14',\n",
       " '12 23',\n",
       " '25',\n",
       " '4',\n",
       " '13 22',\n",
       " '0 1',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '2 6 25',\n",
       " '0 2',\n",
       " '0 25',\n",
       " '11',\n",
       " '23',\n",
       " '3 5',\n",
       " '19 25',\n",
       " '21 25',\n",
       " '14 16 17 18 25',\n",
       " '19 26',\n",
       " '3 4 26',\n",
       " '0 2 5',\n",
       " '13 21 22',\n",
       " '0',\n",
       " '12',\n",
       " '0 2',\n",
       " '0 23 25',\n",
       " '0 25',\n",
       " '0 19',\n",
       " '0 25',\n",
       " '0 12',\n",
       " '0 2 25',\n",
       " '2',\n",
       " '0 18 19 25',\n",
       " '0 2 19',\n",
       " '11',\n",
       " '0 25',\n",
       " '0',\n",
       " '0 25',\n",
       " '2',\n",
       " '0 3',\n",
       " '8 9 10 20 26',\n",
       " '0',\n",
       " '0 12 21',\n",
       " '0 2',\n",
       " '7',\n",
       " '1 2 6',\n",
       " '7 11 24',\n",
       " '0 5 7',\n",
       " '0 14 16',\n",
       " '0 18 21 25',\n",
       " '7',\n",
       " '0 19',\n",
       " '0 21 25',\n",
       " '25',\n",
       " '0 2 25',\n",
       " '0 5 19',\n",
       " '0 2 21',\n",
       " '25 26',\n",
       " '0 5 18 19',\n",
       " '0 11 25',\n",
       " '0 14',\n",
       " '25',\n",
       " '0 19',\n",
       " '1',\n",
       " '14 16 17 25',\n",
       " '0 21',\n",
       " '0',\n",
       " '11 23 25',\n",
       " '11 21',\n",
       " '0 4 7 25',\n",
       " '0',\n",
       " '0',\n",
       " '21 25',\n",
       " '14 16 17 18 25',\n",
       " '0',\n",
       " '0 25',\n",
       " '0 2 17 25',\n",
       " '19',\n",
       " '6 25',\n",
       " '0 25',\n",
       " '0 2 5',\n",
       " '0',\n",
       " '0 2 25',\n",
       " '21 22',\n",
       " '0 2',\n",
       " '0 2 3',\n",
       " '0 14',\n",
       " '5',\n",
       " '3 8 9 18 19 25',\n",
       " '25',\n",
       " '0 1 21',\n",
       " '11 21 25',\n",
       " '7',\n",
       " '24',\n",
       " '2 14 16 17 25',\n",
       " '0 25',\n",
       " '6',\n",
       " '21 25',\n",
       " '0 7',\n",
       " '0 2 4',\n",
       " '14 16 17',\n",
       " '4 12 21 22 25',\n",
       " '0 2 3 19 25',\n",
       " '18 19 24',\n",
       " '3 7',\n",
       " '0 2 21 25',\n",
       " '0 25',\n",
       " '0 7',\n",
       " '20 26',\n",
       " '0 25',\n",
       " '5 19',\n",
       " '0 2 25',\n",
       " '3',\n",
       " '0 7 24',\n",
       " '0 5 21',\n",
       " '25',\n",
       " '2 4 23',\n",
       " '11 24 25',\n",
       " '2 6 25',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '7 25',\n",
       " '11',\n",
       " '11',\n",
       " '0',\n",
       " '0 5 18 19 25',\n",
       " '0 5',\n",
       " '14 16 17 18 25',\n",
       " '0 1 25',\n",
       " '0 21 25',\n",
       " '17 25',\n",
       " '0 23 25',\n",
       " '0 21',\n",
       " '0 3',\n",
       " '0 3 25',\n",
       " '12 21',\n",
       " '4 13 21 25',\n",
       " '0 21',\n",
       " '7',\n",
       " '0',\n",
       " '2 5 14 21',\n",
       " '4 5',\n",
       " '14 16',\n",
       " '0',\n",
       " '2 3',\n",
       " '18 19',\n",
       " '0 25',\n",
       " '14 16 17 18 25',\n",
       " '25',\n",
       " '0 14',\n",
       " '0 17 21 25',\n",
       " '0 5',\n",
       " '0',\n",
       " '0 21',\n",
       " '0 11',\n",
       " '14 17 21',\n",
       " '0',\n",
       " '21 23 25',\n",
       " '0 5',\n",
       " '21',\n",
       " '0 14 16 21',\n",
       " '0 21 22 25',\n",
       " '0 2 23',\n",
       " '0 20 23',\n",
       " '25',\n",
       " '0 21 25',\n",
       " '1 11',\n",
       " '6 23 25',\n",
       " '0 3',\n",
       " '23',\n",
       " '7',\n",
       " '0 25',\n",
       " '0',\n",
       " '0 21 25',\n",
       " '6 25',\n",
       " '0 5 21 25',\n",
       " '0 7 24',\n",
       " '0 7 25',\n",
       " '0 2 5',\n",
       " '17 18 21',\n",
       " '0 3 4 25',\n",
       " '0 25',\n",
       " '0 7 25',\n",
       " '18 25',\n",
       " '0 18 19',\n",
       " '0 3 24',\n",
       " '4 21 25',\n",
       " '0 12 21 25',\n",
       " '0 23 25',\n",
       " '23',\n",
       " '0 14 16',\n",
       " '18 25',\n",
       " '0 7 19 23 25',\n",
       " '0 4',\n",
       " '0 21 25',\n",
       " '2',\n",
       " '7',\n",
       " '1 14',\n",
       " '21 25',\n",
       " '11 25',\n",
       " '0 2',\n",
       " '0 25',\n",
       " '0 13 22 25 26',\n",
       " '0 19',\n",
       " '18 21 25',\n",
       " '16 17 18 25',\n",
       " '16 17 18 21 25',\n",
       " '12 21',\n",
       " '0 2 3',\n",
       " '0 4 21 25',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '5 25',\n",
       " '1 25',\n",
       " '6 11',\n",
       " '4',\n",
       " '0 25',\n",
       " '2 21 25',\n",
       " '0 2 3',\n",
       " '0 2 25',\n",
       " '7 20',\n",
       " '21',\n",
       " '23',\n",
       " '0 23',\n",
       " '0 2 11',\n",
       " '7 11',\n",
       " '3',\n",
       " '23',\n",
       " '0 2',\n",
       " '0 25',\n",
       " '14 16 17 25',\n",
       " '3 4',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '6 25',\n",
       " '4',\n",
       " '14 16',\n",
       " '0 21 25',\n",
       " '4',\n",
       " '0',\n",
       " '0 22',\n",
       " '0 21 25',\n",
       " '6 25',\n",
       " '0 13',\n",
       " '4',\n",
       " '0 1',\n",
       " '0 25',\n",
       " '21 25',\n",
       " '0 11 25',\n",
       " '2 25',\n",
       " '0 25',\n",
       " '7',\n",
       " '21 25',\n",
       " '0 7',\n",
       " '0 23',\n",
       " '0 19 23',\n",
       " '17 25',\n",
       " '2',\n",
       " '0',\n",
       " '21',\n",
       " '21',\n",
       " '0',\n",
       " '0 21 25',\n",
       " '7 25',\n",
       " '0 25',\n",
       " '0 7',\n",
       " '0 21 25',\n",
       " '0 7 21 25',\n",
       " '12 13',\n",
       " '0 2 3 25',\n",
       " '0 13 21 22 25',\n",
       " '21 25',\n",
       " '7',\n",
       " '0',\n",
       " '25 26',\n",
       " '3',\n",
       " '19 26',\n",
       " '0 3 25',\n",
       " '0 1 25',\n",
       " '0 11 14 25',\n",
       " '0',\n",
       " '19 25',\n",
       " '0 4',\n",
       " '7',\n",
       " '0 1 25',\n",
       " '21 23',\n",
       " '18 19',\n",
       " '0 5 21 25',\n",
       " '0 16 17 18 25',\n",
       " '0 16 17 18 21',\n",
       " '4 25',\n",
       " '0 14',\n",
       " '2 25',\n",
       " '19',\n",
       " '0 25',\n",
       " '18 19',\n",
       " '12',\n",
       " '0 5 21',\n",
       " '5 26',\n",
       " '0',\n",
       " '0',\n",
       " '7 11',\n",
       " '4',\n",
       " '0 21',\n",
       " '2 7 21',\n",
       " '2',\n",
       " '23',\n",
       " '14 21',\n",
       " '0 2 25',\n",
       " '0 11 12 25',\n",
       " '2 7 21 25',\n",
       " '21',\n",
       " '12 21 25',\n",
       " '0 25',\n",
       " '0 21 25',\n",
       " '0 25',\n",
       " '7',\n",
       " '0',\n",
       " '21 23',\n",
       " '0 2 7',\n",
       " '0 18 25',\n",
       " '0 17 21 25',\n",
       " '2 18 21',\n",
       " '0 19 25',\n",
       " '6 25',\n",
       " '0 23 25',\n",
       " '18 25',\n",
       " '0 25',\n",
       " '14 16 25',\n",
       " '0 5',\n",
       " '6 7 21 25',\n",
       " '24',\n",
       " '0 16 17 25',\n",
       " '7 23',\n",
       " '0',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '21 23 26',\n",
       " '11 25',\n",
       " '5 19 21 25',\n",
       " '21',\n",
       " '0 25',\n",
       " '21',\n",
       " '4',\n",
       " '23',\n",
       " '21',\n",
       " '0 7 19',\n",
       " '5 23',\n",
       " '0 13 20 22',\n",
       " '6 25',\n",
       " '21 22',\n",
       " '0 25',\n",
       " '19 25',\n",
       " '2 3',\n",
       " '2 14 16',\n",
       " '6 14',\n",
       " '0 11 25',\n",
       " '1 2',\n",
       " '23',\n",
       " '0 25',\n",
       " '25',\n",
       " '0 5 21',\n",
       " '0',\n",
       " '19',\n",
       " '0 4 7',\n",
       " '0 12 21',\n",
       " '0 25',\n",
       " '7 25',\n",
       " '14',\n",
       " '12 14 21',\n",
       " '5 19',\n",
       " '5 21',\n",
       " '23',\n",
       " '1',\n",
       " '0 25',\n",
       " '7',\n",
       " '7 23 25',\n",
       " '3 5',\n",
       " '0 12',\n",
       " '14',\n",
       " '23 25',\n",
       " '0 21',\n",
       " '5 21 22',\n",
       " '21 22',\n",
       " '0 25',\n",
       " '25',\n",
       " '0 12 21',\n",
       " '0 25',\n",
       " '0 7 21',\n",
       " '0 25',\n",
       " '3 5 25',\n",
       " '14',\n",
       " '0 7 18',\n",
       " '0 19',\n",
       " '5',\n",
       " '7',\n",
       " '21 22',\n",
       " '13',\n",
       " '0',\n",
       " '23 25',\n",
       " '0 7',\n",
       " '11 12 21 25',\n",
       " '0 1 25',\n",
       " '23 25',\n",
       " '0 5 25',\n",
       " '25 26',\n",
       " '21 25',\n",
       " '11 14 21 25',\n",
       " '0 21 25',\n",
       " '0 2 3',\n",
       " '2',\n",
       " '5 21',\n",
       " '0 23 25',\n",
       " '1',\n",
       " '0 1 18 19 21 25',\n",
       " '0 25',\n",
       " '0 19',\n",
       " '0',\n",
       " '0 19',\n",
       " '0 16 25',\n",
       " '0 25',\n",
       " '2 4 26',\n",
       " '7',\n",
       " '5 25',\n",
       " '5 25',\n",
       " '21 22',\n",
       " '14 16 17 25',\n",
       " '0 21 22',\n",
       " '0 2 25',\n",
       " '7',\n",
       " '5 25',\n",
       " '18 19',\n",
       " '0 1 25',\n",
       " '0 21',\n",
       " '19 26',\n",
       " '18 19 25',\n",
       " '23 25',\n",
       " '1 25',\n",
       " '0 2 4 14 16',\n",
       " '22',\n",
       " '0 6 21 25',\n",
       " '0 25',\n",
       " '0 22',\n",
       " '0 21',\n",
       " '0 25',\n",
       " '0 16 17 25',\n",
       " '0 25',\n",
       " '23',\n",
       " '0 3 5 21',\n",
       " '4',\n",
       " '13',\n",
       " '0 25',\n",
       " '11 21 25',\n",
       " '0 12 23',\n",
       " '1 25',\n",
       " '0 5 14 16 25',\n",
       " '5',\n",
       " '0 23',\n",
       " '23',\n",
       " '14',\n",
       " '0 2 3 11 25',\n",
       " '5 25',\n",
       " '11',\n",
       " '7',\n",
       " '0 7 21',\n",
       " '3',\n",
       " '0 21',\n",
       " '0 25',\n",
       " '21',\n",
       " '1 21 25',\n",
       " '25',\n",
       " '0 21 25',\n",
       " '11 12 21 22',\n",
       " '0 2',\n",
       " '2 3 4 5',\n",
       " '0 21',\n",
       " '0 18 19',\n",
       " '21',\n",
       " '25',\n",
       " '0 25',\n",
       " '19',\n",
       " '0',\n",
       " '0 2 25',\n",
       " '0 23',\n",
       " '0 1 2 3',\n",
       " '0 19',\n",
       " '0 25',\n",
       " '0 13 22 25',\n",
       " '0 5',\n",
       " '0 17 18 21 25',\n",
       " '0 2 7',\n",
       " '0 1 2',\n",
       " '0 7 21',\n",
       " '0 21 25',\n",
       " '0 25',\n",
       " '23',\n",
       " '7 9 13 18 19 25',\n",
       " '0 21 25',\n",
       " '23 25',\n",
       " '6 23 25',\n",
       " '21',\n",
       " '21 24',\n",
       " '0 13',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '0 21 25',\n",
       " '0',\n",
       " '23',\n",
       " '0 2 11 21',\n",
       " '0',\n",
       " '0 25',\n",
       " '0 5',\n",
       " '6 21 25',\n",
       " '0',\n",
       " '0 25',\n",
       " '23',\n",
       " '13 25',\n",
       " '25',\n",
       " '0 2 21 25',\n",
       " '14 25',\n",
       " '5 7',\n",
       " '14 21 25',\n",
       " '0 7 21 25',\n",
       " '18 19',\n",
       " '7 23',\n",
       " '7 11',\n",
       " '0 7',\n",
       " '18 19 21',\n",
       " '23',\n",
       " '0 5 21 25',\n",
       " '0 25',\n",
       " '0 17',\n",
       " '14 16 25',\n",
       " '6 25',\n",
       " '0 2 16',\n",
       " '21 25',\n",
       " '0 25',\n",
       " '11 23',\n",
       " '14 16 17',\n",
       " '0 16 21 22',\n",
       " '14 16 17 25',\n",
       " '14 25',\n",
       " '0 18 19 25',\n",
       " '6 11 23',\n",
       " '5',\n",
       " '7',\n",
       " '7 23',\n",
       " '0',\n",
       " '0',\n",
       " '0 26',\n",
       " '0 25',\n",
       " '25',\n",
       " '23 25',\n",
       " '0 25',\n",
       " '21 22 25 26',\n",
       " '0 2 25',\n",
       " '0 18 19 25',\n",
       " '12',\n",
       " '0 21',\n",
       " '0 25',\n",
       " '0 21',\n",
       " '0 18 21',\n",
       " '23',\n",
       " '16 17 18 19 25',\n",
       " '0 25',\n",
       " '0 16 19',\n",
       " '1 23',\n",
       " '11',\n",
       " '2',\n",
       " '0 25',\n",
       " '0 7',\n",
       " '5',\n",
       " '0',\n",
       " '14 16 17 18',\n",
       " '0 5 11 21 25',\n",
       " '0 11 21 25',\n",
       " '21 25 26',\n",
       " '0 13 14 16 25',\n",
       " '0 25',\n",
       " '25',\n",
       " '0 21 25',\n",
       " '0 6 11 25',\n",
       " '25',\n",
       " '25',\n",
       " '6 25',\n",
       " '0 21',\n",
       " '21 25',\n",
       " '6 21 25',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '0 1',\n",
       " '0 25',\n",
       " '3',\n",
       " '0',\n",
       " '0 5',\n",
       " '0 2 21',\n",
       " '0 25',\n",
       " '0 5',\n",
       " '0 25',\n",
       " '0 1 5 16',\n",
       " '7',\n",
       " '0 26',\n",
       " '0 25',\n",
       " '0 16 17 21 25',\n",
       " '13 19 21 25',\n",
       " '12 21',\n",
       " '0 25',\n",
       " '0 21 25',\n",
       " '14 16 17',\n",
       " '14 16 17 21 25',\n",
       " '0 2 21 25',\n",
       " '0 19',\n",
       " '6 25 26',\n",
       " '0 2',\n",
       " '21',\n",
       " '0 18 19',\n",
       " '21',\n",
       " '0 23',\n",
       " '0 18',\n",
       " '0 1 16',\n",
       " '19 25',\n",
       " '0 25',\n",
       " '0 5 12 21',\n",
       " '0 16 17 18 25',\n",
       " '0 11 14 16 17',\n",
       " '0 5 25',\n",
       " '11 24',\n",
       " '3 5 24 25',\n",
       " '0 2 3 5',\n",
       " '0 5',\n",
       " '11',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '23',\n",
       " '6',\n",
       " '0 23 25',\n",
       " '0 2 5 25',\n",
       " '21 22',\n",
       " '0 25',\n",
       " '0 5 25',\n",
       " '1 21',\n",
       " '0 23',\n",
       " '0 20 23 25',\n",
       " '0 25',\n",
       " '6 23',\n",
       " '0 2 4 25',\n",
       " '5',\n",
       " '23',\n",
       " '12 21 25',\n",
       " '2 7',\n",
       " '0 1 7',\n",
       " '6',\n",
       " '20 26',\n",
       " '0 2 25',\n",
       " '0 25',\n",
       " '0 7',\n",
       " '0 21 25',\n",
       " '0 13 22',\n",
       " '23',\n",
       " '6 25',\n",
       " '0 14 16 17 25',\n",
       " '4',\n",
       " '0 25',\n",
       " '0 5',\n",
       " '19 21',\n",
       " '0 3',\n",
       " '18 19 25',\n",
       " '0 25',\n",
       " '0 5',\n",
       " '26',\n",
       " '0 7',\n",
       " '14 16',\n",
       " '25',\n",
       " '7',\n",
       " '0 2 25',\n",
       " '7 16',\n",
       " '7 25',\n",
       " '0',\n",
       " '7 23 25',\n",
       " '0 21 25',\n",
       " '0 3 5',\n",
       " '0 21 25',\n",
       " '0 17 18 21 25',\n",
       " '0 2 7 25',\n",
       " '0 25',\n",
       " '0 6 25',\n",
       " '11 18 19 25',\n",
       " '0 2 4',\n",
       " '4',\n",
       " '17 18 21',\n",
       " '14 16 17',\n",
       " '0 21',\n",
       " '0 21 22 25',\n",
       " '19',\n",
       " '21 23',\n",
       " '6 25',\n",
       " '0 21 25',\n",
       " '21',\n",
       " '0 21',\n",
       " '1 21 25',\n",
       " '25',\n",
       " '0 2 16',\n",
       " '7',\n",
       " '0 1 25',\n",
       " '2 14 16 17 21 25',\n",
       " '2 26',\n",
       " '0 25',\n",
       " '14 16 17 25',\n",
       " '0 2 3',\n",
       " '0 17 25',\n",
       " '4 21 25',\n",
       " '21',\n",
       " '0 25',\n",
       " '0 19',\n",
       " '0',\n",
       " '0 18 19',\n",
       " '0 11 23',\n",
       " '25',\n",
       " '7 16 18 19',\n",
       " '12',\n",
       " '7 18',\n",
       " '0 23',\n",
       " '0 25',\n",
       " '0 7',\n",
       " '7',\n",
       " '21 25',\n",
       " '0 12 14 25',\n",
       " '0 4 5 25 26',\n",
       " '0',\n",
       " '0 21 22',\n",
       " '0',\n",
       " '0 1',\n",
       " '0 16 17 25',\n",
       " '0 16 25',\n",
       " '0 21 25',\n",
       " '11',\n",
       " '0 7 14 16',\n",
       " '25',\n",
       " '3',\n",
       " '6 21 25',\n",
       " '0 2 3 5 23',\n",
       " '3',\n",
       " '18 19 25',\n",
       " '23',\n",
       " '5',\n",
       " '0 25',\n",
       " '0 2',\n",
       " '0 1',\n",
       " '0 2',\n",
       " '0 2',\n",
       " '5 25',\n",
       " '0 25 26',\n",
       " '0 21 25',\n",
       " '0 2',\n",
       " '0 23 25',\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['Predicted'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submit\n",
    "# submit = pd.read_csv('../data/sample_submission.csv')\n",
    "# predicted = []\n",
    "# draw_predict = []\n",
    "# # model.load_weights('../cache/InceptionV3.h5')\n",
    "# for name in tqdm(submit['Id']):\n",
    "#     path = os.path.join('../data/test/', name)\n",
    "#     image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "#     score_predict = model.predict(image[np.newaxis])[0]\n",
    "#     draw_predict.append(score_predict)\n",
    "#     label_predict = np.arange(28)[score_predict>=0.2]\n",
    "#     str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "#     predicted.append(str_predict_label)\n",
    "\n",
    "# submit['Predicted'] = predicted\n",
    "# np.save('../cache/draw_predict_InceptionV3-8.npy', score_predict)\n",
    "# submit.to_csv('../submissions/submit_InceptionV3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('../submissions/sub14-a.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/1855095/how-to-create-a-zip-archive-of-a-directory\n",
    "def backup_project_as_zip(project_dir, zip_file):\n",
    "    assert(os.path.isdir(project_dir))\n",
    "    assert(os.path.isdir(os.path.dirname(zip_file)))\n",
    "    shutil.make_archive(zip_file.replace('.zip',''), 'zip', project_dir)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-30 22:02:00.026549\n"
     ]
    }
   ],
   "source": [
    "import datetime, shutil\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "PROJECT_PATH = '/home/watts/lal/Kaggle/kagglehp/scripts_nbs'\n",
    "backup_project_as_zip(PROJECT_PATH, '../cache/code.scripts_nbs.%s.zip'%now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 492k/492k [00:12<00:00, 40.0kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 336 ms, sys: 153 ms, total: 488 ms\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub14-a.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub14-a.csv   2018-10-30 16:32:37               complete  0.452        None          \r\n",
      "sub13-c.csv   2018-10-29 19:24:00               complete  0.459        None          \r\n",
      "sub13-bb.csv  2018-10-29 19:23:01               complete  0.454        None          \r\n",
      "sub13-b.csv   2018-10-29 19:21:53               complete  0.448        None          \r\n",
      "sub13-a.csv   2018-10-29 19:20:40               complete  0.444        None          \r\n",
      "sub12-d.csv   2018-10-26 02:09:32               complete  0.466        None          \r\n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \r\n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \r\n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \r\n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \r\n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \r\n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \r\n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \r\n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \r\n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \r\n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \r\n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \r\n",
      "sub11-g.csv   2018-10-23 09:13:19               complete  0.347        None          \r\n",
      "sub11-f.csv   2018-10-23 09:11:15               complete  0.358        None          \r\n",
      "sub11-b.csv   2018-10-23 05:25:32               complete  0.437        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 98690.10it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for line in tqdm(sub_class_preds):\n",
    "    label_predict = np.arange(28)[line>=0.25]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub14-b.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-30 22:03:04.219937\n"
     ]
    }
   ],
   "source": [
    "import datetime, shutil\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "PROJECT_PATH = '/home/watts/lal/Kaggle/kagglehp/scripts_nbs'\n",
    "backup_project_as_zip(PROJECT_PATH, '../cache/code.scripts_nbs.%s.zip'%now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 486k/486k [00:12<00:00, 39.7kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 322 ms, sys: 168 ms, total: 491 ms\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub14-b.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub14-b.csv   2018-10-30 16:33:35               complete  0.459        None          \r\n",
      "sub14-a.csv   2018-10-30 16:32:37               complete  0.452        None          \r\n",
      "sub13-c.csv   2018-10-29 19:24:00               complete  0.459        None          \r\n",
      "sub13-bb.csv  2018-10-29 19:23:01               complete  0.454        None          \r\n",
      "sub13-b.csv   2018-10-29 19:21:53               complete  0.448        None          \r\n",
      "sub13-a.csv   2018-10-29 19:20:40               complete  0.444        None          \r\n",
      "sub12-d.csv   2018-10-26 02:09:32               complete  0.466        None          \r\n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \r\n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \r\n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \r\n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \r\n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \r\n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \r\n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \r\n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \r\n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \r\n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \r\n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \r\n",
      "sub11-g.csv   2018-10-23 09:13:19               complete  0.347        None          \r\n",
      "sub11-f.csv   2018-10-23 09:11:15               complete  0.358        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {0.3:'bb', 0.35:'c', 0.4:'d', 0.45:'e', 0.5:'f'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 104288.57it/s]\n",
      "100%|██████████| 11702/11702 [00:00<00:00, 119148.96it/s]\n",
      "  0%|          | 0/11702 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub14-bb.csv\n",
      "../submissions/sub14-c.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 124770.56it/s]\n",
      "100%|██████████| 11702/11702 [00:00<00:00, 126339.79it/s]\n",
      "  0%|          | 0/11702 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub14-d.csv\n",
      "../submissions/sub14-e.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 11702/11702 [00:00<00:00, 127874.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub14-f.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for alpha in [0.3, 0.35, 0.4, 0.45, 0.5]:\n",
    "    predicted = []\n",
    "    for line in tqdm(sub_class_preds):\n",
    "        label_predict = np.arange(28)[line>=alpha]\n",
    "        str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "        predicted.append(str_predict_label)\n",
    "    submit['Predicted'] = predicted\n",
    "    name = '../submissions/sub14-' + d[alpha] + '.csv'\n",
    "    print(name)\n",
    "    submit.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 482k/482k [00:08<00:00, 51.7kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 362 ms, sys: 160 ms, total: 522 ms\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub14-bb.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub14-bb.csv  2018-10-30 16:34:35               complete  0.461        None          \r\n",
      "sub14-b.csv   2018-10-30 16:33:35               complete  0.459        None          \r\n",
      "sub14-a.csv   2018-10-30 16:32:37               complete  0.452        None          \r\n",
      "sub13-c.csv   2018-10-29 19:24:00               complete  0.459        None          \r\n",
      "sub13-bb.csv  2018-10-29 19:23:01               complete  0.454        None          \r\n",
      "sub13-b.csv   2018-10-29 19:21:53               complete  0.448        None          \r\n",
      "sub13-a.csv   2018-10-29 19:20:40               complete  0.444        None          \r\n",
      "sub12-d.csv   2018-10-26 02:09:32               complete  0.466        None          \r\n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \r\n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \r\n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \r\n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \r\n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \r\n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \r\n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \r\n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \r\n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \r\n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \r\n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \r\n",
      "sub11-g.csv   2018-10-23 09:13:19               complete  0.347        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 478k/478k [00:08<00:00, 47.9kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationfileName      date                 description  status    publicScore  privateScore  \n",
      "------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub14-c.csv   2018-10-30 16:35:09               complete  0.466        None          \n",
      "sub14-bb.csv  2018-10-30 16:34:35               complete  0.461        None          \n",
      "sub14-b.csv   2018-10-30 16:33:35               complete  0.459        None          \n",
      "sub14-a.csv   2018-10-30 16:32:37               complete  0.452        None          \n",
      "sub13-c.csv   2018-10-29 19:24:00               complete  0.459        None          \n",
      "sub13-bb.csv  2018-10-29 19:23:01               complete  0.454        None          \n",
      "sub13-b.csv   2018-10-29 19:21:53               complete  0.448        None          \n",
      "sub13-a.csv   2018-10-29 19:20:40               complete  0.444        None          \n",
      "sub12-d.csv   2018-10-26 02:09:32               complete  0.466        None          \n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \n",
      "CPU times: user 351 ms, sys: 222 ms, total: 573 ms\n",
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub14-c.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 474k/474k [00:12<00:00, 39.1kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationfileName      date                 description  status    publicScore  privateScore  \n",
      "------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub14-d.csv   2018-10-30 16:36:00               complete  0.464        None          \n",
      "sub14-c.csv   2018-10-30 16:35:09               complete  0.466        None          \n",
      "sub14-bb.csv  2018-10-30 16:34:35               complete  0.461        None          \n",
      "sub14-b.csv   2018-10-30 16:33:35               complete  0.459        None          \n",
      "sub14-a.csv   2018-10-30 16:32:37               complete  0.452        None          \n",
      "sub13-c.csv   2018-10-29 19:24:00               complete  0.459        None          \n",
      "sub13-bb.csv  2018-10-29 19:23:01               complete  0.454        None          \n",
      "sub13-b.csv   2018-10-29 19:21:53               complete  0.448        None          \n",
      "sub13-a.csv   2018-10-29 19:20:40               complete  0.444        None          \n",
      "sub12-d.csv   2018-10-26 02:09:32               complete  0.466        None          \n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \n",
      "CPU times: user 333 ms, sys: 265 ms, total: 598 ms\n",
      "Wall time: 27.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub14-d.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31072, 28)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_class_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31072, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31072, 28)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = 28\n",
    "y_train = np.zeros(oof_class_preds.shape)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31072/31072 [00:00<00:00, 741901.66it/s]\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for labels in tqdm(data['Target'].str.split(' ')):\n",
    "#     print(labels)\n",
    "    for label in labels:\n",
    "        y_train[idx][int(label)] = 1\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 1., 0., 0.],\n",
       "       [1., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.6174081518752018\n",
      "0.01 0.6174081494907646\n",
      "0.1 0.6174079419635514\n",
      "0.5 0.6174049197331472\n",
      "0.75 0.6174023910597577\n",
      "1.0 0.6173997536697944\n",
      "------------------\n",
      "0.001 0.7042131450176463\n",
      "0.01 0.7042131449021258\n",
      "0.1 0.7042131341504188\n",
      "0.5 0.7042129258072632\n",
      "0.75 0.7042126913081677\n",
      "1.0 0.7042123865546952\n",
      "------------------\n",
      "0.001 0.6405293650278552\n",
      "0.01 0.6405293643199262\n",
      "0.1 0.6405293035770623\n",
      "0.5 0.6405284696126301\n",
      "0.75 0.6405278129689744\n",
      "1.0 0.6405271539313317\n",
      "------------------\n",
      "0.001 0.5206868082561187\n",
      "0.01 0.5206868064937693\n",
      "0.1 0.5206866550090807\n",
      "0.5 0.5206845661735036\n",
      "0.75 0.5206829204240566\n",
      "1.0 0.5206812723936773\n",
      "------------------\n",
      "0.001 0.6225629182679313\n",
      "0.01 0.6225628870674288\n",
      "0.1 0.6225602493284829\n",
      "0.5 0.6225265347174933\n",
      "0.75 0.6225024258166685\n",
      "1.0 0.6224802007531789\n",
      "------------------\n",
      "0.001 0.47625033808763034\n",
      "0.01 0.4762503310360723\n",
      "0.1 0.47624973372408597\n",
      "0.5 0.4762420258070793\n",
      "0.75 0.4762364393569697\n",
      "1.0 0.47623122503579396\n",
      "------------------\n",
      "0.001 0.37015383088158105\n",
      "0.01 0.37015383051625794\n",
      "0.1 0.37015379589885844\n",
      "0.5 0.370153120483382\n",
      "0.75 0.3701523989904022\n",
      "1.0 0.3701515220047934\n",
      "------------------\n",
      "0.001 0.6278499957612576\n",
      "0.01 0.6278499933196181\n",
      "0.1 0.6278497675145762\n",
      "0.5 0.6278457789099899\n",
      "0.75 0.6278419420023433\n",
      "1.0 0.627837666925348\n",
      "------------------\n",
      "0.001 0.23972549325385814\n",
      "0.01 0.23972546015299\n",
      "0.1 0.23972230652354265\n",
      "0.5 0.2396593031542591\n",
      "0.75 0.23959034757832143\n",
      "1.0 0.23950497349498\n",
      "------------------\n",
      "0.001 0.31331172323399825\n",
      "0.01 0.3133113149919019\n",
      "0.1 0.3132732609784179\n",
      "0.5 0.3125863700441769\n",
      "0.75 0.3119170265340445\n",
      "1.0 0.31116713928425954\n",
      "------------------\n",
      "0.001 0.2558623247962669\n",
      "0.01 0.255861948626774\n",
      "0.1 0.2558268879333644\n",
      "0.5 0.2551944259969803\n",
      "0.75 0.254578676028139\n",
      "1.0 0.2538894622998168\n",
      "------------------\n",
      "0.001 0.5968983087672983\n",
      "0.01 0.5968983084398413\n",
      "0.1 0.5968982782239397\n",
      "0.5 0.5968977292368627\n",
      "0.75 0.5968971633273094\n",
      "1.0 0.5968964809073818\n",
      "------------------\n",
      "0.001 0.49290871329017727\n",
      "0.01 0.49290871298539585\n",
      "0.1 0.4929086845398815\n",
      "0.5 0.49290813804804207\n",
      "0.75 0.49290753815710786\n",
      "1.0 0.49290677775529046\n",
      "------------------\n",
      "0.001 0.3787539379466851\n",
      "0.01 0.37875392012227116\n",
      "0.1 0.37875241060554954\n",
      "0.5 0.3787329277123833\n",
      "0.75 0.37871876766614176\n",
      "1.0 0.37870547804838306\n",
      "------------------\n",
      "0.001 0.7506973147934757\n",
      "0.01 0.7506973136319354\n",
      "0.1 0.7506972086330781\n",
      "0.5 0.7506954269694857\n",
      "0.75 0.7506936901241192\n",
      "1.0 0.7506916736611128\n",
      "------------------\n",
      "0.001 0.01983763392161686\n",
      "0.01 0.019837535325928135\n",
      "0.1 0.019828450862805713\n",
      "0.5 0.019665777176268495\n",
      "0.75 0.01950321718332293\n",
      "1.0 0.019314882518966292\n",
      "------------------\n",
      "0.001 0.1751297622786967\n",
      "0.01 0.17512974759033484\n",
      "0.1 0.17512846022244855\n",
      "0.5 0.17510915339406974\n",
      "0.75 0.17509248192232651\n",
      "1.0 0.17507468614395694\n",
      "------------------\n",
      "0.001 0.1667931781498947\n",
      "0.01 0.16679317509693092\n",
      "0.1 0.1667928880710675\n",
      "0.5 0.1667873460571777\n",
      "0.75 0.1667814074139947\n",
      "1.0 0.16677415822966601\n",
      "------------------\n",
      "0.001 0.3087568657202341\n",
      "0.01 0.3087568642759011\n",
      "0.1 0.3087567290489941\n",
      "0.5 0.3087542387474458\n",
      "0.75 0.30875175512550346\n",
      "1.0 0.3087489153085835\n",
      "------------------\n",
      "0.001 0.38082130482961174\n",
      "0.01 0.380821304671176\n",
      "0.1 0.38082128970689655\n",
      "0.5 0.38082100099719085\n",
      "0.75 0.38082069536892904\n",
      "1.0 0.3808203259166289\n",
      "------------------\n",
      "0.001 0.1538312914148493\n",
      "0.01 0.15383122528024018\n",
      "0.1 0.1538250686849436\n",
      "0.5 0.15371444612700225\n",
      "0.75 0.15360709884995571\n",
      "1.0 0.15348721808287602\n",
      "------------------\n",
      "0.001 0.48557824579010384\n",
      "0.01 0.48557824528334803\n",
      "0.1 0.4855781985932974\n",
      "0.5 0.48557738349917456\n",
      "0.75 0.48557660601055197\n",
      "1.0 0.48557574340418097\n",
      "------------------\n",
      "0.001 0.3646582386899322\n",
      "0.01 0.3646582372378981\n",
      "0.1 0.364658112376793\n",
      "0.5 0.36465637881526125\n",
      "0.75 0.3646549894024714\n",
      "1.0 0.364653566661458\n",
      "------------------\n",
      "0.001 0.6577178864298491\n",
      "0.01 0.6577178855069651\n",
      "0.1 0.6577177994042775\n",
      "0.5 0.6577162388191071\n",
      "0.75 0.657714710592422\n",
      "1.0 0.6577129906824652\n",
      "------------------\n",
      "0.001 0.48347736718545864\n",
      "0.01 0.4834773624770783\n",
      "0.1 0.4834769405428032\n",
      "0.5 0.4834700745036658\n",
      "0.75 0.48346367708679355\n",
      "1.0 0.483456474479732\n",
      "------------------\n",
      "0.001 0.43786458026883995\n",
      "0.01 0.43786457972979137\n",
      "0.1 0.4378645337127519\n",
      "0.5 0.4378639170133599\n",
      "0.75 0.4378634463465395\n",
      "1.0 0.43786298621777675\n",
      "------------------\n",
      "0.001 0.21837253521077882\n",
      "0.01 0.2183724949425644\n",
      "0.1 0.21836905357883252\n",
      "0.5 0.21832290898991366\n",
      "0.75 0.21828791269733872\n",
      "1.0 0.21825408221249284\n",
      "------------------\n",
      "0.001 0.0019063946880274463\n",
      "0.01 0.0019063937044974109\n",
      "0.1 0.0019063073388934093\n",
      "0.5 0.0019050204660372438\n",
      "0.75 0.0019039365114243976\n",
      "1.0 0.0019028140324179876\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "sub_ridge_preds = np.zeros(sub_class_preds.shape)\n",
    "from sklearn.linear_model import Ridge\n",
    "for cls in np.arange(n_classes):\n",
    "    for alpha in [0.001, 0.01, 0.1, 0.5, 0.75, 1.0]:\n",
    "        y = y_train[:, cls]\n",
    "        X = oof_class_preds\n",
    "        clf = Ridge(alpha=alpha)\n",
    "        clf.fit(X,y)\n",
    "        score_ = clf.score(X, y)\n",
    "        print(alpha, score_)\n",
    "    print('------------------')\n",
    "#         X_test = sub_class_preds[:, cls]\n",
    "#         preds_ = clf.predict(X_test)\n",
    "#         sub_ridge_preds[:,cls] = preds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ridge_preds = np.zeros(sub_class_preds.shape)\n",
    "from sklearn.linear_model import Ridge\n",
    "for cls in np.arange(n_classes):\n",
    "    y = y_train[:, cls]\n",
    "    X = oof_class_preds\n",
    "    clf = Ridge(alpha=0.1)\n",
    "    clf.fit(X,y)\n",
    "    X_test = sub_class_preds\n",
    "    preds_ = clf.predict(X_test)\n",
    "    sub_ridge_preds[:,cls] = preds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.38001867e-02, 1.59430779e-04, 9.98871672e-01, ...,\n",
       "        1.77784316e-03, 1.12266340e-04, 5.20836841e-09],\n",
       "       [2.47062426e-02, 2.68127583e-04, 7.85995722e-04, ...,\n",
       "        6.51888692e-01, 8.71524611e-04, 3.26655725e-05],\n",
       "       [8.41529155e-01, 2.72278007e-04, 3.96186303e-03, ...,\n",
       "        9.23864961e-01, 2.01543609e-03, 1.92988443e-05],\n",
       "       ...,\n",
       "       [6.59056642e-04, 5.12143007e-05, 3.49444263e-05, ...,\n",
       "        1.77463120e-03, 5.20864920e-08, 5.91902866e-09],\n",
       "       [5.01914832e-01, 9.99162483e-01, 2.73049554e-03, ...,\n",
       "        1.16331837e-02, 1.53418808e-04, 1.20560289e-06],\n",
       "       [5.07521251e-01, 3.52088286e-04, 3.01849514e-03, ...,\n",
       "        6.92711103e-01, 1.99362053e-03, 1.05607675e-06]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_class_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11702,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_class_preds[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.26424064e-01,  8.84581981e-03,  8.85833838e-01, ...,\n",
       "         3.20592330e-02,  4.54385428e-03, -6.92529375e-05],\n",
       "       [ 5.51528260e-02,  2.13827355e-03,  1.63897473e-02, ...,\n",
       "         5.38178358e-01,  1.06812025e-03,  1.39571143e-03],\n",
       "       [ 7.77233272e-01, -1.67743788e-03,  1.87791802e-02, ...,\n",
       "         7.51411388e-01, -1.45306321e-03,  1.27131495e-03],\n",
       "       ...,\n",
       "       [ 2.62432560e-02,  4.66381707e-04,  7.97892029e-03, ...,\n",
       "         2.92185935e-02,  3.12140745e-03,  1.76712611e-04],\n",
       "       [ 4.95625470e-01,  8.71181686e-01,  1.99755452e-02, ...,\n",
       "         3.70837162e-02,  3.03441473e-03,  3.03600332e-05],\n",
       "       [ 4.96285545e-01,  1.04218405e-02,  2.67882221e-02, ...,\n",
       "         5.85205684e-01,  4.37290352e-03,  1.52204091e-06]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_ridge_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 88242.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub12-g.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "alpha = 0.35\n",
    "for line in tqdm(sub_ridge_preds):\n",
    "    label_predict = np.arange(28)[line>=alpha]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "name = '../submissions/sub12-g.csv'\n",
    "print(name)\n",
    "submit.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.3.8)\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 372 ms, sys: 201 ms, total: 573 ms\n",
      "Wall time: 17.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub12-g.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.3.8)\r\n",
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \r\n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \r\n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \r\n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \r\n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \r\n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \r\n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \r\n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \r\n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \r\n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \r\n",
      "sub11-g.csv   2018-10-23 09:13:19               complete  0.347        None          \r\n",
      "sub11-f.csv   2018-10-23 09:11:15               complete  0.358        None          \r\n",
      "sub11-b.csv   2018-10-23 05:25:32               complete  0.437        None          \r\n",
      "sub12.csv     2018-10-23 05:18:36               complete  0.436        None          \r\n",
      "sub11.csv     2018-10-23 01:07:18               complete  0.431        None          \r\n",
      "sub10.csv     2018-10-22 17:16:40               complete  0.336        None          \r\n",
      "sub9.csv      2018-10-21 20:04:09               complete  0.098        None          \r\n",
      "sub9.csv      2018-10-21 19:44:17               complete  0.073        None          \r\n",
      "sub9.csv      2018-10-21 19:37:30               complete  0.043        None          \r\n",
      "sub9.csv      2018-10-21 19:27:51               complete  0.000        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 79991.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub12-h.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "alpha = 0.4\n",
    "for line in tqdm(sub_ridge_preds):\n",
    "    label_predict = np.arange(28)[line>=alpha]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "name = '../submissions/sub12-h.csv'\n",
    "print(name)\n",
    "submit.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.0 / client 1.3.8)\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationWarning: Looks like you're using an outdated API Version, please consider updating (server 1.5.0 / client 1.3.8)\n",
      "fileName      date                 description  status    publicScore  privateScore  \n",
      "------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \n",
      "sub11-g.csv   2018-10-23 09:13:19               complete  0.347        None          \n",
      "sub11-f.csv   2018-10-23 09:11:15               complete  0.358        None          \n",
      "sub11-b.csv   2018-10-23 05:25:32               complete  0.437        None          \n",
      "sub12.csv     2018-10-23 05:18:36               complete  0.436        None          \n",
      "sub11.csv     2018-10-23 01:07:18               complete  0.431        None          \n",
      "sub10.csv     2018-10-22 17:16:40               complete  0.336        None          \n",
      "sub9.csv      2018-10-21 20:04:09               complete  0.098        None          \n",
      "sub9.csv      2018-10-21 19:44:17               complete  0.073        None          \n",
      "sub9.csv      2018-10-21 19:37:30               complete  0.043        None          \n",
      "CPU times: user 328 ms, sys: 282 ms, total: 610 ms\n",
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub12-h.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.0 / client 1.3.8)\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationWarning: Looks like you're using an outdated API Version, please consider updating (server 1.5.0 / client 1.3.8)\n",
      "fileName      date                 description  status    publicScore  privateScore  \n",
      "------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub12-d.csv   2018-10-26 02:09:32               complete  0.466        None          \n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \n",
      "sub11-g.csv   2018-10-23 09:13:19               complete  0.347        None          \n",
      "sub11-f.csv   2018-10-23 09:11:15               complete  0.358        None          \n",
      "sub11-b.csv   2018-10-23 05:25:32               complete  0.437        None          \n",
      "sub12.csv     2018-10-23 05:18:36               complete  0.436        None          \n",
      "sub11.csv     2018-10-23 01:07:18               complete  0.431        None          \n",
      "sub10.csv     2018-10-22 17:16:40               complete  0.336        None          \n",
      "sub9.csv      2018-10-21 20:04:09               complete  0.098        None          \n",
      "sub9.csv      2018-10-21 19:44:17               complete  0.073        None          \n",
      "CPU times: user 352 ms, sys: 328 ms, total: 680 ms\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub12-d.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpg",
   "language": "python",
   "name": "hpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/mathormad/inceptionv3-baseline-lb-0-379/code\n",
    "# fork of scratch8, 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "import keras_metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) + (1 - K.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "path_to_train = '../data/train/'\n",
    "data = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_info = []\n",
    "for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "    train_dataset_info.append({\n",
    "        'path':os.path.join(path_to_train, name),\n",
    "        'labels':np.array([int(label) for label in labels])})\n",
    "train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'path': '../data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0', 'labels': array([16,  0])},\n",
       "       {'path': '../data/train/000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', 'labels': array([7, 1, 2, 0])},\n",
       "       {'path': '../data/train/000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([5])},\n",
       "       ...,\n",
       "       {'path': '../data/train/fff189d8-bbab-11e8-b2ba-ac1f6b6435d0', 'labels': array([7])},\n",
       "       {'path': '../data/train/fffdf7e0-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([25,  2, 21])},\n",
       "       {'path': '../data/train/fffe0ffe-bbc0-11e8-b2bb-ac1f6b6435d0', 'labels': array([2, 0])}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            dataset_info = shuffle(dataset_info)\n",
    "            for start in range(0, len(dataset_info), batch_size):\n",
    "                end = min(start + batch_size, len(dataset_info))\n",
    "                batch_images = []\n",
    "                X_train_batch = dataset_info[start:end]\n",
    "                batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "                for i in range(len(X_train_batch)):\n",
    "                    image = data_generator.load_image(\n",
    "                        X_train_batch[i]['path'], shape)\n",
    "#                     image = tdi[i+start]\n",
    "#                     image = cv2.resize(image, (shape[0], shape[1]))\n",
    "                    if augument:\n",
    "                        image = data_generator.augment(image)\n",
    "                    batch_images.append(image/255.)\n",
    "                    batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "                yield np.array(batch_images, np.float32), [batch_labels, np.array(batch_images, np.float32)]\n",
    "\n",
    "    def load_image(path, shape):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image1 = np.stack((\n",
    "            np.array(image_red_ch),\n",
    "            np.array(image_green_ch), \n",
    "            np.array(image_blue_ch)), -1)\n",
    "        w, h = 512, 512\n",
    "#         zero_data = np.zeros((h, w), dtype=np.uint8)\n",
    "#         image2 = np.stack((\n",
    "#             np.array(image_red_ch),\n",
    "#             np.array(image_green_ch), \n",
    "#             np.array(image_yellow_ch)), -1)\n",
    "#         image3 = np.stack((\n",
    "#             np.array(image_yellow_ch),\n",
    "#             np.array(image_green_ch), \n",
    "#             np.array(image_blue_ch)), -1)\n",
    "# #         print(image1.shape, image2.shape)\n",
    "#         image = np.vstack((image1, image2, image3))\n",
    "#         print(image.shape)\n",
    "        image =image1\n",
    "#         image = canny_image4(image1)\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    \n",
    "    def load_image2(path, shape):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image1 = np.stack((\n",
    "            np.array(image_red_ch),\n",
    "            np.array(image_green_ch), \n",
    "            np.array(image_blue_ch)), -1)\n",
    "        w, h = 512, 512\n",
    "#         zero_data = np.zeros((h, w), dtype=np.uint8)\n",
    "#         image2 = np.stack((\n",
    "#             np.array(image_red_ch),\n",
    "#             np.array(image_green_ch), \n",
    "#             np.array(image_yellow_ch)), -1)\n",
    "#         image3 = np.stack((\n",
    "#             np.array(image_yellow_ch),\n",
    "#             np.array(image_green_ch), \n",
    "#             np.array(image_blue_ch)), -1)\n",
    "# #         print(image1.shape, image2.shape)\n",
    "#         image = np.vstack((image1, image2, image3))\n",
    "#         print(image.shape)\n",
    "        image =image1\n",
    "#         image = canny_image4(image1)\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    \n",
    "    def augment(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=0),\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "            ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n",
    "    def augment2(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                    iaa.Fliplr(0.5), # horizontal flips\n",
    "                    iaa.Affine(rotate=0),\n",
    "                    iaa.Affine(rotate=90),\n",
    "                    iaa.Affine(rotate=180),\n",
    "                    iaa.Affine(rotate=270),\n",
    "                    iaa.Flipud(0.5),\n",
    "                    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "                    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "                    # But we only blur about 50% of all images.\n",
    "                    iaa.Sometimes(0.5,\n",
    "                        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "                    ),\n",
    "                    # Strengthen or weaken the contrast in each image.\n",
    "                    iaa.ContrastNormalization((0.75, 1.5)),\n",
    "                    # Add gaussian noise.\n",
    "                    # For 50% of all images, we sample the noise once per pixel.\n",
    "                    # For the other 50% of all images, we sample the noise per pixel AND\n",
    "                    # channel. This can change the color (not only brightness) of the\n",
    "                    # pixels.\n",
    "                    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "                    # Make some images brighter and some darker.\n",
    "                    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "                    # which can end up changing the color of the images.\n",
    "                    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "                    # Apply affine transformations to each image.\n",
    "                    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "                    iaa.Affine(\n",
    "                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                        rotate=(-180, 180),\n",
    "                        shear=(-8, 8)\n",
    "                    )\n",
    "                ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras.layers import Input, Conv2D, Reshape, UpSampling2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = InceptionV3(include_top=False,\n",
    "#                    weights='imagenet',\n",
    "#                    input_shape=(512,512,3))\n",
    "# #     bn = BatchNormalization()(input_tensor)\n",
    "# model.summary()\n",
    "# Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "# ==================================================================================================\n",
    "# input_4 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_408 (Conv2D)             (None, 255, 255, 32) 864         input_4[0][0]                    \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_408 (BatchN (None, 255, 255, 32) 96          conv2d_408[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_407 (Activation)     (None, 255, 255, 32) 0           batch_normalization_408[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_409 (Conv2D)             (None, 253, 253, 32) 9216        activation_407[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_409 (BatchN (None, 253, 253, 32) 96          conv2d_409[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_408 (Activation)     (None, 253, 253, 32) 0           batch_normalization_409[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_410 (Conv2D)             (None, 253, 253, 64) 18432       activation_408[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_410 (BatchN (None, 253, 253, 64) 192         conv2d_410[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_409 (Activation)     (None, 253, 253, 64) 0           batch_normalization_410[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# max_pooling2d_9 (MaxPooling2D)  (None, 126, 126, 64) 0           activation_409[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_411 (Conv2D)             (None, 126, 126, 80) 5120        max_pooling2d_9[0][0]            \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_411 (BatchN (None, 126, 126, 80) 240         conv2d_411[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_410 (Activation)     (None, 126, 126, 80) 0           batch_normalization_411[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_412 (Conv2D)             (None, 124, 124, 192 138240      activation_410[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_412 (BatchN (None, 124, 124, 192 576         conv2d_412[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_411 (Activation)     (None, 124, 124, 192 0           batch_normalization_412[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# max_pooling2d_10 (MaxPooling2D) (None, 61, 61, 192)  0           activation_411[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_416 (Conv2D)             (None, 61, 61, 64)   12288       max_pooling2d_10[0][0]           \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_416 (BatchN (None, 61, 61, 64)   192         conv2d_416[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_415 (Activation)     (None, 61, 61, 64)   0           batch_normalization_416[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_414 (Conv2D)             (None, 61, 61, 48)   9216        max_pooling2d_10[0][0]           \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_417 (Conv2D)             (None, 61, 61, 96)   55296       activation_415[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_414 (BatchN (None, 61, 61, 48)   144         conv2d_414[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_417 (BatchN (None, 61, 61, 96)   288         conv2d_417[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_413 (Activation)     (None, 61, 61, 48)   0           batch_normalization_414[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_416 (Activation)     (None, 61, 61, 96)   0           batch_normalization_417[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_3 (AveragePoo (None, 61, 61, 192)  0           max_pooling2d_10[0][0]           \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_413 (Conv2D)             (None, 61, 61, 64)   12288       max_pooling2d_10[0][0]           \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_415 (Conv2D)             (None, 61, 61, 64)   76800       activation_413[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_418 (Conv2D)             (None, 61, 61, 96)   82944       activation_416[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_419 (Conv2D)             (None, 61, 61, 32)   6144        average_pooling2d_3[0][0]        \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_413 (BatchN (None, 61, 61, 64)   192         conv2d_413[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_415 (BatchN (None, 61, 61, 64)   192         conv2d_415[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_418 (BatchN (None, 61, 61, 96)   288         conv2d_418[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_419 (BatchN (None, 61, 61, 32)   96          conv2d_419[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_412 (Activation)     (None, 61, 61, 64)   0           batch_normalization_413[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_414 (Activation)     (None, 61, 61, 64)   0           batch_normalization_415[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_417 (Activation)     (None, 61, 61, 96)   0           batch_normalization_418[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_418 (Activation)     (None, 61, 61, 32)   0           batch_normalization_419[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed0 (Concatenate)            (None, 61, 61, 256)  0           activation_412[0][0]             \n",
    "#                                                                  activation_414[0][0]             \n",
    "#                                                                  activation_417[0][0]             \n",
    "#                                                                  activation_418[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_423 (Conv2D)             (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_423 (BatchN (None, 61, 61, 64)   192         conv2d_423[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_422 (Activation)     (None, 61, 61, 64)   0           batch_normalization_423[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_421 (Conv2D)             (None, 61, 61, 48)   12288       mixed0[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_424 (Conv2D)             (None, 61, 61, 96)   55296       activation_422[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_421 (BatchN (None, 61, 61, 48)   144         conv2d_421[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_424 (BatchN (None, 61, 61, 96)   288         conv2d_424[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_420 (Activation)     (None, 61, 61, 48)   0           batch_normalization_421[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_423 (Activation)     (None, 61, 61, 96)   0           batch_normalization_424[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_4 (AveragePoo (None, 61, 61, 256)  0           mixed0[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_420 (Conv2D)             (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_422 (Conv2D)             (None, 61, 61, 64)   76800       activation_420[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_425 (Conv2D)             (None, 61, 61, 96)   82944       activation_423[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_426 (Conv2D)             (None, 61, 61, 64)   16384       average_pooling2d_4[0][0]        \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_420 (BatchN (None, 61, 61, 64)   192         conv2d_420[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_422 (BatchN (None, 61, 61, 64)   192         conv2d_422[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_425 (BatchN (None, 61, 61, 96)   288         conv2d_425[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_426 (BatchN (None, 61, 61, 64)   192         conv2d_426[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_419 (Activation)     (None, 61, 61, 64)   0           batch_normalization_420[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_421 (Activation)     (None, 61, 61, 64)   0           batch_normalization_422[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_424 (Activation)     (None, 61, 61, 96)   0           batch_normalization_425[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_425 (Activation)     (None, 61, 61, 64)   0           batch_normalization_426[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed1 (Concatenate)            (None, 61, 61, 288)  0           activation_419[0][0]             \n",
    "#                                                                  activation_421[0][0]             \n",
    "#                                                                  activation_424[0][0]             \n",
    "#                                                                  activation_425[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_430 (Conv2D)             (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_430 (BatchN (None, 61, 61, 64)   192         conv2d_430[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_429 (Activation)     (None, 61, 61, 64)   0           batch_normalization_430[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_428 (Conv2D)             (None, 61, 61, 48)   13824       mixed1[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_431 (Conv2D)             (None, 61, 61, 96)   55296       activation_429[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_428 (BatchN (None, 61, 61, 48)   144         conv2d_428[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_431 (BatchN (None, 61, 61, 96)   288         conv2d_431[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_427 (Activation)     (None, 61, 61, 48)   0           batch_normalization_428[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_430 (Activation)     (None, 61, 61, 96)   0           batch_normalization_431[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_5 (AveragePoo (None, 61, 61, 288)  0           mixed1[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_427 (Conv2D)             (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_429 (Conv2D)             (None, 61, 61, 64)   76800       activation_427[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_432 (Conv2D)             (None, 61, 61, 96)   82944       activation_430[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_433 (Conv2D)             (None, 61, 61, 64)   18432       average_pooling2d_5[0][0]        \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_427 (BatchN (None, 61, 61, 64)   192         conv2d_427[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_429 (BatchN (None, 61, 61, 64)   192         conv2d_429[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_432 (BatchN (None, 61, 61, 96)   288         conv2d_432[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_433 (BatchN (None, 61, 61, 64)   192         conv2d_433[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_426 (Activation)     (None, 61, 61, 64)   0           batch_normalization_427[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_428 (Activation)     (None, 61, 61, 64)   0           batch_normalization_429[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_431 (Activation)     (None, 61, 61, 96)   0           batch_normalization_432[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_432 (Activation)     (None, 61, 61, 64)   0           batch_normalization_433[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed2 (Concatenate)            (None, 61, 61, 288)  0           activation_426[0][0]             \n",
    "#                                                                  activation_428[0][0]             \n",
    "#                                                                  activation_431[0][0]             \n",
    "#                                                                  activation_432[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_435 (Conv2D)             (None, 61, 61, 64)   18432       mixed2[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_435 (BatchN (None, 61, 61, 64)   192         conv2d_435[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_434 (Activation)     (None, 61, 61, 64)   0           batch_normalization_435[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_436 (Conv2D)             (None, 61, 61, 96)   55296       activation_434[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_436 (BatchN (None, 61, 61, 96)   288         conv2d_436[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_435 (Activation)     (None, 61, 61, 96)   0           batch_normalization_436[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_434 (Conv2D)             (None, 30, 30, 384)  995328      mixed2[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_437 (Conv2D)             (None, 30, 30, 96)   82944       activation_435[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_434 (BatchN (None, 30, 30, 384)  1152        conv2d_434[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_437 (BatchN (None, 30, 30, 96)   288         conv2d_437[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_433 (Activation)     (None, 30, 30, 384)  0           batch_normalization_434[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_436 (Activation)     (None, 30, 30, 96)   0           batch_normalization_437[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# max_pooling2d_11 (MaxPooling2D) (None, 30, 30, 288)  0           mixed2[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed3 (Concatenate)            (None, 30, 30, 768)  0           activation_433[0][0]             \n",
    "#                                                                  activation_436[0][0]             \n",
    "#                                                                  max_pooling2d_11[0][0]           \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_442 (Conv2D)             (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_442 (BatchN (None, 30, 30, 128)  384         conv2d_442[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_441 (Activation)     (None, 30, 30, 128)  0           batch_normalization_442[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_443 (Conv2D)             (None, 30, 30, 128)  114688      activation_441[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_443 (BatchN (None, 30, 30, 128)  384         conv2d_443[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_442 (Activation)     (None, 30, 30, 128)  0           batch_normalization_443[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_439 (Conv2D)             (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_444 (Conv2D)             (None, 30, 30, 128)  114688      activation_442[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_439 (BatchN (None, 30, 30, 128)  384         conv2d_439[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_444 (BatchN (None, 30, 30, 128)  384         conv2d_444[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_438 (Activation)     (None, 30, 30, 128)  0           batch_normalization_439[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_443 (Activation)     (None, 30, 30, 128)  0           batch_normalization_444[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_440 (Conv2D)             (None, 30, 30, 128)  114688      activation_438[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_445 (Conv2D)             (None, 30, 30, 128)  114688      activation_443[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_440 (BatchN (None, 30, 30, 128)  384         conv2d_440[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_445 (BatchN (None, 30, 30, 128)  384         conv2d_445[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_439 (Activation)     (None, 30, 30, 128)  0           batch_normalization_440[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_444 (Activation)     (None, 30, 30, 128)  0           batch_normalization_445[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_6 (AveragePoo (None, 30, 30, 768)  0           mixed3[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_438 (Conv2D)             (None, 30, 30, 192)  147456      mixed3[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_441 (Conv2D)             (None, 30, 30, 192)  172032      activation_439[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_446 (Conv2D)             (None, 30, 30, 192)  172032      activation_444[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_447 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_6[0][0]        \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_438 (BatchN (None, 30, 30, 192)  576         conv2d_438[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_441 (BatchN (None, 30, 30, 192)  576         conv2d_441[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_446 (BatchN (None, 30, 30, 192)  576         conv2d_446[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_447 (BatchN (None, 30, 30, 192)  576         conv2d_447[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_437 (Activation)     (None, 30, 30, 192)  0           batch_normalization_438[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_440 (Activation)     (None, 30, 30, 192)  0           batch_normalization_441[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_445 (Activation)     (None, 30, 30, 192)  0           batch_normalization_446[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_446 (Activation)     (None, 30, 30, 192)  0           batch_normalization_447[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed4 (Concatenate)            (None, 30, 30, 768)  0           activation_437[0][0]             \n",
    "#                                                                  activation_440[0][0]             \n",
    "#                                                                  activation_445[0][0]             \n",
    "#                                                                  activation_446[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_452 (Conv2D)             (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_452 (BatchN (None, 30, 30, 160)  480         conv2d_452[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_451 (Activation)     (None, 30, 30, 160)  0           batch_normalization_452[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_453 (Conv2D)             (None, 30, 30, 160)  179200      activation_451[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_453 (BatchN (None, 30, 30, 160)  480         conv2d_453[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_452 (Activation)     (None, 30, 30, 160)  0           batch_normalization_453[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_449 (Conv2D)             (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_454 (Conv2D)             (None, 30, 30, 160)  179200      activation_452[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_449 (BatchN (None, 30, 30, 160)  480         conv2d_449[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_454 (BatchN (None, 30, 30, 160)  480         conv2d_454[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_448 (Activation)     (None, 30, 30, 160)  0           batch_normalization_449[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_453 (Activation)     (None, 30, 30, 160)  0           batch_normalization_454[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_450 (Conv2D)             (None, 30, 30, 160)  179200      activation_448[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_455 (Conv2D)             (None, 30, 30, 160)  179200      activation_453[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_450 (BatchN (None, 30, 30, 160)  480         conv2d_450[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_455 (BatchN (None, 30, 30, 160)  480         conv2d_455[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_449 (Activation)     (None, 30, 30, 160)  0           batch_normalization_450[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_454 (Activation)     (None, 30, 30, 160)  0           batch_normalization_455[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_7 (AveragePoo (None, 30, 30, 768)  0           mixed4[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_448 (Conv2D)             (None, 30, 30, 192)  147456      mixed4[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_451 (Conv2D)             (None, 30, 30, 192)  215040      activation_449[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_456 (Conv2D)             (None, 30, 30, 192)  215040      activation_454[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_457 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_7[0][0]        \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_448 (BatchN (None, 30, 30, 192)  576         conv2d_448[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_451 (BatchN (None, 30, 30, 192)  576         conv2d_451[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_456 (BatchN (None, 30, 30, 192)  576         conv2d_456[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_457 (BatchN (None, 30, 30, 192)  576         conv2d_457[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_447 (Activation)     (None, 30, 30, 192)  0           batch_normalization_448[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_450 (Activation)     (None, 30, 30, 192)  0           batch_normalization_451[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_455 (Activation)     (None, 30, 30, 192)  0           batch_normalization_456[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_456 (Activation)     (None, 30, 30, 192)  0           batch_normalization_457[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed5 (Concatenate)            (None, 30, 30, 768)  0           activation_447[0][0]             \n",
    "#                                                                  activation_450[0][0]             \n",
    "#                                                                  activation_455[0][0]             \n",
    "#                                                                  activation_456[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_462 (Conv2D)             (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_462 (BatchN (None, 30, 30, 160)  480         conv2d_462[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_461 (Activation)     (None, 30, 30, 160)  0           batch_normalization_462[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_463 (Conv2D)             (None, 30, 30, 160)  179200      activation_461[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_463 (BatchN (None, 30, 30, 160)  480         conv2d_463[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_462 (Activation)     (None, 30, 30, 160)  0           batch_normalization_463[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_459 (Conv2D)             (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_464 (Conv2D)             (None, 30, 30, 160)  179200      activation_462[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_459 (BatchN (None, 30, 30, 160)  480         conv2d_459[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_464 (BatchN (None, 30, 30, 160)  480         conv2d_464[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_458 (Activation)     (None, 30, 30, 160)  0           batch_normalization_459[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_463 (Activation)     (None, 30, 30, 160)  0           batch_normalization_464[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_460 (Conv2D)             (None, 30, 30, 160)  179200      activation_458[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_465 (Conv2D)             (None, 30, 30, 160)  179200      activation_463[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_460 (BatchN (None, 30, 30, 160)  480         conv2d_460[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_465 (BatchN (None, 30, 30, 160)  480         conv2d_465[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_459 (Activation)     (None, 30, 30, 160)  0           batch_normalization_460[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_464 (Activation)     (None, 30, 30, 160)  0           batch_normalization_465[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_8 (AveragePoo (None, 30, 30, 768)  0           mixed5[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_458 (Conv2D)             (None, 30, 30, 192)  147456      mixed5[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_461 (Conv2D)             (None, 30, 30, 192)  215040      activation_459[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_466 (Conv2D)             (None, 30, 30, 192)  215040      activation_464[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_467 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_8[0][0]        \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_458 (BatchN (None, 30, 30, 192)  576         conv2d_458[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_461 (BatchN (None, 30, 30, 192)  576         conv2d_461[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_466 (BatchN (None, 30, 30, 192)  576         conv2d_466[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_467 (BatchN (None, 30, 30, 192)  576         conv2d_467[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_457 (Activation)     (None, 30, 30, 192)  0           batch_normalization_458[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_460 (Activation)     (None, 30, 30, 192)  0           batch_normalization_461[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_465 (Activation)     (None, 30, 30, 192)  0           batch_normalization_466[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_466 (Activation)     (None, 30, 30, 192)  0           batch_normalization_467[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed6 (Concatenate)            (None, 30, 30, 768)  0           activation_457[0][0]             \n",
    "#                                                                  activation_460[0][0]             \n",
    "#                                                                  activation_465[0][0]             \n",
    "#                                                                  activation_466[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_472 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_472 (BatchN (None, 30, 30, 192)  576         conv2d_472[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_471 (Activation)     (None, 30, 30, 192)  0           batch_normalization_472[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_473 (Conv2D)             (None, 30, 30, 192)  258048      activation_471[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_473 (BatchN (None, 30, 30, 192)  576         conv2d_473[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_472 (Activation)     (None, 30, 30, 192)  0           batch_normalization_473[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_469 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_474 (Conv2D)             (None, 30, 30, 192)  258048      activation_472[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_469 (BatchN (None, 30, 30, 192)  576         conv2d_469[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_474 (BatchN (None, 30, 30, 192)  576         conv2d_474[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_468 (Activation)     (None, 30, 30, 192)  0           batch_normalization_469[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_473 (Activation)     (None, 30, 30, 192)  0           batch_normalization_474[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_470 (Conv2D)             (None, 30, 30, 192)  258048      activation_468[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_475 (Conv2D)             (None, 30, 30, 192)  258048      activation_473[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_470 (BatchN (None, 30, 30, 192)  576         conv2d_470[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_475 (BatchN (None, 30, 30, 192)  576         conv2d_475[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_469 (Activation)     (None, 30, 30, 192)  0           batch_normalization_470[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_474 (Activation)     (None, 30, 30, 192)  0           batch_normalization_475[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_9 (AveragePoo (None, 30, 30, 768)  0           mixed6[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_468 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_471 (Conv2D)             (None, 30, 30, 192)  258048      activation_469[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_476 (Conv2D)             (None, 30, 30, 192)  258048      activation_474[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_477 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_9[0][0]        \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_468 (BatchN (None, 30, 30, 192)  576         conv2d_468[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_471 (BatchN (None, 30, 30, 192)  576         conv2d_471[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_476 (BatchN (None, 30, 30, 192)  576         conv2d_476[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_477 (BatchN (None, 30, 30, 192)  576         conv2d_477[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_467 (Activation)     (None, 30, 30, 192)  0           batch_normalization_468[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_470 (Activation)     (None, 30, 30, 192)  0           batch_normalization_471[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_475 (Activation)     (None, 30, 30, 192)  0           batch_normalization_476[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_476 (Activation)     (None, 30, 30, 192)  0           batch_normalization_477[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed7 (Concatenate)            (None, 30, 30, 768)  0           activation_467[0][0]             \n",
    "#                                                                  activation_470[0][0]             \n",
    "#                                                                  activation_475[0][0]             \n",
    "#                                                                  activation_476[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_480 (Conv2D)             (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_480 (BatchN (None, 30, 30, 192)  576         conv2d_480[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_479 (Activation)     (None, 30, 30, 192)  0           batch_normalization_480[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_481 (Conv2D)             (None, 30, 30, 192)  258048      activation_479[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_481 (BatchN (None, 30, 30, 192)  576         conv2d_481[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_480 (Activation)     (None, 30, 30, 192)  0           batch_normalization_481[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_478 (Conv2D)             (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_482 (Conv2D)             (None, 30, 30, 192)  258048      activation_480[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_478 (BatchN (None, 30, 30, 192)  576         conv2d_478[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_482 (BatchN (None, 30, 30, 192)  576         conv2d_482[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_477 (Activation)     (None, 30, 30, 192)  0           batch_normalization_478[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_481 (Activation)     (None, 30, 30, 192)  0           batch_normalization_482[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_479 (Conv2D)             (None, 14, 14, 320)  552960      activation_477[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_483 (Conv2D)             (None, 14, 14, 192)  331776      activation_481[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_479 (BatchN (None, 14, 14, 320)  960         conv2d_479[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_483 (BatchN (None, 14, 14, 192)  576         conv2d_483[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_478 (Activation)     (None, 14, 14, 320)  0           batch_normalization_479[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_482 (Activation)     (None, 14, 14, 192)  0           batch_normalization_483[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# max_pooling2d_12 (MaxPooling2D) (None, 14, 14, 768)  0           mixed7[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed8 (Concatenate)            (None, 14, 14, 1280) 0           activation_478[0][0]             \n",
    "#                                                                  activation_482[0][0]             \n",
    "#                                                                  max_pooling2d_12[0][0]           \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_488 (Conv2D)             (None, 14, 14, 448)  573440      mixed8[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_488 (BatchN (None, 14, 14, 448)  1344        conv2d_488[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_487 (Activation)     (None, 14, 14, 448)  0           batch_normalization_488[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_485 (Conv2D)             (None, 14, 14, 384)  491520      mixed8[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_489 (Conv2D)             (None, 14, 14, 384)  1548288     activation_487[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_485 (BatchN (None, 14, 14, 384)  1152        conv2d_485[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_489 (BatchN (None, 14, 14, 384)  1152        conv2d_489[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_484 (Activation)     (None, 14, 14, 384)  0           batch_normalization_485[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_488 (Activation)     (None, 14, 14, 384)  0           batch_normalization_489[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_486 (Conv2D)             (None, 14, 14, 384)  442368      activation_484[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_487 (Conv2D)             (None, 14, 14, 384)  442368      activation_484[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_490 (Conv2D)             (None, 14, 14, 384)  442368      activation_488[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_491 (Conv2D)             (None, 14, 14, 384)  442368      activation_488[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_10 (AveragePo (None, 14, 14, 1280) 0           mixed8[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_484 (Conv2D)             (None, 14, 14, 320)  409600      mixed8[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_486 (BatchN (None, 14, 14, 384)  1152        conv2d_486[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_487 (BatchN (None, 14, 14, 384)  1152        conv2d_487[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_490 (BatchN (None, 14, 14, 384)  1152        conv2d_490[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_491 (BatchN (None, 14, 14, 384)  1152        conv2d_491[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_492 (Conv2D)             (None, 14, 14, 192)  245760      average_pooling2d_10[0][0]       \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_484 (BatchN (None, 14, 14, 320)  960         conv2d_484[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_485 (Activation)     (None, 14, 14, 384)  0           batch_normalization_486[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_486 (Activation)     (None, 14, 14, 384)  0           batch_normalization_487[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_489 (Activation)     (None, 14, 14, 384)  0           batch_normalization_490[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_490 (Activation)     (None, 14, 14, 384)  0           batch_normalization_491[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_492 (BatchN (None, 14, 14, 192)  576         conv2d_492[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_483 (Activation)     (None, 14, 14, 320)  0           batch_normalization_484[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed9_0 (Concatenate)          (None, 14, 14, 768)  0           activation_485[0][0]             \n",
    "#                                                                  activation_486[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# concatenate_1 (Concatenate)     (None, 14, 14, 768)  0           activation_489[0][0]             \n",
    "#                                                                  activation_490[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_491 (Activation)     (None, 14, 14, 192)  0           batch_normalization_492[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed9 (Concatenate)            (None, 14, 14, 2048) 0           activation_483[0][0]             \n",
    "#                                                                  mixed9_0[0][0]                   \n",
    "#                                                                  concatenate_1[0][0]              \n",
    "#                                                                  activation_491[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_497 (Conv2D)             (None, 14, 14, 448)  917504      mixed9[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_497 (BatchN (None, 14, 14, 448)  1344        conv2d_497[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_496 (Activation)     (None, 14, 14, 448)  0           batch_normalization_497[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_494 (Conv2D)             (None, 14, 14, 384)  786432      mixed9[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_498 (Conv2D)             (None, 14, 14, 384)  1548288     activation_496[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_494 (BatchN (None, 14, 14, 384)  1152        conv2d_494[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_498 (BatchN (None, 14, 14, 384)  1152        conv2d_498[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_493 (Activation)     (None, 14, 14, 384)  0           batch_normalization_494[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_497 (Activation)     (None, 14, 14, 384)  0           batch_normalization_498[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_495 (Conv2D)             (None, 14, 14, 384)  442368      activation_493[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_496 (Conv2D)             (None, 14, 14, 384)  442368      activation_493[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_499 (Conv2D)             (None, 14, 14, 384)  442368      activation_497[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_500 (Conv2D)             (None, 14, 14, 384)  442368      activation_497[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_11 (AveragePo (None, 14, 14, 2048) 0           mixed9[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_493 (Conv2D)             (None, 14, 14, 320)  655360      mixed9[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_495 (BatchN (None, 14, 14, 384)  1152        conv2d_495[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_496 (BatchN (None, 14, 14, 384)  1152        conv2d_496[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_499 (BatchN (None, 14, 14, 384)  1152        conv2d_499[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_500 (BatchN (None, 14, 14, 384)  1152        conv2d_500[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_501 (Conv2D)             (None, 14, 14, 192)  393216      average_pooling2d_11[0][0]       \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_493 (BatchN (None, 14, 14, 320)  960         conv2d_493[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_494 (Activation)     (None, 14, 14, 384)  0           batch_normalization_495[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_495 (Activation)     (None, 14, 14, 384)  0           batch_normalization_496[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_498 (Activation)     (None, 14, 14, 384)  0           batch_normalization_499[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_499 (Activation)     (None, 14, 14, 384)  0           batch_normalization_500[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_501 (BatchN (None, 14, 14, 192)  576         conv2d_501[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_492 (Activation)     (None, 14, 14, 320)  0           batch_normalization_493[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed9_1 (Concatenate)          (None, 14, 14, 768)  0           activation_494[0][0]             \n",
    "#                                                                  activation_495[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# concatenate_2 (Concatenate)     (None, 14, 14, 768)  0           activation_498[0][0]             \n",
    "#                                                                  activation_499[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_500 (Activation)     (None, 14, 14, 192)  0           batch_normalization_501[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed10 (Concatenate)           (None, 14, 14, 2048) 0           activation_492[0][0]             \n",
    "#                                                                  mixed9_1[0][0]                   \n",
    "#                                                                  concatenate_2[0][0]              \n",
    "#                                                                  activation_500[0][0]             \n",
    "# ==================================================================================================\n",
    "# Total params: 21,802,784\n",
    "# Trainable params: 21,768,352\n",
    "# Non-trainable params: 34,432\n",
    "# __________________________________________________________________________________________________\n",
    "\n",
    "# # model = InceptionResNetV2(include_top=False,\n",
    "# #                    weights='imagenet',\n",
    "# #                    input_shape=(512,512,3))\n",
    "# # #     bn = BatchNormalization()(input_tensor)\n",
    "# # model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(x,blocks=5,start_filters=256):\n",
    "    for i in range(blocks):\n",
    "        x = Conv2D(start_filters//(2**i),(3,3), activation='relu', padding='same')(x)\n",
    "        x = Conv2D(start_filters//(2**i),(3,3), activation='relu', padding='same')(x)\n",
    "        x = UpSampling2D()(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=input_shape)\n",
    "    bn = BatchNormalization(name='bn1')(input_tensor)\n",
    "    x = base_model(bn)\n",
    "    x1 = x\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "#     print(x.shape)\n",
    "#     x = Conv2D(32, kernel_size=(1,1), activation='relu', name='conv1')(x)\n",
    "#     y = Dense(14 * 14 * 2048)(x)\n",
    "#     x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "    y = Reshape((16, 16, 1568))(x1)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     y = Reshape(16,16,1568)(y)\n",
    "    y = decoder_block(y)\n",
    "    img_out = Dense(3, activation='relu',name='img_out')(y)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='sigmoid',name='output')(x)\n",
    "    model = Model(input_tensor, [output, img_out])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401408"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14*14*2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "401408/(16*16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bn1 (BatchNormalization)        (None, 512, 512, 3)  12          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "inception_v3 (Model)            (None, 14, 14, 2048) 21802784    bn1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 16, 16, 1568) 0           inception_v3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 16, 16, 256)  3612928     reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 16, 16, 256)  590080      conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 256)  0           conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 32, 32, 128)  295040      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 32, 32, 128)  147584      conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 64, 64, 128)  0           conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 64, 64, 64)   73792       up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 64, 64, 64)   36928       conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 128, 128, 64) 0           conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 128, 128, 32) 18464       up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 128, 128, 32) 9248        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 256, 256, 32) 0           conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           inception_v3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 256, 256, 16) 4624        up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 256, 256, 16) 2320        conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 512, 512, 16) 0           conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 28)           28700       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "img_out (Dense)                 (None, 512, 512, 3)  51          up_sampling2d_5[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 28,720,731\n",
      "Trainable params: 28,686,293\n",
      "Non-trainable params: 34,438\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# warm up model\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3), \n",
    "    n_out=28)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "# https://github.com/ailias/Focal-Loss-implement-on-Tensorflow/blob/master/focal_loss.py\n",
    "def focal_loss_org(prediction_tensor, target_tensor, weights=None, alpha=0.25, gamma=2):\n",
    "    r\"\"\"Compute focal loss for predictions.\n",
    "        Multi-labels Focal loss formula:\n",
    "            FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                 ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "    Args:\n",
    "     prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing the predicted logits for each class\n",
    "     target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing one-hot encoded classification targets\n",
    "     weights: A float tensor of shape [batch_size, num_anchors]\n",
    "     alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "     gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "    Returns:\n",
    "        loss: A (scalar) tensor representing the value of the loss function\n",
    "    \"\"\"\n",
    "    sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                          - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(weights=None, alpha=0.25, gamma=2):\n",
    "    def focal_loss_my(target_tensor, prediction_tensor, ):\n",
    "        r\"\"\"Compute focal loss for predictions.\n",
    "            Multi-labels Focal loss formula:\n",
    "                FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                     ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "        Args:\n",
    "         prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "            num_classes] representing the predicted logits for each class\n",
    "         target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "            num_classes] representing one-hot encoded classification targets\n",
    "         weights: A float tensor of shape [batch_size, num_anchors]\n",
    "         alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "         gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "        Returns:\n",
    "            loss: A (scalar) tensor representing the value of the loss function\n",
    "        \"\"\"\n",
    "        sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "        zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "\n",
    "        # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "        # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "        pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "\n",
    "        # For negative prediction, only need consider back part loss, front part is 0;\n",
    "        # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "        neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "        per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                              - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "        return tf.reduce_sum(per_entry_cross_ent)\n",
    "#         return K.mean(K.binary_crossentropy(target_tensor, prediction_tensor), axis=-1) + tf.reduce_sum(per_entry_cross_ent)\n",
    "    return focal_loss_my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_fixed(y_true, y_pred):\n",
    "    gamma = 2.\n",
    "    alpha = 0.25\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "#     pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "#     pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def focal_loss(gamma=2., alpha=.25):\n",
    "#     def focal_loss_fixed(y_true, y_pred):\n",
    "#         pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "#         pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "#         pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "#         pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "#         return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "#     return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epochs = 10; batch_size = 16\n",
    "checkpoint = ModelCheckpoint('../cache/IV3-36-maximus.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
    "                                   verbose=1, mode='auto', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=6)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# split data into train, valid\n",
    "indexes = np.arange(train_dataset_info.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "train_indexes, valid_indexes = train_test_split(indexes, test_size=0.15, random_state=8)\n",
    "\n",
    "# create train and valid datagens\n",
    "# train_generator = data_generator.create_train(\n",
    "#     train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "# validation_generator = data_generator.create_train(\n",
    "#     train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bn1 (BatchNormalization)        (None, 512, 512, 3)  12          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "inception_v3 (Model)            (None, 14, 14, 2048) 21802784    bn1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 16, 16, 1568) 0           inception_v3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 16, 16, 256)  3612928     reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 16, 16, 256)  590080      conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 256)  0           conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 32, 32, 128)  295040      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 32, 32, 128)  147584      conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 64, 64, 128)  0           conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 64, 64, 64)   73792       up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 64, 64, 64)   36928       conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 128, 128, 64) 0           conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 128, 128, 32) 18464       up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 128, 128, 32) 9248        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 256, 256, 32) 0           conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           inception_v3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 256, 256, 16) 4624        up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 256, 256, 16) 2320        conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 512, 512, 16) 0           conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 28)           28700       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "img_out (Dense)                 (None, 512, 512, 3)  51          up_sampling2d_5[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 28,720,731\n",
      "Trainable params: 28,686,293\n",
      "Non-trainable params: 34,438\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# warm up model\n",
    "# model = create_model(\n",
    "#     input_shape=(SIZE,SIZE,3), \n",
    "#     n_out=28)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print(epoch)\n",
    "        print(self)\n",
    "        print(logs)\n",
    "        print(*self)\n",
    "        for x,y in self.val_data:\n",
    "            print(x,y)\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        _val_recall = recall_score(val_targ, val_predict)\n",
    "        _val_precision = precision_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(' — val_f1: %f — val_precision: %f — val_recall %f' %(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    " \n",
    "my_metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "from operator import truediv\n",
    "\n",
    "\n",
    "class Recall(Layer):\n",
    "    '''Compute recall over all batches.\n",
    "    # Arguments\n",
    "        name: String, name for the metric.\n",
    "        class_ind: Integer, class index.\n",
    "    '''\n",
    "    def __init__(self, name='recall', class_ind=1):\n",
    "        super(Recall, self).__init__(name=name)\n",
    "        self.true_positives = K.variable(value=0, dtype='float32')\n",
    "        self.total_positives = K.variable(value=0, dtype='float32')\n",
    "        self.class_ind = class_ind\n",
    "\n",
    "    def reset_states(self):\n",
    "        K.set_value(self.true_positives, 0.0)\n",
    "        K.set_value(self.total_positives, 0.0)\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        '''Update recall computation.\n",
    "        # Arguments\n",
    "            y_true: Tensor, batch_wise labels\n",
    "            y_pred: Tensor, batch_wise predictions\n",
    "        # Returns\n",
    "            Overall recall for the epoch at the completion of the batch.\n",
    "        '''\n",
    "        # Batch\n",
    "        y_true, y_pred = _slice_by_class(y_true, y_pred, self.class_ind)\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        total_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        # Current\n",
    "        current_true_positives = self.true_positives * 1\n",
    "        current_total_positives = self.total_positives * 1\n",
    "        # Updates\n",
    "        updates = [K.update_add(self.true_positives, true_positives),\n",
    "                   K.update_add(self.total_positives, total_positives)]\n",
    "        self.add_update(updates, inputs=[y_true, y_pred])\n",
    "        # Compute recall\n",
    "        return (current_true_positives + true_positives) / \\\n",
    "               (current_total_positives + total_positives + K.epsilon())\n",
    "\n",
    "\n",
    "class Precision(Layer):\n",
    "    '''Compute precision over all batches.\n",
    "    # Arguments\n",
    "        name: String, name for the metric.\n",
    "        class_ind: Integer, class index.\n",
    "    '''\n",
    "    def __init__(self, name='precision', class_ind=1):\n",
    "        super(Precision, self).__init__(name=name)\n",
    "        self.true_positives = K.variable(value=0, dtype='float32')\n",
    "        self.pred_positives = K.variable(value=0, dtype='float32')\n",
    "        self.class_ind = class_ind\n",
    "\n",
    "    def reset_states(self):\n",
    "        K.set_value(self.true_positives, 0.0)\n",
    "        K.set_value(self.pred_positives, 0.0)\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        '''Update precision computation.\n",
    "        # Arguments\n",
    "            y_true: Tensor, batch_wise labels\n",
    "            y_pred: Tensor, batch_wise predictions\n",
    "        # Returns\n",
    "            Overall precision for the epoch at the completion of the batch.\n",
    "        '''\n",
    "        # Batch\n",
    "        y_true, y_pred = _slice_by_class(y_true, y_pred, self.class_ind)\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        pred_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        # Current\n",
    "        current_true_positives = self.true_positives * 1\n",
    "        current_pred_positives = self.pred_positives * 1\n",
    "        # Updates\n",
    "        updates = [K.update_add(self.true_positives, true_positives),\n",
    "                   K.update_add(self.pred_positives, pred_positives)]\n",
    "        self.add_update(updates, inputs=[y_true, y_pred])\n",
    "        # Compute recall\n",
    "        return (current_true_positives + true_positives) / \\\n",
    "               (current_pred_positives + pred_positives + K.epsilon())\n",
    "\n",
    "\n",
    "class F1(Layer):\n",
    "    \"\"\"Create a metric for the model's F1 score calculation.\n",
    "    The F1 score is the harmonic mean of precision and recall.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name='f1', class_ind=1):\n",
    "        super().__init__(name=name)\n",
    "        self.recall = Recall(class_ind=class_ind)\n",
    "        self.precision = Precision(class_ind=class_ind)\n",
    "        self.class_ind = class_ind\n",
    "\n",
    "    def reset_states(self):\n",
    "        \"\"\"Reset the state of the metrics.\"\"\"\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        pr = self.precision(y_true, y_pred)\n",
    "        rec = self.recall(y_true, y_pred)\n",
    "        return 2 * truediv(pr * rec, pr + rec + K.epsilon())\n",
    "\n",
    "\n",
    "def _slice_by_class(y_true, y_pred, class_ind):\n",
    "    ''' Slice the batch predictions and labels with respect to a given class\n",
    "    that is encoded by a categorical or binary label.\n",
    "    #  Arguments:\n",
    "        y_true: Tensor, batch_wise labels.\n",
    "        y_pred: Tensor, batch_wise predictions.\n",
    "        class_ind: Integer, class index.\n",
    "    # Returns:\n",
    "        y_slice_true: Tensor, batch_wise label slice.\n",
    "        y_slice_pred: Tensor,  batch_wise predictions, slice.\n",
    "    '''\n",
    "    # Binary encoded\n",
    "    if y_pred.shape[-1] == 1:\n",
    "        y_slice_true, y_slice_pred = y_true, y_pred\n",
    "    # Categorical encoded\n",
    "    else:\n",
    "        y_slice_true, y_slice_pred = y_true[..., class_ind], y_pred[..., class_ind]\n",
    "    return y_slice_true, y_slice_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.layers[0].trainable = False\n",
    "model.layers[1].trainable = False\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "# model.layers[-1].trainable = True\n",
    "# model.layers[-2].trainable = True\n",
    "# model.layers[-3].trainable = True\n",
    "# model.layers[-4].trainable = True\n",
    "# model.layers[-5].trainable = True\n",
    "# model.layers[-6].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4 1.9.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__, tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = np.zeros((28))\n",
    "# labels[0] = 1\n",
    "model.compile(\n",
    "    loss=[f1_loss, 'mse'],\n",
    "    optimizer=Adam(1e-03),\n",
    "    loss_weights = {\"output\": 1.0, \"img_out\": 10.0 }, \n",
    "    metrics=[f1])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "6603/6603 [==============================] - 687s 104ms/step - loss: 1.3134 - output_loss: 1.1290 - img_out_loss: 0.0184 - output_f1: 0.0265 - img_out_f1: 2.3766e-07 - val_loss: 1.3919 - val_output_loss: 1.1992 - val_img_out_loss: 0.0193 - val_output_f1: 0.0271 - val_img_out_f1: 1.7336e-06\n",
      "0\n",
      "<__main__.Metrics object at 0x7f16106e20f0>\n",
      "{'val_loss': 1.3919043994159612, 'val_output_loss': 1.1992344518788152, 'val_img_out_loss': 0.01926699477483472, 'val_output_f1': 0.02713929113249197, 'val_img_out_f1': 1.7336257668902906e-06, 'loss': 1.3133662115290374, 'output_loss': 1.1290309234992653, 'img_out_loss': 0.01843352879107959, 'output_f1': 0.026529675951386018, 'img_out_f1': 2.376677761066773e-07}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "print() argument after * must be an iterable, not Metrics",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-fcc9f3d51b4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     verbose=1, callbacks=[my_metrics])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hpg/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-3f0c79986839>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: print() argument after * must be an iterable, not Metrics"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "# from keras.callbacks import LambdaCallback\n",
    "\n",
    "# callbacks = callbacks=[LambdaCallback(on_batch_end=lambda batch,logs:print(logs))]\n",
    "\n",
    "# create train and valid datagens\n",
    "train_generator = data_generator.create_train(\n",
    "    train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "validation_generator = data_generator.create_train(\n",
    "    train_dataset_info[valid_indexes], 8, (SIZE,SIZE,3), augument=False)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=2, \n",
    "    verbose=1, callbacks=[my_metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "6603/6603 [==============================] - 2031s 308ms/step - loss: 1.2900 - output_loss: 1.0923 - img_out_loss: 0.0198 - output_f1: 0.0539 - img_out_f1: 0.0000e+00 - val_loss: 1.1767 - val_output_loss: 0.9782 - val_img_out_loss: 0.0199 - val_output_f1: 0.1835 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.17673, saving model to ../cache/IV3-36-maximus.h5\n",
      "Epoch 2/120\n",
      "6603/6603 [==============================] - 2001s 303ms/step - loss: 1.2444 - output_loss: 1.0468 - img_out_loss: 0.0198 - output_f1: 0.0831 - img_out_f1: 0.0000e+00 - val_loss: 1.1171 - val_output_loss: 0.9187 - val_img_out_loss: 0.0198 - val_output_f1: 0.2465 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.17673 to 1.11713, saving model to ../cache/IV3-36-maximus.h5\n",
      "Epoch 3/120\n",
      "6603/6603 [==============================] - 2000s 303ms/step - loss: 1.2236 - output_loss: 1.0259 - img_out_loss: 0.0198 - output_f1: 0.0957 - img_out_f1: 0.0000e+00 - val_loss: 1.0823 - val_output_loss: 0.8838 - val_img_out_loss: 0.0198 - val_output_f1: 0.2738 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.11713 to 1.08228, saving model to ../cache/IV3-36-maximus.h5\n",
      "Epoch 4/120\n",
      "6603/6603 [==============================] - 2008s 304ms/step - loss: 1.2121 - output_loss: 1.0145 - img_out_loss: 0.0198 - output_f1: 0.1021 - img_out_f1: 0.0000e+00 - val_loss: 1.0720 - val_output_loss: 0.8734 - val_img_out_loss: 0.0199 - val_output_f1: 0.2887 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.08228 to 1.07196, saving model to ../cache/IV3-36-maximus.h5\n",
      "Epoch 5/120\n",
      "6603/6603 [==============================] - 1997s 302ms/step - loss: 1.2060 - output_loss: 1.0082 - img_out_loss: 0.0198 - output_f1: 0.1055 - img_out_f1: 0.0000e+00 - val_loss: 1.0456 - val_output_loss: 0.8470 - val_img_out_loss: 0.0199 - val_output_f1: 0.3045 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.07196 to 1.04560, saving model to ../cache/IV3-36-maximus.h5\n",
      "Epoch 6/120\n",
      "6603/6603 [==============================] - 2006s 304ms/step - loss: 1.1974 - output_loss: 0.9997 - img_out_loss: 0.0198 - output_f1: 0.1103 - img_out_f1: 0.0000e+00 - val_loss: 1.0266 - val_output_loss: 0.8283 - val_img_out_loss: 0.0198 - val_output_f1: 0.3174 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.04560 to 1.02663, saving model to ../cache/IV3-36-maximus.h5\n",
      "Epoch 7/120\n",
      "6603/6603 [==============================] - 2016s 305ms/step - loss: 1.1937 - output_loss: 0.9961 - img_out_loss: 0.0198 - output_f1: 0.1120 - img_out_f1: 0.0000e+00 - val_loss: 1.0378 - val_output_loss: 0.8392 - val_img_out_loss: 0.0199 - val_output_f1: 0.3050 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.02663\n",
      "Epoch 8/120\n",
      "6603/6603 [==============================] - 2024s 306ms/step - loss: 1.1880 - output_loss: 0.9904 - img_out_loss: 0.0198 - output_f1: 0.1147 - img_out_f1: 0.0000e+00 - val_loss: 1.0390 - val_output_loss: 0.8404 - val_img_out_loss: 0.0199 - val_output_f1: 0.3197 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.02663\n",
      "Epoch 9/120\n",
      "6603/6603 [==============================] - 2021s 306ms/step - loss: 1.1843 - output_loss: 0.9866 - img_out_loss: 0.0198 - output_f1: 0.1170 - img_out_f1: 0.0000e+00 - val_loss: 0.9971 - val_output_loss: 0.7988 - val_img_out_loss: 0.0198 - val_output_f1: 0.3322 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.02663 to 0.99706, saving model to ../cache/IV3-36-maximus.h5\n",
      "Epoch 10/120\n",
      "6603/6603 [==============================] - 2020s 306ms/step - loss: 1.1805 - output_loss: 0.9827 - img_out_loss: 0.0198 - output_f1: 0.1185 - img_out_f1: 0.0000e+00 - val_loss: 0.9970 - val_output_loss: 0.7983 - val_img_out_loss: 0.0199 - val_output_f1: 0.3333 - val_img_out_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.99706 to 0.99704, saving model to ../cache/IV3-36-maximus.h5\n",
      "Epoch 11/120\n",
      "1978/6603 [=======>......................] - ETA: 16:22 - loss: 1.1770 - output_loss: 0.9779 - img_out_loss: 0.0199 - output_f1: 0.1206 - img_out_f1: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "# train all layers\n",
    "epochs=120\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.compile(loss=[f1_loss,'mse'],\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            loss_weights = {\"output\": 1.0, \"img_out\": 10.0 }, \n",
    "            metrics=[f1])\n",
    "# model.load_weights('../cache/IV3-36-maximus.h5')\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=epochs, \n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [15:00<00:00, 12.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create submit\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "predicted = []\n",
    "draw_predict = []\n",
    "# model = create_model(\n",
    "#     input_shape=(SIZE,SIZE,3), \n",
    "#     n_out=28)\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = True\n",
    "# model.compile(loss=f1_loss,\n",
    "#             optimizer=Adam(lr=1e-4),\n",
    "#             metrics=[f1])\n",
    "model.load_weights('../cache/IV3-36-maximus.h5')\n",
    "for name in tqdm(submit['Id']):\n",
    "    path = os.path.join('../data/test/', name)\n",
    "    image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "    score_predict = model.predict(image[np.newaxis])[0]\n",
    "    draw_predict.append(score_predict)\n",
    "    label_predict = np.arange(28)[score_predict[0]>=0.2]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted\n",
    "# np.save('../cache/draw_predict_InceptionV3-30.npy', score_predict)\n",
    "# submit.to_csv('../submissions/submit_InceptionV3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('../submissions/sub36-max.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/1855095/how-to-create-a-zip-archive-of-a-directory\n",
    "def backup_project_as_zip(project_dir, zip_file):\n",
    "    assert(os.path.isdir(project_dir))\n",
    "    assert(os.path.isdir(os.path.dirname(zip_file)))\n",
    "    shutil.make_archive(zip_file.replace('.zip',''), 'zip', project_dir)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-26 06:28:07.734445\n"
     ]
    }
   ],
   "source": [
    "import datetime, shutil\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "PROJECT_PATH = '/home/watts/lal/Kaggle/kagglehp/scripts_nbs'\n",
    "backup_project_as_zip(PROJECT_PATH, '../cache/code.scripts_nbs.%s.zip'%now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 481k/481k [00:14<00:00, 34.5kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 366 ms, sys: 172 ms, total: 538 ms\n",
      "Wall time: 17.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub35-max.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName        date                 description  status    publicScore  privateScore  \r\n",
      "--------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub35-max.csv   2018-11-26 00:58:38               complete  0.473        None          \r\n",
      "sub34b-max.csv  2018-11-24 19:03:59               complete  0.459        None          \r\n",
      "sub34a-max.csv  2018-11-24 18:50:22               complete  0.469        None          \r\n",
      "sub34-max.csv   2018-11-24 17:27:36               complete  0.473        None          \r\n",
      "sub33-h.csv     2018-11-24 06:48:41               complete  0.464        None          \r\n",
      "sub33-g.csv     2018-11-24 06:46:19               complete  0.472        None          \r\n",
      "sub33-c.csv     2018-11-23 11:48:41               complete  0.493        None          \r\n",
      "sub33-bb.csv    2018-11-23 11:47:32               complete  0.493        None          \r\n",
      "sub33-b.csv     2018-11-23 11:46:26               complete  0.498        None          \r\n",
      "sub33-a.csv     2018-11-23 11:45:09               complete  0.496        None          \r\n",
      "sub36-a.csv     2018-11-23 10:12:45               complete  0.287        None          \r\n",
      "sub35b-c.csv    2018-11-22 08:00:23               complete  0.417        None          \r\n",
      "sub35b-b.csv    2018-11-22 07:59:44               complete  0.415        None          \r\n",
      "sub35b-a.csv    2018-11-22 07:58:57               complete  0.432        None          \r\n",
      "sub35-a.csv     2018-11-20 06:54:01               complete  0.315        None          \r\n",
      "sub8i1-e.csv    2018-11-19 07:12:37               complete  0.457        None          \r\n",
      "sub8i1-d.csv    2018-11-19 07:09:46               complete  0.459        None          \r\n",
      "sub8i1-c.csv    2018-11-19 07:08:49               complete  0.462        None          \r\n",
      "sub8i1-b.csv    2018-11-19 07:08:05               complete  0.462        None          \r\n",
      "sub8i1-a.csv    2018-11-19 07:07:14               complete  0.460        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(60)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.3.8)\r\n",
      "fileName  date                 description  status    publicScore  privateScore  \r\n",
      "--------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub8.csv  2018-10-20 20:08:45               complete  0.422        None          \r\n",
      "sub7.csv  2018-10-20 17:06:09               complete  0.389        None          \r\n",
      "sub5.csv  2018-10-19 18:27:33               complete  0.387        None          \r\n",
      "sub4.csv  2018-10-19 14:45:15               complete  0.411        None          \r\n",
      "sub3.csv  2018-10-19 10:19:26               complete  0.377        None          \r\n",
      "sub2.csv  2018-10-19 08:07:30               complete  0.135        None          \r\n",
      "sub1.csv  2018-10-19 06:28:57               complete  0.374        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(60)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpg",
   "language": "python",
   "name": "hpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

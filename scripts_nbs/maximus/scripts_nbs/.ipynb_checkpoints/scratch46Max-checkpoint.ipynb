{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/mathormad/inceptionv3-baseline-lb-0-379/code\n",
    "# fork of scratch8, 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "import keras_metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) + (1 - K.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "path_to_train = '../data/train/'\n",
    "data = pd.read_csv('../data/train.csv')\n",
    "# path_to_external_data = '../data/external_data/external_data_1/'\n",
    "# edata = pd.read_csv('../data/external_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_info = []\n",
    "\n",
    "for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "    train_dataset_info.append({\n",
    "        'path':os.path.join(path_to_train, name),\n",
    "        'labels':np.array([int(label) for label in labels])})\n",
    "    \n",
    "# for name, labels in zip(edata['id'], edata['labels'].str.strip('[]')):\n",
    "#     labels = labels.split(',')\n",
    "#     train_dataset_info.append({\n",
    "#         'path':os.path.join(path_to_external_data, name),\n",
    "#         'labels':np.array([int(label) for label in labels])})\n",
    "    \n",
    "train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'path': '../data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0', 'labels': array([16,  0])},\n",
       "       {'path': '../data/train/000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', 'labels': array([7, 1, 2, 0])},\n",
       "       {'path': '../data/train/000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([5])},\n",
       "       ...,\n",
       "       {'path': '../data/train/fff189d8-bbab-11e8-b2ba-ac1f6b6435d0', 'labels': array([7])},\n",
       "       {'path': '../data/train/fffdf7e0-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([25,  2, 21])},\n",
       "       {'path': '../data/train/fffe0ffe-bbc0-11e8-b2bb-ac1f6b6435d0', 'labels': array([2, 0])}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            dataset_info = shuffle(dataset_info)\n",
    "            for start in range(0, len(dataset_info), batch_size):\n",
    "                end = min(start + batch_size, len(dataset_info))\n",
    "                batch_images = []\n",
    "                X_train_batch = dataset_info[start:end]\n",
    "                batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "                for i in range(len(X_train_batch)):\n",
    "                    image = data_generator.load_image(\n",
    "                        X_train_batch[i]['path'], shape)\n",
    "#                     image = tdi[i+start]\n",
    "#                     image = cv2.resize(image, (shape[0], shape[1]))\n",
    "                    if augument:\n",
    "                        image = data_generator.augment(image)\n",
    "                    batch_images.append(image/255.)\n",
    "                    batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "                yield np.array(batch_images, np.float32), batch_labels\n",
    "\n",
    "    def load_image(path, shape):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image1 = np.stack((\n",
    "            np.array(image_red_ch),\n",
    "            np.array(image_green_ch), \n",
    "            np.array(image_blue_ch)), -1)\n",
    "        w, h = 512, 512\n",
    "#         zero_data = np.zeros((h, w), dtype=np.uint8)\n",
    "#         image2 = np.stack((\n",
    "#             np.array(image_red_ch),\n",
    "#             np.array(image_green_ch), \n",
    "#             np.array(image_yellow_ch)), -1)\n",
    "#         image3 = np.stack((\n",
    "#             np.array(image_yellow_ch),\n",
    "#             np.array(image_green_ch), \n",
    "#             np.array(image_blue_ch)), -1)\n",
    "# #         print(image1.shape, image2.shape)\n",
    "#         image = np.vstack((image1, image2, image3))\n",
    "#         print(image.shape)\n",
    "        image =image1\n",
    "#         image = canny_image4(image1)\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    \n",
    "    def load_image2(path, shape):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image1 = np.stack((\n",
    "            np.array(image_red_ch),\n",
    "            np.array(image_green_ch), \n",
    "            np.array(image_blue_ch)), -1)\n",
    "        w, h = 512, 512\n",
    "#         zero_data = np.zeros((h, w), dtype=np.uint8)\n",
    "#         image2 = np.stack((\n",
    "#             np.array(image_red_ch),\n",
    "#             np.array(image_green_ch), \n",
    "#             np.array(image_yellow_ch)), -1)\n",
    "#         image3 = np.stack((\n",
    "#             np.array(image_yellow_ch),\n",
    "#             np.array(image_green_ch), \n",
    "#             np.array(image_blue_ch)), -1)\n",
    "# #         print(image1.shape, image2.shape)\n",
    "#         image = np.vstack((image1, image2, image3))\n",
    "#         print(image.shape)\n",
    "        image =image1\n",
    "#         image = canny_image4(image1)\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    \n",
    "    def augment2(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=0),\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "            ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n",
    "    def augment(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                    iaa.Fliplr(0.5), # horizontal flips\n",
    "                    iaa.Affine(rotate=0),\n",
    "                    iaa.Affine(rotate=90),\n",
    "                    iaa.Affine(rotate=180),\n",
    "                    iaa.Affine(rotate=270),\n",
    "                    iaa.Flipud(0.5),\n",
    "                    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "                    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "                    # But we only blur about 50% of all images.\n",
    "                    iaa.Sometimes(0.5,\n",
    "                        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "                    ),\n",
    "                    # Strengthen or weaken the contrast in each image.\n",
    "                    iaa.ContrastNormalization((0.75, 1.5)),\n",
    "                    # Add gaussian noise.\n",
    "                    # For 50% of all images, we sample the noise once per pixel.\n",
    "                    # For the other 50% of all images, we sample the noise per pixel AND\n",
    "                    # channel. This can change the color (not only brightness) of the\n",
    "                    # pixels.\n",
    "                    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "                    # Make some images brighter and some darker.\n",
    "                    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "                    # which can end up changing the color of the images.\n",
    "                    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "                    # Apply affine transformations to each image.\n",
    "                    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "                    iaa.Affine(\n",
    "                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                        rotate=(-180, 180),\n",
    "                        shear=(-8, 8)\n",
    "                    )\n",
    "                ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Lambda\n",
    "from keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D, BatchNormalization, Input, Conv2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=input_shape)\n",
    "    bn = BatchNormalization(name='bn1')(input_tensor)\n",
    "    x = base_model(bn)\n",
    "    x = GlobalAveragePooling2D(name='cam')(x)\n",
    "#     x = Conv2D(32, kernel_size=(1,1), activation='relu', name='conv1')(x)\n",
    "#     x = Flatten()(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512, 512, 3)       0         \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 512, 512, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 14, 14, 2048)      21802784  \n",
      "_________________________________________________________________\n",
      "cam (GlobalAveragePooling2D) (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 23,929,672\n",
      "Trainable params: 23,895,234\n",
      "Non-trainable params: 34,438\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# warm up model\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3), \n",
    "    n_out=28)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=f1_loss,\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=[f1])\n",
    "model.load_weights('../cache/IV3-34-maximus.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_category_loss(x, category_index, nb_classes):\n",
    "    return tf.multiply(x, K.one_hot([category_index], nb_classes))\n",
    "\n",
    "def target_category_loss_output_shape(input_shape):\n",
    "    return input_shape\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "# def load_image(path):\n",
    "#     img_path = sys.argv[1]\n",
    "#     img = image.load_img(img_path, target_size=(224, 224))\n",
    "#     x = image.img_to_array(img)\n",
    "#     x = np.expand_dims(x, axis=0)\n",
    "#     x = preprocess_input(x)\n",
    "#     return x\n",
    "\n",
    "def load_image(path):\n",
    "    return data_generator.load_image(path, (512,512,3))\n",
    "    \n",
    "def register_gradient():\n",
    "    if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
    "        @ops.RegisterGradient(\"GuidedBackProp\")\n",
    "        def _GuidedBackProp(op, grad):\n",
    "            dtype = op.inputs[0].dtype\n",
    "            return grad * tf.cast(grad > 0., dtype) * \\\n",
    "                tf.cast(op.inputs[0] > 0., dtype)\n",
    "\n",
    "def compile_saliency_function(model, activation_layer='block5_conv3'):\n",
    "    input_img = model.input\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    layer_output = layer_dict[activation_layer].output\n",
    "    max_output = K.max(layer_output, axis=3)\n",
    "    saliency = K.gradients(K.sum(max_output), input_img)[0]\n",
    "    return K.function([input_img, K.learning_phase()], [saliency])\n",
    "\n",
    "def modify_backprop(model, name):\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({'Relu': name}):\n",
    "\n",
    "        # get layers that have an activation\n",
    "        layer_dict = [layer for layer in model.layers[1:]\n",
    "                      if hasattr(layer, 'activation')]\n",
    "\n",
    "        # replace relu activation\n",
    "        for layer in layer_dict:\n",
    "            if layer.activation == keras.activations.relu:\n",
    "                layer.activation = tf.nn.relu\n",
    "\n",
    "        # re-instanciate a new model\n",
    "        new_model = VGG16(weights='imagenet')\n",
    "    return new_model\n",
    "\n",
    "def deprocess_image(x):\n",
    "    '''\n",
    "    Same normalization as in:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    '''\n",
    "    if np.ndim(x) > 3:\n",
    "        x = np.squeeze(x)\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def grad_cam(input_model, image, cls, layer_name):\n",
    "    \"\"\"GradCAM method for visualizing input saliency.\"\"\"\n",
    "    y_c = input_model.output[0, cls]\n",
    "#     conv_output = input_model.get_layer(layer_name).output\n",
    "    conv_output = input_model.get_layer(layer_name).get_output_at(-1)\n",
    "    grads = K.gradients(y_c, conv_output)[0]\n",
    "    # Normalize if necessary\n",
    "    # grads = normalize(grads)\n",
    "    gradient_function = K.function([input_model.input], [conv_output, grads])\n",
    "\n",
    "    output, grads_val = gradient_function([image])\n",
    "    output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
    "\n",
    "    weights = np.mean(grads_val, axis=(0, 1))\n",
    "    cam = np.dot(output, weights)\n",
    "\n",
    "    # Process CAM\n",
    "    cam = cv2.resize(cam, (512, 512), cv2.INTER_LINEAR)\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / cam.max()\n",
    "    return cam\n",
    "\n",
    "def grad_cam2(input_model, image, category_index, layer_name):\n",
    "    model = Sequential()\n",
    "    model.add(input_model)\n",
    "\n",
    "    nb_classes = 28\n",
    "    target_layer = lambda x: target_category_loss(x, category_index, nb_classes)\n",
    "    model.add(Lambda(target_layer,\n",
    "                     output_shape = target_category_loss_output_shape))\n",
    "\n",
    "    loss = K.sum(model.layers[-1].output)\n",
    "#     conv_output =  [l for l in model.layers[0].layers if l.name is layer_name][0].output\n",
    "    conv_output = [l for l in model.layers[0].layers if l.name is layer_name][0].get_output_at(-1)\n",
    "    print(conv_output.shape)\n",
    "    grads = normalize(K.gradients(loss, conv_output)[0])\n",
    "    gradient_function = K.function([model.layers[0].input], [conv_output, grads])\n",
    "\n",
    "    output, grads_val = gradient_function([image])\n",
    "    output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
    "\n",
    "    weights = np.mean(grads_val, axis = (0, 1))\n",
    "    cam = np.ones(output.shape[0 : 2], dtype = np.float32)\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * output[:, :, i]\n",
    "\n",
    "    cam = cv2.resize(cam, (512, 512))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    heatmap = cam / np.max(cam)\n",
    "\n",
    "    #Return to BGR [0..255] from the preprocessed image\n",
    "    image = image[0, :]\n",
    "    image -= np.min(image)\n",
    "    image = np.minimum(image, 255)\n",
    "\n",
    "    cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
    "    cam = np.float32(cam) + np.float32(image)\n",
    "    cam = 255 * cam / np.max(cam)\n",
    "    return np.uint8(cam), heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(train_dataset_info[1]['path'])\n",
    "preprocessed_input = image[np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info[0]['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512, 512, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_predict = model.predict(preprocessed_input)[0]\n",
    "# draw_predict.append(score_predict)\n",
    "label_predict = np.arange(28)[score_predict>=0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = grad_cam(model, preprocessed_input, label_predict[0], \"inception_v3\")\n",
    "# cv2.imwrite(\"../cache/gradcam-my.png\", cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.imwrite(\"../cache/gradcam-my.jpg\", cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heat = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
    "# heat = np.float32(heat) + np.float32(image)\n",
    "# heat = 255 * cam / np.max(heat)\n",
    "# cv2.imwrite(\"../cache/gradcam-my.png\", cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jetcam = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "jetcam = (np.float32(jetcam) + image) / 2\n",
    "cv2.imwrite('../cache/gradcam2.png', np.uint8(jetcam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'cam'\n",
    "input_img_data = preprocessed_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bn1': <keras.layers.normalization.BatchNormalization at 0x7f70ff1bb438>,\n",
       " 'cam': <keras.layers.pooling.GlobalAveragePooling2D at 0x7f70fd4375c0>,\n",
       " 'dense_1': <keras.layers.core.Dense at 0x7f70fd3f5128>,\n",
       " 'dense_2': <keras.layers.core.Dense at 0x7f70fe404390>,\n",
       " 'dropout_1': <keras.layers.core.Dropout at 0x7f70fe404b70>,\n",
       " 'inception_v3': <keras.engine.training.Model at 0x7f70ff1cf5f8>}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize2(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d406417026a4847b21d9ab205c8a8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2048), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "kept_filters = []\n",
    "for filter_index in tqdm_notebook(range(200)):\n",
    "    # we only scan through the first 200 filters,\n",
    "    # but there are actually 512 of them\n",
    "#     print('Processing filter %d' % filter_index)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # we build a loss function that maximizes the activation\n",
    "    # of the nth filter of the layer considered\n",
    "    layer_output = layer_dict[layer_name].output\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "    else:\n",
    "        loss = K.mean(layer_output[:, filter_index])\n",
    "\n",
    "    # we compute the gradient of the input picture wrt this loss\n",
    "    grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "    # normalization trick: we normalize the gradient\n",
    "    grads = normalize2(grads)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "    # step size for gradient ascent\n",
    "    step = 1.\n",
    "\n",
    "    # we start from a gray image with some random noise\n",
    "#     if K.image_data_format() == 'channels_first':\n",
    "#         input_img_data = np.random.random((1, 3, img_width, img_height))\n",
    "#     else:\n",
    "#         input_img_data = np.random.random((1, img_width, img_height, 3))\n",
    "    \n",
    "    input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "    # we run gradient ascent for 20 steps\n",
    "    for i in range(20):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "\n",
    "#         print('Current loss value:', loss_value)\n",
    "        if loss_value <= 0.:\n",
    "            # some filters get stuck to 0, we can skip them\n",
    "            break\n",
    "\n",
    "    # decode the resulting input image\n",
    "    if loss_value > 0:\n",
    "        img = deprocess_image(input_img_data[0])\n",
    "        kept_filters.append((img, loss_value))\n",
    "    end_time = time.time()\n",
    "#     print('Filter %d processed in %ds' % (filter_index, end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epochs = 10; batch_size = 16\n",
    "checkpoint = ModelCheckpoint('../cache/IV3-34-maximus.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
    "                                   verbose=1, mode='auto', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=6)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split data into train, valid\n",
    "indexes = np.arange(train_dataset_info.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "train_indexes, valid_indexes = train_test_split(indexes, test_size=0.15, random_state=8)\n",
    "\n",
    "# create train and valid datagens\n",
    "train_generator = data_generator.create_train(\n",
    "    train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "validation_generator = data_generator.create_train(\n",
    "    train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512, 512, 3)       0         \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 512, 512, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 14, 14, 2048)      21802784  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 23,929,672\n",
      "Trainable params: 23,895,234\n",
      "Non-trainable params: 34,438\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# warm up model\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3), \n",
    "    n_out=28)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "model.layers[-1].trainable = True\n",
    "model.layers[-2].trainable = True\n",
    "model.layers[-3].trainable = True\n",
    "model.layers[-4].trainable = True\n",
    "# model.layers[-5].trainable = True\n",
    "# model.layers[-6].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = np.zeros((28))\n",
    "# labels[0] = 1\n",
    "model.compile(\n",
    "    loss=f1_loss, \n",
    "    optimizer=Adam(1e-03),\n",
    "    metrics=[f1])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1651/1651 [==============================] - 590s 357ms/step - loss: 1.1123 - f1: 0.0432 - val_loss: 1.1623 - val_f1: 0.0262\n",
      "Epoch 2/2\n",
      "1651/1651 [==============================] - 567s 344ms/step - loss: 1.0942 - f1: 0.0598 - val_loss: 1.2361 - val_f1: 0.0353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f373333fb70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=2, \n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "1651/1651 [==============================] - 975s 591ms/step - loss: 0.9865 - f1: 0.1671 - val_loss: 0.8538 - val_f1: 0.2857\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.85385, saving model to ../cache/IV3-34-maximus.h5\n",
      "Epoch 2/120\n",
      "1651/1651 [==============================] - 958s 580ms/step - loss: 0.9035 - f1: 0.2369 - val_loss: 0.8483 - val_f1: 0.2926\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.85385 to 0.84827, saving model to ../cache/IV3-34-maximus.h5\n",
      "Epoch 3/120\n",
      "1651/1651 [==============================] - 965s 584ms/step - loss: 0.8761 - f1: 0.2533 - val_loss: 0.7978 - val_f1: 0.3238\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.84827 to 0.79781, saving model to ../cache/IV3-34-maximus.h5\n",
      "Epoch 4/120\n",
      "1651/1651 [==============================] - 984s 596ms/step - loss: 0.8591 - f1: 0.2646 - val_loss: 0.8129 - val_f1: 0.3201\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.79781\n",
      "Epoch 5/120\n",
      "1651/1651 [==============================] - 965s 584ms/step - loss: 0.8455 - f1: 0.2750 - val_loss: 0.7992 - val_f1: 0.3272\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.79781\n",
      "Epoch 6/120\n",
      "1651/1651 [==============================] - 943s 571ms/step - loss: 0.8351 - f1: 0.2811 - val_loss: 0.7876 - val_f1: 0.3402\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.79781 to 0.78763, saving model to ../cache/IV3-34-maximus.h5\n",
      "Epoch 7/120\n",
      "1651/1651 [==============================] - 958s 580ms/step - loss: 0.8265 - f1: 0.2861 - val_loss: 0.7760 - val_f1: 0.3485\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.78763 to 0.77603, saving model to ../cache/IV3-34-maximus.h5\n",
      "Epoch 8/120\n",
      "1651/1651 [==============================] - 966s 585ms/step - loss: 0.8154 - f1: 0.2938 - val_loss: 0.7620 - val_f1: 0.3547\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.77603 to 0.76199, saving model to ../cache/IV3-34-maximus.h5\n",
      "Epoch 9/120\n",
      "1651/1651 [==============================] - 985s 596ms/step - loss: 0.8079 - f1: 0.2986 - val_loss: 0.7750 - val_f1: 0.3499\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.76199\n",
      "Epoch 10/120\n",
      "1651/1651 [==============================] - 988s 599ms/step - loss: 0.7998 - f1: 0.3037 - val_loss: 0.7481 - val_f1: 0.3683\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.76199 to 0.74812, saving model to ../cache/IV3-34-maximus.h5\n",
      "Epoch 11/120\n",
      "1651/1651 [==============================] - 959s 581ms/step - loss: 0.7949 - f1: 0.3071 - val_loss: 0.7451 - val_f1: 0.3629\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.74812 to 0.74514, saving model to ../cache/IV3-34-maximus.h5\n",
      "Epoch 12/120\n",
      "1651/1651 [==============================] - 991s 600ms/step - loss: 0.7889 - f1: 0.3110 - val_loss: 0.7535 - val_f1: 0.3664\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.74514\n",
      "Epoch 13/120\n",
      "1651/1651 [==============================] - 981s 594ms/step - loss: 0.7819 - f1: 0.3149 - val_loss: 0.7569 - val_f1: 0.3640\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.74514\n",
      "Epoch 14/120\n",
      "1651/1651 [==============================] - 968s 586ms/step - loss: 0.7772 - f1: 0.3182 - val_loss: 0.7710 - val_f1: 0.3534\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.74514\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 15/120\n",
      "1651/1651 [==============================] - 994s 602ms/step - loss: 0.7549 - f1: 0.3302 - val_loss: 0.7086 - val_f1: 0.3931\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.74514 to 0.70859, saving model to ../cache/IV3-34-maximus.h5\n",
      "Epoch 16/120\n",
      "1651/1651 [==============================] - 965s 584ms/step - loss: 0.7432 - f1: 0.3374 - val_loss: 0.7127 - val_f1: 0.3886\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.70859\n",
      "Epoch 17/120\n",
      "1651/1651 [==============================] - 987s 598ms/step - loss: 0.7394 - f1: 0.3388 - val_loss: 0.7137 - val_f1: 0.3878\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.70859\n",
      "Epoch 18/120\n",
      "1651/1651 [==============================] - 1003s 608ms/step - loss: 0.7338 - f1: 0.3423 - val_loss: 0.7135 - val_f1: 0.3885\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.70859\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 19/120\n",
      "1651/1651 [==============================] - 958s 581ms/step - loss: 0.7302 - f1: 0.3442 - val_loss: 0.7045 - val_f1: 0.3975\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.70859 to 0.70453, saving model to ../cache/IV3-34-maximus.h5\n",
      "Epoch 20/120\n",
      "1651/1651 [==============================] - 992s 601ms/step - loss: 0.7299 - f1: 0.3439 - val_loss: 0.7108 - val_f1: 0.3882\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.70453\n",
      "Epoch 21/120\n",
      "1651/1651 [==============================] - 975s 591ms/step - loss: 0.7277 - f1: 0.3458 - val_loss: 0.7079 - val_f1: 0.3939\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.70453\n",
      "Epoch 22/120\n",
      "1651/1651 [==============================] - 970s 588ms/step - loss: 0.7279 - f1: 0.3453 - val_loss: 0.7105 - val_f1: 0.3908\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.70453\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 23/120\n",
      "1651/1651 [==============================] - 980s 594ms/step - loss: 0.7277 - f1: 0.3453 - val_loss: 0.7088 - val_f1: 0.3917\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.70453\n",
      "Epoch 24/120\n",
      "1651/1651 [==============================] - 949s 575ms/step - loss: 0.7269 - f1: 0.3459 - val_loss: 0.7096 - val_f1: 0.3908\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.70453\n",
      "Epoch 25/120\n",
      "1651/1651 [==============================] - 990s 599ms/step - loss: 0.7262 - f1: 0.3478 - val_loss: 0.7063 - val_f1: 0.3938\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.70453\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f373333f940>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train all layers\n",
    "epochs=120\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.compile(loss=f1_loss,\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=[f1])\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=epochs, \n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [08:29<00:00, 22.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create submit\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "predicted = []\n",
    "draw_predict = []\n",
    "# model = create_model(\n",
    "#     input_shape=(SIZE,SIZE,3), \n",
    "#     n_out=28)\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = True\n",
    "# model.compile(loss=f1_loss,\n",
    "#             optimizer=Adam(lr=1e-4),\n",
    "#             metrics=[f1])\n",
    "model.load_weights('../cache/IV3-34-maximus.h5')\n",
    "for name in tqdm(submit['Id']):\n",
    "    path = os.path.join('../data/test/', name)\n",
    "    image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "    score_predict = model.predict(image[np.newaxis])[0]\n",
    "    draw_predict.append(score_predict)\n",
    "    label_predict = np.arange(28)[score_predict>=0.2]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted\n",
    "# np.save('../cache/draw_predict_InceptionV3-30.npy', score_predict)\n",
    "# submit.to_csv('../submissions/submit_InceptionV3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('../submissions/sub34-max.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/1855095/how-to-create-a-zip-archive-of-a-directory\n",
    "def backup_project_as_zip(project_dir, zip_file):\n",
    "    assert(os.path.isdir(project_dir))\n",
    "    assert(os.path.isdir(os.path.dirname(zip_file)))\n",
    "    shutil.make_archive(zip_file.replace('.zip',''), 'zip', project_dir)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-24 22:57:07.641600\n"
     ]
    }
   ],
   "source": [
    "import datetime, shutil\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "PROJECT_PATH = '/home/watts/lal/Kaggle/kagglehp/scripts_nbs'\n",
    "backup_project_as_zip(PROJECT_PATH, '../cache/code.scripts_nbs.%s.zip'%now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 479k/479k [00:14<00:00, 34.4kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 371 ms, sys: 196 ms, total: 567 ms\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub34-max.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [08:21<00:00, 23.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create submit\n",
    "\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "predicted = []\n",
    "draw_predict = []\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3), \n",
    "    n_out=28)\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.compile(loss=f1_loss,\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=[f1])\n",
    "model.load_weights('../cache/IV3-34-maximus.h5')\n",
    "for name in tqdm(submit['Id']):\n",
    "    path = os.path.join('../data/test/', name)\n",
    "    image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "    score_predict = model.predict(image[np.newaxis])[0]\n",
    "    draw_predict.append(score_predict)\n",
    "    label_predict = np.arange(28)[score_predict>=0.35]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('../submissions/sub34a-max.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 472k/472k [00:12<00:00, 37.5kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationfileName        date                 description  status    publicScore  privateScore  \n",
      "--------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub34a-max.csv  2018-11-24 18:50:22               complete  0.469        None          \n",
      "sub34-max.csv   2018-11-24 17:27:36               complete  0.473        None          \n",
      "sub33-h.csv     2018-11-24 06:48:41               complete  0.464        None          \n",
      "sub33-g.csv     2018-11-24 06:46:19               complete  0.472        None          \n",
      "sub33-c.csv     2018-11-23 11:48:41               complete  0.493        None          \n",
      "sub33-bb.csv    2018-11-23 11:47:32               complete  0.493        None          \n",
      "sub33-b.csv     2018-11-23 11:46:26               complete  0.498        None          \n",
      "sub33-a.csv     2018-11-23 11:45:09               complete  0.496        None          \n",
      "sub36-a.csv     2018-11-23 10:12:45               complete  0.287        None          \n",
      "sub35b-c.csv    2018-11-22 08:00:23               complete  0.417        None          \n",
      "sub35b-b.csv    2018-11-22 07:59:44               complete  0.415        None          \n",
      "sub35b-a.csv    2018-11-22 07:58:57               complete  0.432        None          \n",
      "sub35-a.csv     2018-11-20 06:54:01               complete  0.315        None          \n",
      "sub8i1-e.csv    2018-11-19 07:12:37               complete  0.457        None          \n",
      "sub8i1-d.csv    2018-11-19 07:09:46               complete  0.459        None          \n",
      "sub8i1-c.csv    2018-11-19 07:08:49               complete  0.462        None          \n",
      "sub8i1-b.csv    2018-11-19 07:08:05               complete  0.462        None          \n",
      "sub8i1-a.csv    2018-11-19 07:07:14               complete  0.460        None          \n",
      "sub32-c.csv     2018-11-15 11:47:19               complete  0.463        None          \n",
      "sub32-bb.csv    2018-11-15 11:46:01               complete  0.465        None          \n",
      "CPU times: user 454 ms, sys: 214 ms, total: 668 ms\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub34a-max.csv -m \"\"\n",
    "\n",
    "from time import sleep\n",
    "sleep(60)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [09:22<00:00, 20.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create submit\n",
    "\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "predicted = []\n",
    "draw_predict = []\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3), \n",
    "    n_out=28)\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.compile(loss=f1_loss,\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=[f1])\n",
    "model.load_weights('../cache/IV3-34-maximus.h5')\n",
    "for name in tqdm(submit['Id']):\n",
    "    path = os.path.join('../data/test/', name)\n",
    "    image = data_generator.load_image(path, (SIZE,SIZE,3))\n",
    "    image = data_generator.augment(image)\n",
    "    image = image/255.\n",
    "    score_predict = model.predict(image[np.newaxis])[0]\n",
    "    draw_predict.append(score_predict)\n",
    "    label_predict = np.arange(28)[score_predict>=0.25]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted\n",
    "\n",
    "submit.to_csv('../submissions/sub34b-max.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 476k/476k [00:13<00:00, 36.9kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationfileName        date                 description  status    publicScore  privateScore  \n",
      "--------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub34b-max.csv  2018-11-24 19:03:59               complete  0.459        None          \n",
      "sub34a-max.csv  2018-11-24 18:50:22               complete  0.469        None          \n",
      "sub34-max.csv   2018-11-24 17:27:36               complete  0.473        None          \n",
      "sub33-h.csv     2018-11-24 06:48:41               complete  0.464        None          \n",
      "sub33-g.csv     2018-11-24 06:46:19               complete  0.472        None          \n",
      "sub33-c.csv     2018-11-23 11:48:41               complete  0.493        None          \n",
      "sub33-bb.csv    2018-11-23 11:47:32               complete  0.493        None          \n",
      "sub33-b.csv     2018-11-23 11:46:26               complete  0.498        None          \n",
      "sub33-a.csv     2018-11-23 11:45:09               complete  0.496        None          \n",
      "sub36-a.csv     2018-11-23 10:12:45               complete  0.287        None          \n",
      "sub35b-c.csv    2018-11-22 08:00:23               complete  0.417        None          \n",
      "sub35b-b.csv    2018-11-22 07:59:44               complete  0.415        None          \n",
      "sub35b-a.csv    2018-11-22 07:58:57               complete  0.432        None          \n",
      "sub35-a.csv     2018-11-20 06:54:01               complete  0.315        None          \n",
      "sub8i1-e.csv    2018-11-19 07:12:37               complete  0.457        None          \n",
      "sub8i1-d.csv    2018-11-19 07:09:46               complete  0.459        None          \n",
      "sub8i1-c.csv    2018-11-19 07:08:49               complete  0.462        None          \n",
      "sub8i1-b.csv    2018-11-19 07:08:05               complete  0.462        None          \n",
      "sub8i1-a.csv    2018-11-19 07:07:14               complete  0.460        None          \n",
      "sub32-c.csv     2018-11-15 11:47:19               complete  0.463        None          \n",
      "CPU times: user 426 ms, sys: 230 ms, total: 656 ms\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub34b-max.csv -m \"\"\n",
    "\n",
    "from time import sleep\n",
    "sleep(60)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submit\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "predicted = []\n",
    "draw_predict = []\n",
    "# model = create_model(\n",
    "#     input_shape=(SIZE,SIZE,3), \n",
    "#     n_out=28)\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = True\n",
    "# model.compile(loss=f1_loss,\n",
    "#             optimizer=Adam(lr=1e-4),\n",
    "#             metrics=[f1])\n",
    "model.load_weights('../cache/IV3-34-maximus.h5')\n",
    "for name in tqdm(submit['Id']):\n",
    "    path = os.path.join('../data/test/', name)\n",
    "    image = data_generator.load_image(path, (SIZE,SIZE,3))\n",
    "    image = data_generator.augment(image)\n",
    "    image = image/255.\n",
    "    score_predict = model.predict(image[np.newaxis])[0]\n",
    "    draw_predict.append(score_predict)\n",
    "    label_predict = np.arange(28)[score_predict>=0.2]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted\n",
    "# np.save('../cache/draw_predict_InceptionV3-30.npy', score_predict)\n",
    "# submit.to_csv('../submissions/submit_InceptionV3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName       date                 description  status    publicScore  privateScore  \r\n",
      "-------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub34-max.csv  2018-11-24 17:27:36               complete  0.473        None          \r\n",
      "sub33-h.csv    2018-11-24 06:48:41               complete  0.464        None          \r\n",
      "sub33-g.csv    2018-11-24 06:46:19               complete  0.472        None          \r\n",
      "sub33-c.csv    2018-11-23 11:48:41               complete  0.493        None          \r\n",
      "sub33-bb.csv   2018-11-23 11:47:32               complete  0.493        None          \r\n",
      "sub33-b.csv    2018-11-23 11:46:26               complete  0.498        None          \r\n",
      "sub33-a.csv    2018-11-23 11:45:09               complete  0.496        None          \r\n",
      "sub36-a.csv    2018-11-23 10:12:45               complete  0.287        None          \r\n",
      "sub35b-c.csv   2018-11-22 08:00:23               complete  0.417        None          \r\n",
      "sub35b-b.csv   2018-11-22 07:59:44               complete  0.415        None          \r\n",
      "sub35b-a.csv   2018-11-22 07:58:57               complete  0.432        None          \r\n",
      "sub35-a.csv    2018-11-20 06:54:01               complete  0.315        None          \r\n",
      "sub8i1-e.csv   2018-11-19 07:12:37               complete  0.457        None          \r\n",
      "sub8i1-d.csv   2018-11-19 07:09:46               complete  0.459        None          \r\n",
      "sub8i1-c.csv   2018-11-19 07:08:49               complete  0.462        None          \r\n",
      "sub8i1-b.csv   2018-11-19 07:08:05               complete  0.462        None          \r\n",
      "sub8i1-a.csv   2018-11-19 07:07:14               complete  0.460        None          \r\n",
      "sub32-c.csv    2018-11-15 11:47:19               complete  0.463        None          \r\n",
      "sub32-bb.csv   2018-11-15 11:46:01               complete  0.465        None          \r\n",
      "sub32-b.csv    2018-11-15 11:44:43               complete  0.456        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(60)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.3.8)\r\n",
      "fileName  date                 description  status    publicScore  privateScore  \r\n",
      "--------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub8.csv  2018-10-20 20:08:45               complete  0.422        None          \r\n",
      "sub7.csv  2018-10-20 17:06:09               complete  0.389        None          \r\n",
      "sub5.csv  2018-10-19 18:27:33               complete  0.387        None          \r\n",
      "sub4.csv  2018-10-19 14:45:15               complete  0.411        None          \r\n",
      "sub3.csv  2018-10-19 10:19:26               complete  0.377        None          \r\n",
      "sub2.csv  2018-10-19 08:07:30               complete  0.135        None          \r\n",
      "sub1.csv  2018-10-19 06:28:57               complete  0.374        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(60)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpg",
   "language": "python",
   "name": "hpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

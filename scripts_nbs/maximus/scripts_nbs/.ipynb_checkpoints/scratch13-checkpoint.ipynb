{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/mathormad/inceptionv3-baseline-lb-0-379/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) + (1 - K.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "path_to_train = '../data/train/'\n",
    "data = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_info = []\n",
    "for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "    train_dataset_info.append({\n",
    "        'path':os.path.join(path_to_train, name),\n",
    "        'labels':np.array([int(label) for label in labels])})\n",
    "train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'path': '../data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0', 'labels': array([16,  0])},\n",
       "       {'path': '../data/train/000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', 'labels': array([7, 1, 2, 0])},\n",
       "       {'path': '../data/train/000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([5])},\n",
       "       ...,\n",
       "       {'path': '../data/train/fff189d8-bbab-11e8-b2ba-ac1f6b6435d0', 'labels': array([7])},\n",
       "       {'path': '../data/train/fffdf7e0-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([25,  2, 21])},\n",
       "       {'path': '../data/train/fffe0ffe-bbc0-11e8-b2bb-ac1f6b6435d0', 'labels': array([2, 0])}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            dataset_info = shuffle(dataset_info)\n",
    "            for start in range(0, len(dataset_info), batch_size):\n",
    "                end = min(start + batch_size, len(dataset_info))\n",
    "                batch_images = []\n",
    "                X_train_batch = dataset_info[start:end]\n",
    "                batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "                for i in range(len(X_train_batch)):\n",
    "                    image = data_generator.load_image(\n",
    "                        X_train_batch[i]['path'], shape)   \n",
    "                    if augument:\n",
    "                        image = data_generator.augment(image)\n",
    "                    batch_images.append(image/255.)\n",
    "                    batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "                yield np.array(batch_images, np.float32), batch_labels\n",
    "\n",
    "    def load_image(path, shape):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image = np.stack((\n",
    "            np.array(image_red_ch),\n",
    "            np.array(image_green_ch), \n",
    "            np.array(image_blue_ch)), -1)\n",
    "        w, h = 512, 512\n",
    "        zero_data = np.zeros((h, w), dtype=np.uint8)\n",
    "#         image2 = np.stack((\n",
    "#             np.array(image_yellow_ch),\n",
    "#             zero_data, zero_data), -1)\n",
    "#         print(image1.shape, image2.shape)\n",
    "#         image = np.vstack((image1, image2))\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "\n",
    "    def augment(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=0),\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "            ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=input_shape)\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = base_model(bn)\n",
    "    x = Conv2D(32, kernel_size=(1,1), activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "# https://github.com/ailias/Focal-Loss-implement-on-Tensorflow/blob/master/focal_loss.py\n",
    "def focal_loss_org(prediction_tensor, target_tensor, weights=None, alpha=0.25, gamma=2):\n",
    "    r\"\"\"Compute focal loss for predictions.\n",
    "        Multi-labels Focal loss formula:\n",
    "            FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                 ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "    Args:\n",
    "     prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing the predicted logits for each class\n",
    "     target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing one-hot encoded classification targets\n",
    "     weights: A float tensor of shape [batch_size, num_anchors]\n",
    "     alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "     gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "    Returns:\n",
    "        loss: A (scalar) tensor representing the value of the loss function\n",
    "    \"\"\"\n",
    "    sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                          - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(weights=None, alpha=0.25, gamma=2):\n",
    "    def focal_loss_my(target_tensor, prediction_tensor, ):\n",
    "        r\"\"\"Compute focal loss for predictions.\n",
    "            Multi-labels Focal loss formula:\n",
    "                FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                     ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "        Args:\n",
    "         prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "            num_classes] representing the predicted logits for each class\n",
    "         target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "            num_classes] representing one-hot encoded classification targets\n",
    "         weights: A float tensor of shape [batch_size, num_anchors]\n",
    "         alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "         gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "        Returns:\n",
    "            loss: A (scalar) tensor representing the value of the loss function\n",
    "        \"\"\"\n",
    "        sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "        zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "\n",
    "        # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "        # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "        pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "\n",
    "        # For negative prediction, only need consider back part loss, front part is 0;\n",
    "        # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "        neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "        per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                              - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "        return tf.reduce_sum(per_entry_cross_ent)\n",
    "#         return K.mean(K.binary_crossentropy(target_tensor, prediction_tensor), axis=-1) + tf.reduce_sum(per_entry_cross_ent)\n",
    "    return focal_loss_my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_fixed(y_true, y_pred):\n",
    "    gamma = 2.\n",
    "    alpha = 0.25\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "#     pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "#     pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def focal_loss(gamma=2., alpha=.25):\n",
    "#     def focal_loss_fixed(y_true, y_pred):\n",
    "#         pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "#         pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "#         pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "#         pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "#         return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "#     return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31072,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1554/1554 [==============================] - 363s 234ms/step - loss: 1.1206 - f1: 0.0377 - val_loss: 1.1826 - val_f1: 0.0349\n",
      "Epoch 2/2\n",
      "1554/1554 [==============================] - 352s 227ms/step - loss: 1.1054 - f1: 0.0499 - val_loss: 1.2400 - val_f1: 0.0322\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 422s 271ms/step - loss: 1.0539 - f1: 0.1014 - val_loss: 1.0248 - val_f1: 0.1594\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02485, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.9784 - f1: 0.1705 - val_loss: 0.9364 - val_f1: 0.2308\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02485 to 0.93639, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.9339 - f1: 0.2085 - val_loss: 0.8930 - val_f1: 0.2699\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.93639 to 0.89304, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.9061 - f1: 0.2290 - val_loss: 0.8458 - val_f1: 0.2973\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.89304 to 0.84578, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8882 - f1: 0.2407 - val_loss: 0.8394 - val_f1: 0.3038\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.84578 to 0.83939, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 403s 259ms/step - loss: 0.8725 - f1: 0.2519 - val_loss: 0.8251 - val_f1: 0.3160\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.83939 to 0.82510, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8606 - f1: 0.2593 - val_loss: 0.8120 - val_f1: 0.3258\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.82510 to 0.81198, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8515 - f1: 0.2657 - val_loss: 0.8100 - val_f1: 0.3216\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.81198 to 0.81002, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.8410 - f1: 0.2732 - val_loss: 0.7907 - val_f1: 0.3342\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.81002 to 0.79066, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 402s 258ms/step - loss: 0.8350 - f1: 0.2763 - val_loss: 0.8210 - val_f1: 0.3202\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.79066\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.8271 - f1: 0.2821 - val_loss: 0.7939 - val_f1: 0.3410\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.79066\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8205 - f1: 0.2861 - val_loss: 0.8065 - val_f1: 0.3244\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.79066\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 402s 258ms/step - loss: 0.7927 - f1: 0.3037 - val_loss: 0.7474 - val_f1: 0.3683\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.79066 to 0.74744, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 404s 260ms/step - loss: 0.7829 - f1: 0.3099 - val_loss: 0.7453 - val_f1: 0.3732\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.74744 to 0.74525, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 403s 259ms/step - loss: 0.7817 - f1: 0.3098 - val_loss: 0.7422 - val_f1: 0.3733\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.74525 to 0.74221, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 403s 259ms/step - loss: 0.7740 - f1: 0.3148 - val_loss: 0.7454 - val_f1: 0.3716\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.74221\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 402s 259ms/step - loss: 0.7711 - f1: 0.3163 - val_loss: 0.7476 - val_f1: 0.3703\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.74221\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 403s 259ms/step - loss: 0.7677 - f1: 0.3178 - val_loss: 0.7429 - val_f1: 0.3748\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.74221\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 403s 259ms/step - loss: 0.7660 - f1: 0.3199 - val_loss: 0.7400 - val_f1: 0.3768\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.74221 to 0.73999, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 403s 259ms/step - loss: 0.7668 - f1: 0.3187 - val_loss: 0.7390 - val_f1: 0.3774\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.73999 to 0.73900, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 402s 258ms/step - loss: 0.7630 - f1: 0.3216 - val_loss: 0.7397 - val_f1: 0.3757\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.73900\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 403s 260ms/step - loss: 0.7621 - f1: 0.3228 - val_loss: 0.7380 - val_f1: 0.3771\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.73900 to 0.73798, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 403s 260ms/step - loss: 0.7621 - f1: 0.3218 - val_loss: 0.7397 - val_f1: 0.3774\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.73798\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 404s 260ms/step - loss: 0.7625 - f1: 0.3216 - val_loss: 0.7409 - val_f1: 0.3750\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.73798\n",
      "Epoch 25/120\n",
      "1554/1554 [==============================] - 402s 259ms/step - loss: 0.7633 - f1: 0.3209 - val_loss: 0.7365 - val_f1: 0.3797\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.73798 to 0.73655, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 26/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7614 - f1: 0.3217 - val_loss: 0.7383 - val_f1: 0.3782\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.73655\n",
      "Epoch 27/120\n",
      "1554/1554 [==============================] - 403s 260ms/step - loss: 0.7616 - f1: 0.3226 - val_loss: 0.7418 - val_f1: 0.3740\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.73655\n",
      "Epoch 28/120\n",
      "1554/1554 [==============================] - 402s 259ms/step - loss: 0.7629 - f1: 0.3205 - val_loss: 0.7405 - val_f1: 0.3766\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.73655\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 29/120\n",
      "1554/1554 [==============================] - 403s 259ms/step - loss: 0.7617 - f1: 0.3216 - val_loss: 0.7386 - val_f1: 0.3790\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.73655\n",
      "Epoch 30/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7606 - f1: 0.3226 - val_loss: 0.7382 - val_f1: 0.3780\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.73655\n",
      "Epoch 31/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7590 - f1: 0.3232 - val_loss: 0.7391 - val_f1: 0.3776\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.73655\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "Epoch 32/120\n",
      "1554/1554 [==============================] - 402s 259ms/step - loss: 0.7596 - f1: 0.3236 - val_loss: 0.7397 - val_f1: 0.3774\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.73655\n",
      "Epoch 33/120\n",
      "1554/1554 [==============================] - 403s 260ms/step - loss: 0.7608 - f1: 0.3223 - val_loss: 0.7398 - val_f1: 0.3767\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.73655\n",
      "Epoch 34/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7588 - f1: 0.3250 - val_loss: 0.7419 - val_f1: 0.3751\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.73655\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "Epoch 35/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7612 - f1: 0.3219 - val_loss: 0.7394 - val_f1: 0.3759\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.73655\n",
      "Epoch 36/120\n",
      "1554/1554 [==============================] - 397s 255ms/step - loss: 0.7604 - f1: 0.3234 - val_loss: 0.7371 - val_f1: 0.3796\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.73655\n",
      "Epoch 37/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7612 - f1: 0.3220 - val_loss: 0.7410 - val_f1: 0.3753\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.73655\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
      "Epoch 38/120\n",
      "1554/1554 [==============================] - 397s 256ms/step - loss: 0.7608 - f1: 0.3226 - val_loss: 0.7395 - val_f1: 0.3764\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.73655\n",
      "Epoch 39/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7587 - f1: 0.3238 - val_loss: 0.7384 - val_f1: 0.3787\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.73655\n",
      "Epoch 40/120\n",
      "1554/1554 [==============================] - 397s 256ms/step - loss: 0.7621 - f1: 0.3209 - val_loss: 0.7391 - val_f1: 0.3774\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.73655\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7601 - f1: 0.3231 - val_loss: 0.7390 - val_f1: 0.3784\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.73655\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7610 - f1: 0.3231 - val_loss: 0.7396 - val_f1: 0.3761\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.73655\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7605 - f1: 0.3227 - val_loss: 0.7399 - val_f1: 0.3770\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.73655\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7609 - f1: 0.3224 - val_loss: 0.7384 - val_f1: 0.3782\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.73655\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7633 - f1: 0.3200 - val_loss: 0.7385 - val_f1: 0.3789\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.73655\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7618 - f1: 0.3217 - val_loss: 0.7406 - val_f1: 0.3757\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.73655\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7612 - f1: 0.3222 - val_loss: 0.7383 - val_f1: 0.3781\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.73655\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7581 - f1: 0.3243 - val_loss: 0.7389 - val_f1: 0.3791\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.73655\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7601 - f1: 0.3233 - val_loss: 0.7373 - val_f1: 0.3789\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.73655\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7610 - f1: 0.3218 - val_loss: 0.7405 - val_f1: 0.3755\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.73655\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 399s 256ms/step - loss: 0.7606 - f1: 0.3223 - val_loss: 0.7402 - val_f1: 0.3776\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.73655\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7617 - f1: 0.3217 - val_loss: 0.7377 - val_f1: 0.3800\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.73655\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7594 - f1: 0.3225 - val_loss: 0.7422 - val_f1: 0.3735\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.73655\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7595 - f1: 0.3230 - val_loss: 0.7410 - val_f1: 0.3753\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.73655\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7604 - f1: 0.3226 - val_loss: 0.7428 - val_f1: 0.3737\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.73655\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7567 - f1: 0.3263 - val_loss: 0.7404 - val_f1: 0.3757\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.73655\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7604 - f1: 0.3222 - val_loss: 0.7405 - val_f1: 0.3761\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.73655\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7606 - f1: 0.3230 - val_loss: 0.7390 - val_f1: 0.3769\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.73655\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 399s 256ms/step - loss: 0.7610 - f1: 0.3223 - val_loss: 0.7406 - val_f1: 0.3748\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.73655\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7580 - f1: 0.3249 - val_loss: 0.7392 - val_f1: 0.3777\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.73655\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7593 - f1: 0.3243 - val_loss: 0.7391 - val_f1: 0.3766\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.73655\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7595 - f1: 0.3236 - val_loss: 0.7399 - val_f1: 0.3766\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.73655\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7607 - f1: 0.3227 - val_loss: 0.7413 - val_f1: 0.3751\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.73655\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7600 - f1: 0.3229 - val_loss: 0.7390 - val_f1: 0.3783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6215 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00024: val_loss did not improve from 0.73655\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6215/6215 [05:51<00:00, 17.68it/s]\n",
      "11702it [08:02, 24.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1554/1554 [==============================] - 345s 222ms/step - loss: 1.1203 - f1: 0.0368 - val_loss: 1.1669 - val_f1: 0.0155\n",
      "Epoch 2/2\n",
      "1554/1554 [==============================] - 348s 224ms/step - loss: 1.1065 - f1: 0.0472 - val_loss: 1.1952 - val_f1: 0.0228\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 423s 272ms/step - loss: 1.0611 - f1: 0.0892 - val_loss: 1.0033 - val_f1: 0.1593\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00331, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.9918 - f1: 0.1597 - val_loss: 0.9273 - val_f1: 0.2388\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.00331 to 0.92726, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.9377 - f1: 0.2064 - val_loss: 0.8701 - val_f1: 0.2792\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.92726 to 0.87006, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.9088 - f1: 0.2277 - val_loss: 0.8571 - val_f1: 0.2935\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.87006 to 0.85711, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.8908 - f1: 0.2392 - val_loss: 0.8372 - val_f1: 0.3107\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.85711 to 0.83719, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.8778 - f1: 0.2481 - val_loss: 0.8331 - val_f1: 0.3034\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.83719 to 0.83306, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.8649 - f1: 0.2576 - val_loss: 0.8139 - val_f1: 0.3215\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.83306 to 0.81386, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.8560 - f1: 0.2625 - val_loss: 0.8005 - val_f1: 0.3356\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.81386 to 0.80049, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8461 - f1: 0.2690 - val_loss: 0.8142 - val_f1: 0.3219\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.80049\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.8358 - f1: 0.2768 - val_loss: 0.7975 - val_f1: 0.3374\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.80049 to 0.79750, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.8294 - f1: 0.2806 - val_loss: 0.8057 - val_f1: 0.3373\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.79750\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.8201 - f1: 0.2870 - val_loss: 0.7889 - val_f1: 0.3528\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.79750 to 0.78893, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.8159 - f1: 0.2903 - val_loss: 0.7872 - val_f1: 0.3461\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.78893 to 0.78719, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.8100 - f1: 0.2927 - val_loss: 0.7688 - val_f1: 0.3536\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.78719 to 0.76880, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.8023 - f1: 0.2978 - val_loss: 0.7682 - val_f1: 0.3602\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.76880 to 0.76818, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7989 - f1: 0.2991 - val_loss: 0.7743 - val_f1: 0.3517\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.76818\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7928 - f1: 0.3039 - val_loss: 0.7579 - val_f1: 0.3666\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.76818 to 0.75794, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.7847 - f1: 0.3090 - val_loss: 0.7765 - val_f1: 0.3600\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.75794\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7792 - f1: 0.3112 - val_loss: 0.7753 - val_f1: 0.3535\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.75794\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7750 - f1: 0.3149 - val_loss: 0.7609 - val_f1: 0.3675\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.75794\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7504 - f1: 0.3285 - val_loss: 0.7321 - val_f1: 0.3833\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.75794 to 0.73206, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.7404 - f1: 0.3342 - val_loss: 0.7325 - val_f1: 0.3832\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.73206\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7357 - f1: 0.3372 - val_loss: 0.7333 - val_f1: 0.3855\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.73206\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.7304 - f1: 0.3403 - val_loss: 0.7354 - val_f1: 0.3815\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.73206\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 25/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.7279 - f1: 0.3415 - val_loss: 0.7319 - val_f1: 0.3845\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.73206 to 0.73186, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 26/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7259 - f1: 0.3429 - val_loss: 0.7299 - val_f1: 0.3873\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.73186 to 0.72993, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 27/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7252 - f1: 0.3443 - val_loss: 0.7339 - val_f1: 0.3839\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.72993\n",
      "Epoch 28/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7260 - f1: 0.3431 - val_loss: 0.7333 - val_f1: 0.3829\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.72993\n",
      "Epoch 29/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7268 - f1: 0.3418 - val_loss: 0.7332 - val_f1: 0.3844\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.72993\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 30/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7241 - f1: 0.3445 - val_loss: 0.7328 - val_f1: 0.3851\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.72993\n",
      "Epoch 31/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7250 - f1: 0.3435 - val_loss: 0.7302 - val_f1: 0.3872\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.72993\n",
      "Epoch 32/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.7250 - f1: 0.3440 - val_loss: 0.7328 - val_f1: 0.3848\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.72993\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "Epoch 33/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.7238 - f1: 0.3439 - val_loss: 0.7329 - val_f1: 0.3849\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.72993\n",
      "Epoch 34/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.7224 - f1: 0.3453 - val_loss: 0.7323 - val_f1: 0.3848\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.72993\n",
      "Epoch 35/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7251 - f1: 0.3436 - val_loss: 0.7314 - val_f1: 0.3854\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.72993\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "Epoch 36/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7247 - f1: 0.3440 - val_loss: 0.7328 - val_f1: 0.3848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00036: val_loss did not improve from 0.72993\n",
      "Epoch 37/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7243 - f1: 0.3436 - val_loss: 0.7344 - val_f1: 0.3824\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.72993\n",
      "Epoch 38/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7237 - f1: 0.3446 - val_loss: 0.7337 - val_f1: 0.3830\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.72993\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
      "Epoch 39/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.7245 - f1: 0.3426 - val_loss: 0.7337 - val_f1: 0.3836\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.72993\n",
      "Epoch 40/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.7239 - f1: 0.3438 - val_loss: 0.7325 - val_f1: 0.3845\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.72993\n",
      "Epoch 41/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7263 - f1: 0.3412 - val_loss: 0.7329 - val_f1: 0.3847\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.72993\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7240 - f1: 0.3441 - val_loss: 0.7344 - val_f1: 0.3819\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.72993\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.7262 - f1: 0.3427 - val_loss: 0.7323 - val_f1: 0.3851\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.72993\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.7232 - f1: 0.3447 - val_loss: 0.7296 - val_f1: 0.3878\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.72993 to 0.72956, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7214 - f1: 0.3471 - val_loss: 0.7344 - val_f1: 0.3832\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.72956\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7256 - f1: 0.3423 - val_loss: 0.7294 - val_f1: 0.3884\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.72956 to 0.72937, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 402s 259ms/step - loss: 0.7250 - f1: 0.3429 - val_loss: 0.7319 - val_f1: 0.3846\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.72937\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 402s 259ms/step - loss: 0.7236 - f1: 0.3450 - val_loss: 0.7318 - val_f1: 0.3854\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.72937\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 402s 259ms/step - loss: 0.7238 - f1: 0.3443 - val_loss: 0.7335 - val_f1: 0.3840\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.72937\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 402s 259ms/step - loss: 0.7244 - f1: 0.3432 - val_loss: 0.7328 - val_f1: 0.3834\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.72937\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7217 - f1: 0.3464 - val_loss: 0.7321 - val_f1: 0.3838\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.72937\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 402s 259ms/step - loss: 0.7250 - f1: 0.3434 - val_loss: 0.7325 - val_f1: 0.3846\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.72937\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.7226 - f1: 0.3450 - val_loss: 0.7318 - val_f1: 0.3850\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.72937\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 402s 259ms/step - loss: 0.7237 - f1: 0.3446 - val_loss: 0.7349 - val_f1: 0.3819\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.72937\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 402s 258ms/step - loss: 0.7237 - f1: 0.3445 - val_loss: 0.7322 - val_f1: 0.3849\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.72937\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7243 - f1: 0.3440 - val_loss: 0.7317 - val_f1: 0.3852\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.72937\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.7239 - f1: 0.3437 - val_loss: 0.7349 - val_f1: 0.3826\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.72937\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.7229 - f1: 0.3452 - val_loss: 0.7315 - val_f1: 0.3847\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.72937\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7238 - f1: 0.3445 - val_loss: 0.7317 - val_f1: 0.3856\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.72937\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7232 - f1: 0.3447 - val_loss: 0.7322 - val_f1: 0.3852\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.72937\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7253 - f1: 0.3424 - val_loss: 0.7329 - val_f1: 0.3838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6215 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: val_loss did not improve from 0.72937\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6215/6215 [05:23<00:00, 19.22it/s]\n",
      "11702it [08:20, 23.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1554/1554 [==============================] - 350s 225ms/step - loss: 1.1202 - f1: 0.0389 - val_loss: 1.1775 - val_f1: 0.0343\n",
      "Epoch 2/2\n",
      "1554/1554 [==============================] - 339s 218ms/step - loss: 1.1056 - f1: 0.0496 - val_loss: 1.1529 - val_f1: 0.0326\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 415s 267ms/step - loss: 1.0538 - f1: 0.1002 - val_loss: 0.9814 - val_f1: 0.1819\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.98139, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 392s 253ms/step - loss: 0.9798 - f1: 0.1715 - val_loss: 0.9063 - val_f1: 0.2484\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.98139 to 0.90634, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 393s 253ms/step - loss: 0.9327 - f1: 0.2100 - val_loss: 0.8906 - val_f1: 0.2634\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.90634 to 0.89056, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 393s 253ms/step - loss: 0.9060 - f1: 0.2285 - val_loss: 0.8776 - val_f1: 0.2731\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.89056 to 0.87762, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 393s 253ms/step - loss: 0.8868 - f1: 0.2414 - val_loss: 0.8803 - val_f1: 0.2733\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.87762\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 393s 253ms/step - loss: 0.8730 - f1: 0.2513 - val_loss: 0.8173 - val_f1: 0.3268\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.87762 to 0.81734, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 393s 253ms/step - loss: 0.8606 - f1: 0.2596 - val_loss: 0.8034 - val_f1: 0.3297\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.81734 to 0.80342, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 393s 253ms/step - loss: 0.8500 - f1: 0.2668 - val_loss: 0.8075 - val_f1: 0.3229\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.80342\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 393s 253ms/step - loss: 0.8405 - f1: 0.2736 - val_loss: 0.7979 - val_f1: 0.3356\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.80342 to 0.79794, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 393s 253ms/step - loss: 0.8342 - f1: 0.2774 - val_loss: 0.8189 - val_f1: 0.3242\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.79794\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 393s 253ms/step - loss: 0.8264 - f1: 0.2827 - val_loss: 0.7856 - val_f1: 0.3394\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.79794 to 0.78558, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 393s 253ms/step - loss: 0.8174 - f1: 0.2891 - val_loss: 0.7752 - val_f1: 0.3478\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.78558 to 0.77524, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 393s 253ms/step - loss: 0.8113 - f1: 0.2928 - val_loss: 0.7945 - val_f1: 0.3416\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.77524\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 392s 252ms/step - loss: 0.8046 - f1: 0.2961 - val_loss: 0.7694 - val_f1: 0.3565\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.77524 to 0.76942, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 393s 253ms/step - loss: 0.7996 - f1: 0.3001 - val_loss: 0.8007 - val_f1: 0.3370\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.76942\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 393s 253ms/step - loss: 0.7926 - f1: 0.3046 - val_loss: 0.7837 - val_f1: 0.3496\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.76942\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 393s 253ms/step - loss: 0.7857 - f1: 0.3086 - val_loss: 0.8091 - val_f1: 0.3276\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.76942\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 394s 253ms/step - loss: 0.7622 - f1: 0.3230 - val_loss: 0.7348 - val_f1: 0.3802\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.76942 to 0.73477, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7532 - f1: 0.3283 - val_loss: 0.7345 - val_f1: 0.3830\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.73477 to 0.73446, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 394s 253ms/step - loss: 0.7476 - f1: 0.3304 - val_loss: 0.7352 - val_f1: 0.3805\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.73446\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7431 - f1: 0.3345 - val_loss: 0.7344 - val_f1: 0.3810\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.73446 to 0.73442, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 393s 253ms/step - loss: 0.7381 - f1: 0.3376 - val_loss: 0.7333 - val_f1: 0.3826\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.73442 to 0.73335, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7364 - f1: 0.3377 - val_loss: 0.7343 - val_f1: 0.3809\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.73335\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7328 - f1: 0.3399 - val_loss: 0.7369 - val_f1: 0.3778\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.73335\n",
      "Epoch 25/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7319 - f1: 0.3399 - val_loss: 0.7346 - val_f1: 0.3844\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.73335\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 26/120\n",
      "1554/1554 [==============================] - 402s 259ms/step - loss: 0.7247 - f1: 0.3452 - val_loss: 0.7359 - val_f1: 0.3817\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.73335\n",
      "Epoch 27/120\n",
      "1554/1554 [==============================] - 402s 259ms/step - loss: 0.7269 - f1: 0.3430 - val_loss: 0.7328 - val_f1: 0.3845\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.73335 to 0.73278, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 28/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7268 - f1: 0.3424 - val_loss: 0.7321 - val_f1: 0.3854\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.73278 to 0.73207, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 29/120\n",
      "1554/1554 [==============================] - 402s 259ms/step - loss: 0.7252 - f1: 0.3447 - val_loss: 0.7329 - val_f1: 0.3844\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.73207\n",
      "Epoch 30/120\n",
      "1554/1554 [==============================] - 402s 258ms/step - loss: 0.7254 - f1: 0.3437 - val_loss: 0.7326 - val_f1: 0.3857\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.73207\n",
      "Epoch 31/120\n",
      "1554/1554 [==============================] - 402s 259ms/step - loss: 0.7265 - f1: 0.3429 - val_loss: 0.7351 - val_f1: 0.3828\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.73207\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 32/120\n",
      "1554/1554 [==============================] - 402s 259ms/step - loss: 0.7245 - f1: 0.3448 - val_loss: 0.7311 - val_f1: 0.3862\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.73207 to 0.73110, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 33/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7259 - f1: 0.3431 - val_loss: 0.7318 - val_f1: 0.3871\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.73110\n",
      "Epoch 34/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7229 - f1: 0.3453 - val_loss: 0.7357 - val_f1: 0.3818\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.73110\n",
      "Epoch 35/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7244 - f1: 0.3448 - val_loss: 0.7341 - val_f1: 0.3832\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.73110\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "Epoch 36/120\n",
      "1554/1554 [==============================] - 402s 258ms/step - loss: 0.7255 - f1: 0.3437 - val_loss: 0.7334 - val_f1: 0.3844\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.73110\n",
      "Epoch 37/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7236 - f1: 0.3455 - val_loss: 0.7333 - val_f1: 0.3846\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.73110\n",
      "Epoch 38/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7244 - f1: 0.3446 - val_loss: 0.7317 - val_f1: 0.3861\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.73110\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "Epoch 39/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7231 - f1: 0.3460 - val_loss: 0.7356 - val_f1: 0.3824\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.73110\n",
      "Epoch 40/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7246 - f1: 0.3444 - val_loss: 0.7340 - val_f1: 0.3836\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.73110\n",
      "Epoch 41/120\n",
      "1554/1554 [==============================] - 394s 253ms/step - loss: 0.7255 - f1: 0.3443 - val_loss: 0.7298 - val_f1: 0.3887\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.73110 to 0.72984, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 42/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7240 - f1: 0.3447 - val_loss: 0.7367 - val_f1: 0.3814\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.72984\n",
      "Epoch 43/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7246 - f1: 0.3437 - val_loss: 0.7349 - val_f1: 0.3827\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.72984\n",
      "Epoch 44/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7241 - f1: 0.3449 - val_loss: 0.7345 - val_f1: 0.3829\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
      "Epoch 45/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7264 - f1: 0.3428 - val_loss: 0.7338 - val_f1: 0.3840\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.72984\n",
      "Epoch 46/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7237 - f1: 0.3453 - val_loss: 0.7326 - val_f1: 0.3851\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.72984\n",
      "Epoch 47/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7246 - f1: 0.3445 - val_loss: 0.7357 - val_f1: 0.3820\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
      "Epoch 48/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7247 - f1: 0.3437 - val_loss: 0.7353 - val_f1: 0.3824\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.72984\n",
      "Epoch 49/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7255 - f1: 0.3435 - val_loss: 0.7333 - val_f1: 0.3840\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.72984\n",
      "Epoch 50/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7234 - f1: 0.3448 - val_loss: 0.7345 - val_f1: 0.3820\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "Epoch 51/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7275 - f1: 0.3415 - val_loss: 0.7380 - val_f1: 0.3791\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.72984\n",
      "Epoch 52/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7250 - f1: 0.3443 - val_loss: 0.7337 - val_f1: 0.3840\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.72984\n",
      "Epoch 53/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7239 - f1: 0.3451 - val_loss: 0.7350 - val_f1: 0.3827\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "Epoch 54/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7231 - f1: 0.3456 - val_loss: 0.7367 - val_f1: 0.3812\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.72984\n",
      "Epoch 55/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7245 - f1: 0.3442 - val_loss: 0.7345 - val_f1: 0.3832\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.72984\n",
      "Epoch 56/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7239 - f1: 0.3453 - val_loss: 0.7316 - val_f1: 0.3865\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7239 - f1: 0.3452 - val_loss: 0.7322 - val_f1: 0.3842\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.72984\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7238 - f1: 0.3448 - val_loss: 0.7316 - val_f1: 0.3856\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.72984\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7224 - f1: 0.3466 - val_loss: 0.7335 - val_f1: 0.3837\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.72984\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7231 - f1: 0.3457 - val_loss: 0.7339 - val_f1: 0.3838\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.72984\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7244 - f1: 0.3447 - val_loss: 0.7345 - val_f1: 0.3823\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7243 - f1: 0.3446 - val_loss: 0.7345 - val_f1: 0.3835\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.72984\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7245 - f1: 0.3439 - val_loss: 0.7333 - val_f1: 0.3838\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.72984\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7232 - f1: 0.3458 - val_loss: 0.7319 - val_f1: 0.3853\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7236 - f1: 0.3453 - val_loss: 0.7343 - val_f1: 0.3835\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.72984\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7264 - f1: 0.3435 - val_loss: 0.7324 - val_f1: 0.3848\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.72984\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7239 - f1: 0.3453 - val_loss: 0.7328 - val_f1: 0.3849\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999998845134856e-18.\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7242 - f1: 0.3446 - val_loss: 0.7332 - val_f1: 0.3836\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.72984\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7241 - f1: 0.3450 - val_loss: 0.7341 - val_f1: 0.3839\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.72984\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7253 - f1: 0.3435 - val_loss: 0.7341 - val_f1: 0.3843\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.999999010570977e-19.\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7251 - f1: 0.3445 - val_loss: 0.7323 - val_f1: 0.3860\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.72984\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7247 - f1: 0.3438 - val_loss: 0.7354 - val_f1: 0.3815\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.72984\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7263 - f1: 0.3426 - val_loss: 0.7314 - val_f1: 0.3871\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.72984\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7243 - f1: 0.3448 - val_loss: 0.7339 - val_f1: 0.3831\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.72984\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7254 - f1: 0.3432 - val_loss: 0.7316 - val_f1: 0.3865\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.72984\n",
      "Epoch 20/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7237 - f1: 0.3452 - val_loss: 0.7314 - val_f1: 0.3865\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999424161285e-20.\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7243 - f1: 0.3443 - val_loss: 0.7327 - val_f1: 0.3844\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.72984\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7235 - f1: 0.3452 - val_loss: 0.7347 - val_f1: 0.3830\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.72984\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7239 - f1: 0.3453 - val_loss: 0.7355 - val_f1: 0.3830\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-21.\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7230 - f1: 0.3453 - val_loss: 0.7356 - val_f1: 0.3812\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.72984\n",
      "Epoch 25/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7250 - f1: 0.3440 - val_loss: 0.7347 - val_f1: 0.3837\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.72984\n",
      "Epoch 26/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7239 - f1: 0.3453 - val_loss: 0.7328 - val_f1: 0.3841\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-22.\n",
      "Epoch 27/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7232 - f1: 0.3457 - val_loss: 0.7313 - val_f1: 0.3879\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.72984\n",
      "Epoch 28/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7237 - f1: 0.3452 - val_loss: 0.7318 - val_f1: 0.3854\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.72984\n",
      "Epoch 29/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7255 - f1: 0.3432 - val_loss: 0.7322 - val_f1: 0.3864\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-23.\n",
      "Epoch 30/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7242 - f1: 0.3448 - val_loss: 0.7334 - val_f1: 0.3843\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.72984\n",
      "Epoch 31/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7241 - f1: 0.3447 - val_loss: 0.7337 - val_f1: 0.3841\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.72984\n",
      "Epoch 32/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7222 - f1: 0.3468 - val_loss: 0.7357 - val_f1: 0.3807\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999682655227e-24.\n",
      "Epoch 33/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7216 - f1: 0.3473 - val_loss: 0.7344 - val_f1: 0.3841\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.72984\n",
      "Epoch 34/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7232 - f1: 0.3461 - val_loss: 0.7324 - val_f1: 0.3848\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.72984\n",
      "Epoch 35/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7255 - f1: 0.3433 - val_loss: 0.7334 - val_f1: 0.3837\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.999999998199588e-25.\n",
      "Epoch 36/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7241 - f1: 0.3450 - val_loss: 0.7336 - val_f1: 0.3837\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.72984\n",
      "Epoch 37/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7244 - f1: 0.3448 - val_loss: 0.7338 - val_f1: 0.3843\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.72984\n",
      "Epoch 38/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7242 - f1: 0.3440 - val_loss: 0.7316 - val_f1: 0.3862\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000195414814e-25.\n",
      "Epoch 39/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7249 - f1: 0.3446 - val_loss: 0.7342 - val_f1: 0.3840\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.72984\n",
      "Epoch 40/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7244 - f1: 0.3438 - val_loss: 0.7340 - val_f1: 0.3840\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.72984\n",
      "Epoch 41/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7252 - f1: 0.3440 - val_loss: 0.7346 - val_f1: 0.3828\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.72984\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000000195414814e-26.\n",
      "Epoch 42/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7237 - f1: 0.3453 - val_loss: 0.7325 - val_f1: 0.3848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6214 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: val_loss did not improve from 0.72984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6214/6214 [05:44<00:00, 18.03it/s]\n",
      "11702it [08:18, 23.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1554/1554 [==============================] - 366s 235ms/step - loss: 1.1221 - f1: 0.0355 - val_loss: 1.1347 - val_f1: 0.0244\n",
      "Epoch 2/2\n",
      "1554/1554 [==============================] - 352s 227ms/step - loss: 1.1066 - f1: 0.0476 - val_loss: 1.1796 - val_f1: 0.0304\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 426s 274ms/step - loss: 1.0536 - f1: 0.1038 - val_loss: 0.9845 - val_f1: 0.1771\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.98448, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.9767 - f1: 0.1735 - val_loss: 0.9188 - val_f1: 0.2433\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.98448 to 0.91880, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.9310 - f1: 0.2111 - val_loss: 0.9199 - val_f1: 0.2420\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.91880\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.9033 - f1: 0.2316 - val_loss: 0.8468 - val_f1: 0.2916\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.91880 to 0.84680, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.8840 - f1: 0.2448 - val_loss: 0.8234 - val_f1: 0.3122\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.84680 to 0.82342, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8708 - f1: 0.2538 - val_loss: 0.8197 - val_f1: 0.3180\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.82342 to 0.81969, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.8583 - f1: 0.2619 - val_loss: 0.8084 - val_f1: 0.3252\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.81969 to 0.80836, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8479 - f1: 0.2691 - val_loss: 0.8156 - val_f1: 0.3219\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.80836\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8389 - f1: 0.2745 - val_loss: 0.7990 - val_f1: 0.3329\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.80836 to 0.79898, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 405s 260ms/step - loss: 0.8299 - f1: 0.2798 - val_loss: 0.7922 - val_f1: 0.3513\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.79898 to 0.79216, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 408s 263ms/step - loss: 0.8231 - f1: 0.2844 - val_loss: 0.7832 - val_f1: 0.3457\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.79216 to 0.78320, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.8168 - f1: 0.2885 - val_loss: 0.7881 - val_f1: 0.3430\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.78320\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.8110 - f1: 0.2927 - val_loss: 0.7879 - val_f1: 0.3393\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.78320\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.8027 - f1: 0.2981 - val_loss: 0.7837 - val_f1: 0.3446\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.78320\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7791 - f1: 0.3114 - val_loss: 0.7428 - val_f1: 0.3719\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.78320 to 0.74278, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7677 - f1: 0.3181 - val_loss: 0.7370 - val_f1: 0.3778\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.74278 to 0.73702, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7613 - f1: 0.3227 - val_loss: 0.7396 - val_f1: 0.3766\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.73702\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7604 - f1: 0.3230 - val_loss: 0.7385 - val_f1: 0.3789\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.73702\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7556 - f1: 0.3257 - val_loss: 0.7346 - val_f1: 0.3827\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.73702 to 0.73464, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7532 - f1: 0.3283 - val_loss: 0.7363 - val_f1: 0.3812\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.73464\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7490 - f1: 0.3306 - val_loss: 0.7371 - val_f1: 0.3796\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.73464\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7447 - f1: 0.3322 - val_loss: 0.7389 - val_f1: 0.3775\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.73464\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7417 - f1: 0.3345 - val_loss: 0.7366 - val_f1: 0.3815\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.73464\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7419 - f1: 0.3346 - val_loss: 0.7347 - val_f1: 0.3838\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.73464\n",
      "Epoch 25/120\n",
      "1554/1554 [==============================] - 405s 261ms/step - loss: 0.7415 - f1: 0.3344 - val_loss: 0.7375 - val_f1: 0.3797\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.73464\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 26/120\n",
      "1554/1554 [==============================] - 405s 261ms/step - loss: 0.7403 - f1: 0.3348 - val_loss: 0.7332 - val_f1: 0.3847\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.73464 to 0.73319, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 27/120\n",
      "1554/1554 [==============================] - 405s 261ms/step - loss: 0.7399 - f1: 0.3355 - val_loss: 0.7347 - val_f1: 0.3834\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.73319\n",
      "Epoch 28/120\n",
      "1554/1554 [==============================] - 405s 261ms/step - loss: 0.7408 - f1: 0.3349 - val_loss: 0.7362 - val_f1: 0.3839\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.73319\n",
      "Epoch 29/120\n",
      "1554/1554 [==============================] - 405s 261ms/step - loss: 0.7394 - f1: 0.3362 - val_loss: 0.7340 - val_f1: 0.3838\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.73319\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "Epoch 30/120\n",
      "1554/1554 [==============================] - 405s 261ms/step - loss: 0.7407 - f1: 0.3350 - val_loss: 0.7349 - val_f1: 0.3827\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.73319\n",
      "Epoch 31/120\n",
      "1554/1554 [==============================] - 405s 261ms/step - loss: 0.7374 - f1: 0.3382 - val_loss: 0.7349 - val_f1: 0.3840\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.73319\n",
      "Epoch 32/120\n",
      "1554/1554 [==============================] - 405s 261ms/step - loss: 0.7404 - f1: 0.3353 - val_loss: 0.7369 - val_f1: 0.3811\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.73319\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "Epoch 33/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7416 - f1: 0.3338 - val_loss: 0.7335 - val_f1: 0.3849\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.73319\n",
      "Epoch 34/120\n",
      "1554/1554 [==============================] - 405s 260ms/step - loss: 0.7401 - f1: 0.3349 - val_loss: 0.7336 - val_f1: 0.3836\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.73319\n",
      "Epoch 35/120\n",
      "1554/1554 [==============================] - 405s 260ms/step - loss: 0.7400 - f1: 0.3351 - val_loss: 0.7359 - val_f1: 0.3820\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.73319\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
      "Epoch 36/120\n",
      "1554/1554 [==============================] - 405s 261ms/step - loss: 0.7418 - f1: 0.3337 - val_loss: 0.7323 - val_f1: 0.3862\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.73319 to 0.73233, saving model to ../cache/InceptionV3.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7403 - f1: 0.3351 - val_loss: 0.7345 - val_f1: 0.3825\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.73233\n",
      "Epoch 38/120\n",
      "1554/1554 [==============================] - 397s 256ms/step - loss: 0.7391 - f1: 0.3361 - val_loss: 0.7334 - val_f1: 0.3855\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.73233\n",
      "Epoch 39/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7407 - f1: 0.3339 - val_loss: 0.7328 - val_f1: 0.3861\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.73233\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
      "Epoch 40/120\n",
      "1554/1554 [==============================] - 397s 256ms/step - loss: 0.7405 - f1: 0.3347 - val_loss: 0.7338 - val_f1: 0.3859\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.73233\n",
      "Epoch 41/120\n",
      "1554/1554 [==============================] - 397s 255ms/step - loss: 0.7395 - f1: 0.3359 - val_loss: 0.7335 - val_f1: 0.3847\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.73233\n",
      "Epoch 42/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7408 - f1: 0.3340 - val_loss: 0.7353 - val_f1: 0.3829\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.73233\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "Epoch 43/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7395 - f1: 0.3358 - val_loss: 0.7365 - val_f1: 0.3815\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.73233\n",
      "Epoch 44/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7397 - f1: 0.3354 - val_loss: 0.7336 - val_f1: 0.3848\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.73233\n",
      "Epoch 45/120\n",
      "1554/1554 [==============================] - 397s 256ms/step - loss: 0.7415 - f1: 0.3348 - val_loss: 0.7352 - val_f1: 0.3825\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.73233\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "Epoch 46/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7397 - f1: 0.3357 - val_loss: 0.7352 - val_f1: 0.3817\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.73233\n",
      "Epoch 47/120\n",
      "1554/1554 [==============================] - 397s 256ms/step - loss: 0.7411 - f1: 0.3346 - val_loss: 0.7340 - val_f1: 0.3840\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.73233\n",
      "Epoch 48/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7409 - f1: 0.3345 - val_loss: 0.7342 - val_f1: 0.3844\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.73233\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "Epoch 49/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7416 - f1: 0.3339 - val_loss: 0.7372 - val_f1: 0.3812\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.73233\n",
      "Epoch 50/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.7411 - f1: 0.3343 - val_loss: 0.7350 - val_f1: 0.3828\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.73233\n",
      "Epoch 51/120\n",
      "1554/1554 [==============================] - 409s 263ms/step - loss: 0.7413 - f1: 0.3335 - val_loss: 0.7373 - val_f1: 0.3820\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.73233\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7400 - f1: 0.3357 - val_loss: 0.7352 - val_f1: 0.3832\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.73233\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7393 - f1: 0.3356 - val_loss: 0.7324 - val_f1: 0.3860\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.73233\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 406s 262ms/step - loss: 0.7392 - f1: 0.3358 - val_loss: 0.7333 - val_f1: 0.3841\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.73233\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 406s 262ms/step - loss: 0.7387 - f1: 0.3368 - val_loss: 0.7343 - val_f1: 0.3835\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.73233\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7404 - f1: 0.3351 - val_loss: 0.7333 - val_f1: 0.3850\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.73233\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 406s 262ms/step - loss: 0.7407 - f1: 0.3347 - val_loss: 0.7356 - val_f1: 0.3825\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.73233\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7392 - f1: 0.3360 - val_loss: 0.7381 - val_f1: 0.3809\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.73233\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7407 - f1: 0.3343 - val_loss: 0.7328 - val_f1: 0.3864\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.73233\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999998845134856e-18.\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7382 - f1: 0.3367 - val_loss: 0.7343 - val_f1: 0.3828\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.73233\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7386 - f1: 0.3371 - val_loss: 0.7347 - val_f1: 0.3830\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.73233\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7404 - f1: 0.3349 - val_loss: 0.7365 - val_f1: 0.3817\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.73233\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999010570977e-19.\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7412 - f1: 0.3344 - val_loss: 0.7351 - val_f1: 0.3833\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.73233\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 406s 262ms/step - loss: 0.7386 - f1: 0.3366 - val_loss: 0.7342 - val_f1: 0.3827\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.73233\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7384 - f1: 0.3367 - val_loss: 0.7357 - val_f1: 0.3830\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.73233\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.999999424161285e-20.\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7390 - f1: 0.3360 - val_loss: 0.7362 - val_f1: 0.3821\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.73233\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7404 - f1: 0.3350 - val_loss: 0.7342 - val_f1: 0.3844\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.73233\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 406s 262ms/step - loss: 0.7396 - f1: 0.3360 - val_loss: 0.7334 - val_f1: 0.3850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6214 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00017: val_loss did not improve from 0.73233\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-21.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6214/6214 [05:35<00:00, 18.55it/s]\n",
      "11702it [08:28, 23.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1554/1554 [==============================] - 368s 237ms/step - loss: 1.1202 - f1: 0.0386 - val_loss: 1.2982 - val_f1: 0.0319\n",
      "Epoch 2/2\n",
      "1554/1554 [==============================] - 347s 223ms/step - loss: 1.1061 - f1: 0.0487 - val_loss: 1.1658 - val_f1: 0.0317\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 430s 277ms/step - loss: 1.0582 - f1: 0.0981 - val_loss: 1.0155 - val_f1: 0.1488\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01546, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.9840 - f1: 0.1673 - val_loss: 0.9163 - val_f1: 0.2399\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01546 to 0.91633, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.9369 - f1: 0.2062 - val_loss: 0.9298 - val_f1: 0.2405\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.91633\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.9086 - f1: 0.2285 - val_loss: 0.8618 - val_f1: 0.2939\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.91633 to 0.86176, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8907 - f1: 0.2396 - val_loss: 0.8490 - val_f1: 0.2981\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.86176 to 0.84903, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8761 - f1: 0.2502 - val_loss: 0.8381 - val_f1: 0.3032\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.84903 to 0.83811, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8657 - f1: 0.2575 - val_loss: 0.8127 - val_f1: 0.3179\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.83811 to 0.81265, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8572 - f1: 0.2612 - val_loss: 0.8249 - val_f1: 0.3213\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.81265\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8458 - f1: 0.2692 - val_loss: 0.7919 - val_f1: 0.3383\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.81265 to 0.79193, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8410 - f1: 0.2722 - val_loss: 0.7979 - val_f1: 0.3372\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.79193\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.8312 - f1: 0.2795 - val_loss: 0.8122 - val_f1: 0.3231\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.79193\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8254 - f1: 0.2819 - val_loss: 0.8101 - val_f1: 0.3302\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.79193\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.8001 - f1: 0.2979 - val_loss: 0.7440 - val_f1: 0.3728\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.79193 to 0.74397, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.7892 - f1: 0.3056 - val_loss: 0.7406 - val_f1: 0.3779\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.74397 to 0.74063, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7843 - f1: 0.3075 - val_loss: 0.7460 - val_f1: 0.3701\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.74063\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.7798 - f1: 0.3115 - val_loss: 0.7445 - val_f1: 0.3741\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.74063\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7766 - f1: 0.3128 - val_loss: 0.7421 - val_f1: 0.3753\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.74063\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7741 - f1: 0.3145 - val_loss: 0.7381 - val_f1: 0.3787\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.74063 to 0.73815, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7729 - f1: 0.3148 - val_loss: 0.7413 - val_f1: 0.3761\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.73815\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 417s 268ms/step - loss: 0.7716 - f1: 0.3163 - val_loss: 0.7396 - val_f1: 0.3760\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.73815\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.7692 - f1: 0.3178 - val_loss: 0.7422 - val_f1: 0.3735\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.73815\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.7701 - f1: 0.3165 - val_loss: 0.7408 - val_f1: 0.3757\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.73815\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 409s 263ms/step - loss: 0.7712 - f1: 0.3161 - val_loss: 0.7414 - val_f1: 0.3759\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.73815\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 412s 265ms/step - loss: 0.7691 - f1: 0.3180 - val_loss: 0.7389 - val_f1: 0.3773\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.73815\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "Epoch 25/120\n",
      "1554/1554 [==============================] - 412s 265ms/step - loss: 0.7705 - f1: 0.3164 - val_loss: 0.7400 - val_f1: 0.3765\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.73815\n",
      "Epoch 26/120\n",
      "1554/1554 [==============================] - 412s 265ms/step - loss: 0.7698 - f1: 0.3173 - val_loss: 0.7420 - val_f1: 0.3737\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.73815\n",
      "Epoch 27/120\n",
      "1554/1554 [==============================] - 409s 263ms/step - loss: 0.7704 - f1: 0.3170 - val_loss: 0.7378 - val_f1: 0.3782\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.73815 to 0.73782, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 28/120\n",
      "1554/1554 [==============================] - 411s 265ms/step - loss: 0.7708 - f1: 0.3159 - val_loss: 0.7415 - val_f1: 0.3752\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.73782\n",
      "Epoch 29/120\n",
      "1554/1554 [==============================] - 413s 266ms/step - loss: 0.7702 - f1: 0.3168 - val_loss: 0.7385 - val_f1: 0.3778\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.73782\n",
      "Epoch 30/120\n",
      "1554/1554 [==============================] - 412s 265ms/step - loss: 0.7686 - f1: 0.3187 - val_loss: 0.7388 - val_f1: 0.3780\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.73782\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "Epoch 31/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.7715 - f1: 0.3156 - val_loss: 0.7382 - val_f1: 0.3782\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.73782\n",
      "Epoch 32/120\n",
      "1554/1554 [==============================] - 411s 264ms/step - loss: 0.7693 - f1: 0.3182 - val_loss: 0.7417 - val_f1: 0.3736\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.73782\n",
      "Epoch 33/120\n",
      "1554/1554 [==============================] - 412s 265ms/step - loss: 0.7696 - f1: 0.3171 - val_loss: 0.7392 - val_f1: 0.3768\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.73782\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
      "Epoch 34/120\n",
      "1554/1554 [==============================] - 412s 265ms/step - loss: 0.7684 - f1: 0.3187 - val_loss: 0.7403 - val_f1: 0.3757\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.73782\n",
      "Epoch 35/120\n",
      "1554/1554 [==============================] - 411s 264ms/step - loss: 0.7696 - f1: 0.3163 - val_loss: 0.7414 - val_f1: 0.3752\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.73782\n",
      "Epoch 36/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.7687 - f1: 0.3179 - val_loss: 0.7414 - val_f1: 0.3748\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.73782\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
      "Epoch 37/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1554/1554 [==============================] - 403s 260ms/step - loss: 0.7711 - f1: 0.3161 - val_loss: 0.7410 - val_f1: 0.3752\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.73782\n",
      "Epoch 38/120\n",
      "1554/1554 [==============================] - 403s 259ms/step - loss: 0.7705 - f1: 0.3156 - val_loss: 0.7431 - val_f1: 0.3725\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.73782\n",
      "Epoch 39/120\n",
      "1554/1554 [==============================] - 403s 259ms/step - loss: 0.7690 - f1: 0.3173 - val_loss: 0.7400 - val_f1: 0.3762\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.73782\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "Epoch 40/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7693 - f1: 0.3173 - val_loss: 0.7408 - val_f1: 0.3749\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.73782\n",
      "Epoch 41/120\n",
      "1554/1554 [==============================] - 404s 260ms/step - loss: 0.7700 - f1: 0.3172 - val_loss: 0.7371 - val_f1: 0.3793\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.73782 to 0.73707, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 42/120\n",
      "1554/1554 [==============================] - 404s 260ms/step - loss: 0.7699 - f1: 0.3172 - val_loss: 0.7404 - val_f1: 0.3760\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.73707\n",
      "Epoch 43/120\n",
      "1554/1554 [==============================] - 403s 259ms/step - loss: 0.7702 - f1: 0.3178 - val_loss: 0.7413 - val_f1: 0.3740\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.73707\n",
      "Epoch 44/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7694 - f1: 0.3171 - val_loss: 0.7375 - val_f1: 0.3780\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.73707\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "Epoch 45/120\n",
      "1554/1554 [==============================] - 404s 260ms/step - loss: 0.7704 - f1: 0.3168 - val_loss: 0.7401 - val_f1: 0.3767\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.73707\n",
      "Epoch 46/120\n",
      "1554/1554 [==============================] - 415s 267ms/step - loss: 0.7695 - f1: 0.3172 - val_loss: 0.7411 - val_f1: 0.3753\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.73707\n",
      "Epoch 47/120\n",
      "1554/1554 [==============================] - 416s 267ms/step - loss: 0.7697 - f1: 0.3177 - val_loss: 0.7439 - val_f1: 0.3721\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.73707\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "Epoch 48/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7711 - f1: 0.3160 - val_loss: 0.7397 - val_f1: 0.3761\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.73707\n",
      "Epoch 49/120\n",
      "1554/1554 [==============================] - 409s 263ms/step - loss: 0.7697 - f1: 0.3176 - val_loss: 0.7385 - val_f1: 0.3783\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.73707\n",
      "Epoch 50/120\n",
      "1554/1554 [==============================] - 411s 264ms/step - loss: 0.7684 - f1: 0.3177 - val_loss: 0.7429 - val_f1: 0.3723\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.73707\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
      "Epoch 51/120\n",
      "1554/1554 [==============================] - 408s 263ms/step - loss: 0.7706 - f1: 0.3166 - val_loss: 0.7418 - val_f1: 0.3750\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.73707\n",
      "Epoch 52/120\n",
      "1554/1554 [==============================] - 404s 260ms/step - loss: 0.7702 - f1: 0.3164 - val_loss: 0.7404 - val_f1: 0.3758\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.73707\n",
      "Epoch 53/120\n",
      "1554/1554 [==============================] - 408s 263ms/step - loss: 0.7700 - f1: 0.3169 - val_loss: 0.7416 - val_f1: 0.3741\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.73707\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 9.999998977483754e-17.\n",
      "Epoch 54/120\n",
      "1554/1554 [==============================] - 408s 263ms/step - loss: 0.7694 - f1: 0.3171 - val_loss: 0.7401 - val_f1: 0.3775\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.73707\n",
      "Epoch 55/120\n",
      "1554/1554 [==============================] - 404s 260ms/step - loss: 0.7698 - f1: 0.3172 - val_loss: 0.7408 - val_f1: 0.3754\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.73707\n",
      "Epoch 56/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7713 - f1: 0.3159 - val_loss: 0.7384 - val_f1: 0.3781\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.73707\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 9.999998845134856e-18.\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7713 - f1: 0.3161 - val_loss: 0.7362 - val_f1: 0.3801\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.73707 to 0.73619, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 411s 264ms/step - loss: 0.7705 - f1: 0.3167 - val_loss: 0.7407 - val_f1: 0.3753\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.73619\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 408s 262ms/step - loss: 0.7695 - f1: 0.3173 - val_loss: 0.7378 - val_f1: 0.3788\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.73619\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7691 - f1: 0.3179 - val_loss: 0.7400 - val_f1: 0.3757\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.73619\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999010570977e-19.\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 408s 263ms/step - loss: 0.7714 - f1: 0.3153 - val_loss: 0.7417 - val_f1: 0.3754\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.73619\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 408s 262ms/step - loss: 0.7696 - f1: 0.3171 - val_loss: 0.7412 - val_f1: 0.3756\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.73619\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7701 - f1: 0.3170 - val_loss: 0.7405 - val_f1: 0.3761\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.73619\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999424161285e-20.\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 405s 261ms/step - loss: 0.7689 - f1: 0.3177 - val_loss: 0.7393 - val_f1: 0.3766\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.73619\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7696 - f1: 0.3172 - val_loss: 0.7442 - val_f1: 0.3736\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.73619\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7700 - f1: 0.3170 - val_loss: 0.7402 - val_f1: 0.3769\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.73619\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-21.\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 408s 262ms/step - loss: 0.7711 - f1: 0.3165 - val_loss: 0.7408 - val_f1: 0.3747\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.73619\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7701 - f1: 0.3163 - val_loss: 0.7408 - val_f1: 0.3741\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.73619\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7701 - f1: 0.3164 - val_loss: 0.7403 - val_f1: 0.3753\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.73619\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-22.\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 408s 262ms/step - loss: 0.7715 - f1: 0.3155 - val_loss: 0.7407 - val_f1: 0.3755\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.73619\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7695 - f1: 0.3172 - val_loss: 0.7386 - val_f1: 0.3773\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.73619\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7684 - f1: 0.3182 - val_loss: 0.7397 - val_f1: 0.3770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6214 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00016: val_loss did not improve from 0.73619\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 9.999999682655225e-23.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6214/6214 [05:39<00:00, 18.28it/s]\n",
      "11702it [08:49, 22.09it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split data into train, valid\n",
    "indexes = np.arange(train_dataset_info.shape[0])\n",
    "# np.random.shuffle(indexes)\n",
    "# train_indexes, valid_indexes = train_test_split(indexes, test_size=0.15, random_state=8)\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "# train_generator = data_generator.create_train(\n",
    "#     train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "# validation_generator = data_generator.create_train(\n",
    "#     train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)\n",
    "\n",
    "oof_class_preds = np.zeros((train_dataset_info.shape[0], 28))\n",
    "sub_class_preds = np.zeros((submit.shape[0], 28))\n",
    "\n",
    "fold_ = 0\n",
    "epochs = 10; batch_size = 16\n",
    "for train_indexes, valid_indexes in kf.split(indexes):\n",
    "    \n",
    "    checkpoint = ModelCheckpoint('../cache/InceptionV3.h5', monitor='val_loss', verbose=1, \n",
    "                                 save_best_only=True, mode='min', save_weights_only = True)\n",
    "    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
    "                                       verbose=1, mode='auto', epsilon=0.0001)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", \n",
    "                          mode=\"min\", \n",
    "                          patience=15)\n",
    "    callbacks_list = [checkpoint, early, reduceLROnPlat]\n",
    "\n",
    "\n",
    "    _preds = []\n",
    "    # create train and valid datagens\n",
    "    train_generator = data_generator.create_train(\n",
    "        train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "    validation_generator = data_generator.create_train(\n",
    "        train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)\n",
    "\n",
    "    # warm up model\n",
    "    model = create_model(\n",
    "        input_shape=(SIZE,SIZE,3), \n",
    "        n_out=28)\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.layers[-1].trainable = True\n",
    "    model.layers[-2].trainable = True\n",
    "    model.layers[-3].trainable = True\n",
    "    model.layers[-4].trainable = True\n",
    "    model.layers[-5].trainable = True\n",
    "    model.layers[-6].trainable = True\n",
    "    \n",
    "    model.compile(\n",
    "        loss=f1_loss, \n",
    "        optimizer=Adam(1e-03),\n",
    "        metrics=[f1])\n",
    "#     model.summary()\n",
    "\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "        epochs=2, \n",
    "        verbose=1)\n",
    "    \n",
    "    # train all layers\n",
    "    epochs=120\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    model.compile(loss=f1_loss,\n",
    "                optimizer=Adam(lr=1e-4),\n",
    "                metrics=[f1])\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "        epochs=epochs, \n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list)\n",
    "    \n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "        epochs=epochs, \n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list)\n",
    "    \n",
    "    for idx in tqdm(valid_indexes):\n",
    "        item = train_dataset_info[idx]\n",
    "        path = item['path']\n",
    "        labels = item['labels']\n",
    "        image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "        score_predict = model.predict(image[np.newaxis])[0]\n",
    "        oof_class_preds[idx] = score_predict\n",
    "        np.save('../cache/oof_class_preds-13.npy', oof_class_preds)\n",
    "    \n",
    "    for idx, name in tqdm(enumerate(submit['Id'])):\n",
    "        path = os.path.join('../data/test/', name)\n",
    "        image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "        score_predict = model.predict(image[np.newaxis])[0]\n",
    "        sub_class_preds[idx] += score_predict\n",
    "        np.save('../cache/sub_class_preds-13.npy', sub_class_preds)\n",
    "    fold_ += 1\n",
    "sub_class_preds /= n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../cache/oof_class_preds-13.npy', oof_class_preds)\n",
    "np.save('../cache/sub_class_preds-13.npy', sub_class_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof_class_preds = np.zeros((train_dataset_info.shape[0], 28))\n",
    "# sub_class_preds = np.zeros((submit.shape[0], 28))\n",
    "# score_predict = model.predict(image[np.newaxis])[0]\n",
    "# oof_class_preds[idx] = score_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train_index, test_index in kf.split(indexes):\n",
    "# ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11702, 28)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_class_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 89289.54it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for line in tqdm(sub_class_preds):\n",
    "    label_predict = np.arange(28)[line>=0.2]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11702"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " '5 25',\n",
       " '0 5 25',\n",
       " '0 25',\n",
       " '7 25',\n",
       " '4',\n",
       " '4 25',\n",
       " '0 23 25',\n",
       " '0',\n",
       " '0 25',\n",
       " '17 18 19 25',\n",
       " '3 5',\n",
       " '0 2 25',\n",
       " '7 9 10 20',\n",
       " '23',\n",
       " '0 4 5 18 25',\n",
       " '2 14',\n",
       " '0 5',\n",
       " '14 21',\n",
       " '0 5',\n",
       " '6',\n",
       " '3 5 24',\n",
       " '0 2 11 25',\n",
       " '0',\n",
       " '0 4 25',\n",
       " '0 11 12 21 25 26',\n",
       " '0',\n",
       " '0',\n",
       " '0 25',\n",
       " '0',\n",
       " '0 21',\n",
       " '0 7 25',\n",
       " '14 16 17 18 21 25',\n",
       " '0 5 25',\n",
       " '0 7',\n",
       " '13',\n",
       " '0 13 25',\n",
       " '0 3',\n",
       " '0 21 25',\n",
       " '1',\n",
       " '0 16 17 21 25',\n",
       " '6 25',\n",
       " '0 21 25',\n",
       " '18 19 25',\n",
       " '0 16 25',\n",
       " '6',\n",
       " '0',\n",
       " '0 23',\n",
       " '6 11 23',\n",
       " '0',\n",
       " '0 16 17 25',\n",
       " '0 5',\n",
       " '8 20 23 24',\n",
       " '0 25',\n",
       " '3',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '11 23',\n",
       " '0 25',\n",
       " '21 25',\n",
       " '2 21 22',\n",
       " '0 5 21',\n",
       " '0 14 16 21',\n",
       " '7 21 25',\n",
       " '23',\n",
       " '0 18 19 25',\n",
       " '3 6 21 25',\n",
       " '0 25',\n",
       " '0 16 25',\n",
       " '21',\n",
       " '2 3',\n",
       " '0 2',\n",
       " '14',\n",
       " '4',\n",
       " '0 21',\n",
       " '0',\n",
       " '0 2 4',\n",
       " '0 1',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '6 23 25',\n",
       " '0 25',\n",
       " '0 21',\n",
       " '21 25',\n",
       " '17 18',\n",
       " '0 23 25',\n",
       " '20 23',\n",
       " '0 21',\n",
       " '14 16 25',\n",
       " '11 14',\n",
       " '0 25',\n",
       " '11 14',\n",
       " '23',\n",
       " '13',\n",
       " '0 16 17 25',\n",
       " '0 25',\n",
       " '7 17 18 25',\n",
       " '0 7 19 25',\n",
       " '24',\n",
       " '0 23 25',\n",
       " '0 25',\n",
       " '23',\n",
       " '21 23',\n",
       " '0 23',\n",
       " '6 7 25',\n",
       " '2 19 21 25',\n",
       " '0 14 16',\n",
       " '0 11 16 24',\n",
       " '20 26',\n",
       " '0 25',\n",
       " '0 2',\n",
       " '1',\n",
       " '16 17 18 25',\n",
       " '0 22 25',\n",
       " '21 25',\n",
       " '21 22 23',\n",
       " '0 2 25',\n",
       " '4 25',\n",
       " '6 14 16 17 25',\n",
       " '26',\n",
       " '0 18 19 25',\n",
       " '21 25',\n",
       " '2 6 21 25',\n",
       " '8 9 20',\n",
       " '0 2 4',\n",
       " '0',\n",
       " '0 25',\n",
       " '25',\n",
       " '0 4',\n",
       " '19',\n",
       " '25',\n",
       " '0 21',\n",
       " '0',\n",
       " '0 23',\n",
       " '0 11',\n",
       " '5',\n",
       " '0 14 16 25',\n",
       " '0',\n",
       " '0 5 21',\n",
       " '6 25',\n",
       " '0 19',\n",
       " '21 25',\n",
       " '0 1',\n",
       " '0 5',\n",
       " '0 25',\n",
       " '22 26',\n",
       " '0 16 21 25',\n",
       " '0 6 7 25',\n",
       " '0',\n",
       " '0 25',\n",
       " '0 3 23',\n",
       " '7 16 17 18',\n",
       " '0 7',\n",
       " '0 11 25',\n",
       " '6 7 25',\n",
       " '6',\n",
       " '0 7',\n",
       " '0 16 17 25',\n",
       " '1 21 25',\n",
       " '6 25',\n",
       " '0 19 25',\n",
       " '0 17 25',\n",
       " '0 11 19 25',\n",
       " '4 25',\n",
       " '5',\n",
       " '21 23',\n",
       " '0 19 25',\n",
       " '19',\n",
       " '14 21 25',\n",
       " '7 16 25',\n",
       " '5 25',\n",
       " '0 6 21 25',\n",
       " '0 14 16 25',\n",
       " '23',\n",
       " '0 2',\n",
       " '0 11',\n",
       " '21',\n",
       " '0 7 25',\n",
       " '0 12 21',\n",
       " '0 2 25',\n",
       " '0',\n",
       " '23',\n",
       " '0',\n",
       " '0 19',\n",
       " '14',\n",
       " '0 16 21 25',\n",
       " '0 16 25',\n",
       " '0 5 25',\n",
       " '23',\n",
       " '14 16 17 21 25',\n",
       " '14 17 21 25',\n",
       " '0 25',\n",
       " '5 25 26',\n",
       " '25 26',\n",
       " '0 5 25',\n",
       " '0 13 22',\n",
       " '0 25',\n",
       " '16 25',\n",
       " '23 25',\n",
       " '0 4 25',\n",
       " '2 21',\n",
       " '0 2 3',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '0 3 19 25',\n",
       " '21',\n",
       " '0 25',\n",
       " '0 5',\n",
       " '0',\n",
       " '21',\n",
       " '0 14 16',\n",
       " '0 5 22',\n",
       " '7',\n",
       " '0 5',\n",
       " '18 19',\n",
       " '0 21',\n",
       " '24 26',\n",
       " '7',\n",
       " '21 22',\n",
       " '2 3',\n",
       " '0 2 3 22',\n",
       " '14 17 21 25',\n",
       " '21 26',\n",
       " '2 3 12 21',\n",
       " '6 8 20 23',\n",
       " '0 23',\n",
       " '14 21',\n",
       " '12 25',\n",
       " '0 19',\n",
       " '0 5',\n",
       " '2 25',\n",
       " '0 11',\n",
       " '0 2',\n",
       " '23',\n",
       " '0 23 25',\n",
       " '11',\n",
       " '13 20 22 26',\n",
       " '0 12',\n",
       " '0 23',\n",
       " '16 17 18 25',\n",
       " '0 21',\n",
       " '2 7',\n",
       " '21',\n",
       " '0 25',\n",
       " '2 4 7 11 14 25',\n",
       " '12 23',\n",
       " '25',\n",
       " '4',\n",
       " '13 22',\n",
       " '0 1',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '2 6 23 25',\n",
       " '0 2 26',\n",
       " '0 25',\n",
       " '11',\n",
       " '23 25',\n",
       " '3 5',\n",
       " '2 6 25',\n",
       " '21 25',\n",
       " '14 16 17 18 25',\n",
       " '19',\n",
       " '0 4 5 26',\n",
       " '0 2',\n",
       " '13 21 22',\n",
       " '0',\n",
       " '12',\n",
       " '0 2 5',\n",
       " '0 23 25',\n",
       " '0 25',\n",
       " '0 19',\n",
       " '0 21 25',\n",
       " '0 12 21',\n",
       " '0 2 25',\n",
       " '2',\n",
       " '0 18 19 25',\n",
       " '0 2 18 19',\n",
       " '11',\n",
       " '0 25',\n",
       " '0 18 25',\n",
       " '0 25',\n",
       " '2',\n",
       " '0 3',\n",
       " '8 9 10 20 26',\n",
       " '0 19',\n",
       " '0 12 21',\n",
       " '0 2',\n",
       " '7',\n",
       " '1 2 6',\n",
       " '7 11 24',\n",
       " '0 5 7',\n",
       " '0 14 16',\n",
       " '0 18 21 25',\n",
       " '7',\n",
       " '0 19',\n",
       " '0 21 25',\n",
       " '25',\n",
       " '0 2 25',\n",
       " '0 5 19',\n",
       " '0 2 21 25',\n",
       " '26',\n",
       " '0 5 19',\n",
       " '11 25',\n",
       " '0 14 16',\n",
       " '25',\n",
       " '0',\n",
       " '1',\n",
       " '14 16 17 25',\n",
       " '0 21',\n",
       " '0',\n",
       " '11 23 25',\n",
       " '11',\n",
       " '0 7 25',\n",
       " '0',\n",
       " '0 25',\n",
       " '21 25',\n",
       " '14 16 17 18 25',\n",
       " '0',\n",
       " '0 25',\n",
       " '0 2 17 25',\n",
       " '19',\n",
       " '6 25',\n",
       " '0 25',\n",
       " '0 2',\n",
       " '0',\n",
       " '0 2',\n",
       " '21 22',\n",
       " '0 2',\n",
       " '0 2 3',\n",
       " '14',\n",
       " '5',\n",
       " '3 25',\n",
       " '7 25',\n",
       " '0 1 21',\n",
       " '11 21 25',\n",
       " '7',\n",
       " '24',\n",
       " '14 16 17 25',\n",
       " '0 25',\n",
       " '1 6',\n",
       " '0 21 25',\n",
       " '0 7 18',\n",
       " '0 2 4',\n",
       " '14 16 17',\n",
       " '4 12 21 25',\n",
       " '0 2 3 25',\n",
       " '18 19',\n",
       " '2 3 7',\n",
       " '0 2 6 21 25',\n",
       " '0 25',\n",
       " '0 7',\n",
       " '9 10 20 26',\n",
       " '25',\n",
       " '5 19',\n",
       " '0 2 16 25',\n",
       " '3',\n",
       " '0 7 24',\n",
       " '0 5 21',\n",
       " '25',\n",
       " '2 4 23',\n",
       " '11 24 25',\n",
       " '2 6 25',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '7 25',\n",
       " '11',\n",
       " '11',\n",
       " '0',\n",
       " '0 5 19 25',\n",
       " '0 5',\n",
       " '11 14 16 17 18 25',\n",
       " '0 1 25',\n",
       " '0 21 25',\n",
       " '21 25',\n",
       " '0 23 25',\n",
       " '0 21',\n",
       " '0 3',\n",
       " '0 3 25',\n",
       " '12 21',\n",
       " '4 13 21 25',\n",
       " '0 21',\n",
       " '7',\n",
       " '0',\n",
       " '2 14 21',\n",
       " '0 4 5',\n",
       " '14 16 17',\n",
       " '0',\n",
       " '2 3',\n",
       " '18 19',\n",
       " '0 25',\n",
       " '16 17 18 25',\n",
       " '0 25',\n",
       " '0 14',\n",
       " '0 21 25',\n",
       " '0 5 19',\n",
       " '0',\n",
       " '0 21',\n",
       " '0 11 25',\n",
       " '14 16 17 21 25',\n",
       " '0',\n",
       " '13 21 23 25',\n",
       " '0 5',\n",
       " '21 23 25',\n",
       " '0 14 16 21',\n",
       " '0 22 25',\n",
       " '0 2 23',\n",
       " '0 23',\n",
       " '19',\n",
       " '0 6 21 25',\n",
       " '1 11',\n",
       " '6 25',\n",
       " '0 3',\n",
       " '23',\n",
       " '7',\n",
       " '0 25',\n",
       " '0',\n",
       " '21 25',\n",
       " '6 25',\n",
       " '0 21 25',\n",
       " '0 7 24',\n",
       " '0 7 25',\n",
       " '0 2 5',\n",
       " '17 18 21',\n",
       " '0 3 4 25',\n",
       " '0 25',\n",
       " '0 7 25',\n",
       " '18 25',\n",
       " '0 18 19',\n",
       " '0 3 24',\n",
       " '4 21 25',\n",
       " '0 12 21 25',\n",
       " '0 23 25',\n",
       " '23',\n",
       " '0 14 16',\n",
       " '18 25',\n",
       " '0 23 25',\n",
       " '0 4',\n",
       " '0 21 25',\n",
       " '2 4',\n",
       " '7',\n",
       " '1 14 16 17',\n",
       " '21 25',\n",
       " '11 25',\n",
       " '0 2',\n",
       " '0 25',\n",
       " '0 13 22 25 26',\n",
       " '0 19',\n",
       " '18 21',\n",
       " '0 14 16 17 18 25',\n",
       " '0 17 18 21',\n",
       " '12 21',\n",
       " '0 2',\n",
       " '0 4 21 25',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0 3 5 25',\n",
       " '25',\n",
       " '6 11',\n",
       " '4',\n",
       " '0 25',\n",
       " '2 21 25',\n",
       " '0 2',\n",
       " '0 2 25',\n",
       " '7 9 20',\n",
       " '21',\n",
       " '23',\n",
       " '0 23',\n",
       " '0 2 11',\n",
       " '7 11',\n",
       " '3',\n",
       " '18 23 25',\n",
       " '0 2',\n",
       " '0 25',\n",
       " '14 16 17 25',\n",
       " '3 4',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '6 23 25',\n",
       " '4',\n",
       " '14 16',\n",
       " '0 21 22 25',\n",
       " '4',\n",
       " '0',\n",
       " '0 22',\n",
       " '0 21 25',\n",
       " '5 25',\n",
       " '0 13',\n",
       " '4',\n",
       " '0 1',\n",
       " '0 25',\n",
       " '21 25',\n",
       " '0 25',\n",
       " '2 25',\n",
       " '0 25',\n",
       " '7 20',\n",
       " '21 25',\n",
       " '0 7',\n",
       " '0 23',\n",
       " '0 19 23',\n",
       " '16 17 25',\n",
       " '2',\n",
       " '0',\n",
       " '21 25',\n",
       " '21',\n",
       " '0',\n",
       " '0 21 25',\n",
       " '2 7 25',\n",
       " '0 20 25 26',\n",
       " '0 7',\n",
       " '0 21 25',\n",
       " '0 7 21 25',\n",
       " '12 13',\n",
       " '0 2 25',\n",
       " '0 13 21 22 25',\n",
       " '21 25',\n",
       " '7',\n",
       " '0',\n",
       " '16 19 25 26',\n",
       " '3',\n",
       " '0 19 25',\n",
       " '0 3 25',\n",
       " '1 25',\n",
       " '0 11 25',\n",
       " '12',\n",
       " '19 25',\n",
       " '0 4',\n",
       " '7',\n",
       " '0 25',\n",
       " '23',\n",
       " '18 19',\n",
       " '0 21 25',\n",
       " '0 2 16 17 25',\n",
       " '0 16 17 18 21',\n",
       " '4 25',\n",
       " '0 14 16',\n",
       " '2 7 25',\n",
       " '19',\n",
       " '0 25',\n",
       " '18 19',\n",
       " '12',\n",
       " '0 5 21',\n",
       " '5 26',\n",
       " '0 7',\n",
       " '0 16',\n",
       " '7 11',\n",
       " '4',\n",
       " '0',\n",
       " '2 7 21 25',\n",
       " '2',\n",
       " '23',\n",
       " '14',\n",
       " '0 2 25',\n",
       " '0 11 12 25',\n",
       " '2 7 21 25',\n",
       " '21',\n",
       " '21 25',\n",
       " '0 25',\n",
       " '0 17 21 25',\n",
       " '0 25',\n",
       " '7',\n",
       " '0',\n",
       " '21 23',\n",
       " '0 2 7',\n",
       " '0 18 25',\n",
       " '0 21 25',\n",
       " '2 21',\n",
       " '0 25',\n",
       " '6 25',\n",
       " '0 23 25',\n",
       " '18 21',\n",
       " '0 17 25',\n",
       " '14 16 17 25',\n",
       " '0 5 7 18 19',\n",
       " '7 21 25',\n",
       " '18 24',\n",
       " '0 16 17 25',\n",
       " '7',\n",
       " '0',\n",
       " '0 21 25',\n",
       " '0 25',\n",
       " '20 25 26',\n",
       " '11 25',\n",
       " '5 18 25',\n",
       " '21',\n",
       " '0 25',\n",
       " '21',\n",
       " '4 21',\n",
       " '23',\n",
       " '21',\n",
       " '0 7 19',\n",
       " '5 23',\n",
       " '0 5 13 22',\n",
       " '6 25',\n",
       " '21 22 25',\n",
       " '0 25',\n",
       " '5 19 25',\n",
       " '2 3',\n",
       " '2 14 16',\n",
       " '6 11 14',\n",
       " '0 25',\n",
       " '1 2',\n",
       " '23',\n",
       " '0 25',\n",
       " '25',\n",
       " '0 21',\n",
       " '0',\n",
       " '18 19 25',\n",
       " '0 4 7',\n",
       " '0 12 21',\n",
       " '0 25',\n",
       " '7 25',\n",
       " '14',\n",
       " '12 14 21',\n",
       " '5 19',\n",
       " '5 21',\n",
       " '23',\n",
       " '1 25',\n",
       " '0 25',\n",
       " '7',\n",
       " '0 21 25',\n",
       " '3 5',\n",
       " '0 12',\n",
       " '14',\n",
       " '23 25',\n",
       " '0 21',\n",
       " '5 21 22',\n",
       " '21',\n",
       " '0 25',\n",
       " '25',\n",
       " '0 2 12 21',\n",
       " '0 25',\n",
       " '0 7 21',\n",
       " '0 12 25',\n",
       " '3 5 25',\n",
       " '14 16',\n",
       " '0 7 18 21',\n",
       " '0 19',\n",
       " '5',\n",
       " '7',\n",
       " '21 22',\n",
       " '13',\n",
       " '0',\n",
       " '23 25',\n",
       " '0 7',\n",
       " '11 12 21 25',\n",
       " '0 1 25',\n",
       " '23 25',\n",
       " '0 5 25',\n",
       " '25 26',\n",
       " '21 25',\n",
       " '11 14 21 25',\n",
       " '0 21 25',\n",
       " '0 2 3 7',\n",
       " '0 2',\n",
       " '5 21',\n",
       " '0 21 23 25',\n",
       " '1 4',\n",
       " '0 1 18 19 21 25',\n",
       " '0 25',\n",
       " '0 19',\n",
       " '0 2 5',\n",
       " '0 18 19',\n",
       " '0 16 22 25',\n",
       " '0 25',\n",
       " '2 4',\n",
       " '7',\n",
       " '5 25',\n",
       " '5 25',\n",
       " '21 22',\n",
       " '14 16 17 25',\n",
       " '0 21 22',\n",
       " '0 2 25',\n",
       " '7',\n",
       " '5 25',\n",
       " '18 19',\n",
       " '0 1 25',\n",
       " '0 21',\n",
       " '19 26',\n",
       " '4 18 19 25',\n",
       " '23 25',\n",
       " '1 2 25',\n",
       " '0 14 16',\n",
       " '22',\n",
       " '0 6 21 25',\n",
       " '0 25',\n",
       " '0 22',\n",
       " '0 21',\n",
       " '0 25',\n",
       " '0 16 17 25',\n",
       " '0 25',\n",
       " '23',\n",
       " '0 2 5 21',\n",
       " '4',\n",
       " '13',\n",
       " '0 25',\n",
       " '21 25',\n",
       " '0 12 23 25',\n",
       " '0 1 21 25',\n",
       " '0 5 14 21 25',\n",
       " '5',\n",
       " '0',\n",
       " '21 23',\n",
       " '14 16',\n",
       " '0 2 11 25',\n",
       " '5 25',\n",
       " '11',\n",
       " '0 7',\n",
       " '0 21',\n",
       " '3',\n",
       " '0 21',\n",
       " '0 25',\n",
       " '21 25',\n",
       " '1 6 21 25',\n",
       " '7 25',\n",
       " '0 21 25',\n",
       " '11 12 21 22 24',\n",
       " '0 2',\n",
       " '2 3 4',\n",
       " '0 21',\n",
       " '0 2 18',\n",
       " '21',\n",
       " '25',\n",
       " '0 7 25',\n",
       " '19',\n",
       " '0 3',\n",
       " '0 2 25',\n",
       " '0 23',\n",
       " '0 1 2',\n",
       " '0',\n",
       " '0 25',\n",
       " '0 13 22 25',\n",
       " '0 5',\n",
       " '0 17 18 21 25',\n",
       " '0 2 7',\n",
       " '0 1 2',\n",
       " '0 7',\n",
       " '0 21 25',\n",
       " '0 25',\n",
       " '6 23',\n",
       " '7 13 20 25',\n",
       " '0 3 21 25',\n",
       " '23 25',\n",
       " '23 25',\n",
       " '21',\n",
       " '21 24 25',\n",
       " '0 13',\n",
       " '0 21 25',\n",
       " '25',\n",
       " '0 21 25',\n",
       " '0',\n",
       " '23',\n",
       " '0 2 17 21 25',\n",
       " '0 1 2',\n",
       " '0 25',\n",
       " '0 5',\n",
       " '6 21 25',\n",
       " '0 25',\n",
       " '0 25 26',\n",
       " '23',\n",
       " '13 21 25',\n",
       " '23 25',\n",
       " '0 2 21 25',\n",
       " '14 16 25',\n",
       " '5 7',\n",
       " '12 14 16 21 25',\n",
       " '0 7 21 25',\n",
       " '18 19 21',\n",
       " '7',\n",
       " '7 11',\n",
       " '7',\n",
       " '18 19',\n",
       " '23',\n",
       " '0 5 21 25',\n",
       " '25',\n",
       " '0',\n",
       " '0 14 16 17 25',\n",
       " '6 25',\n",
       " '0 2 16',\n",
       " '21 25',\n",
       " '0 25',\n",
       " '11',\n",
       " '14 16 17 25',\n",
       " '0 16 19 21 22',\n",
       " '14 16 17 25',\n",
       " '14 25',\n",
       " '0 18 19 25',\n",
       " '6 11 23',\n",
       " '5',\n",
       " '2 7',\n",
       " '7 23',\n",
       " '0',\n",
       " '0 1',\n",
       " '0 20 26',\n",
       " '0 25',\n",
       " '25',\n",
       " '23 25',\n",
       " '0 25',\n",
       " '21 22 25',\n",
       " '0 2 3 25',\n",
       " '0 18 19 25',\n",
       " '12',\n",
       " '0 21',\n",
       " '0 25',\n",
       " '0 4 21',\n",
       " '0 18 21',\n",
       " '23',\n",
       " '4 18 25',\n",
       " '0 25',\n",
       " '0 16 23',\n",
       " '1 23',\n",
       " '11',\n",
       " '2 3',\n",
       " '0 21 25',\n",
       " '0 7',\n",
       " '5 25',\n",
       " '0 25',\n",
       " '14 16 17 18 21',\n",
       " '0 25',\n",
       " '0 21 25',\n",
       " '21 25',\n",
       " '0 13 14 16 25',\n",
       " '0 25',\n",
       " '25',\n",
       " '0 21 25',\n",
       " '0 11 25',\n",
       " '25',\n",
       " '25',\n",
       " '25',\n",
       " '0 21',\n",
       " '21 25',\n",
       " '6 21 25',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '0 1',\n",
       " '0 25',\n",
       " '3',\n",
       " '0',\n",
       " '0 5',\n",
       " '0 2 21',\n",
       " '0 25',\n",
       " '0 5',\n",
       " '0',\n",
       " '0 1 3 5',\n",
       " '7 9 10',\n",
       " '0 26',\n",
       " '0 25',\n",
       " '0 16 17 21 25',\n",
       " '19 21 25',\n",
       " '12 21',\n",
       " '0 25',\n",
       " '0 21 25',\n",
       " '14 16 17',\n",
       " '14 17 21 25',\n",
       " '0 21 25',\n",
       " '0 19',\n",
       " '6 25',\n",
       " '0 2',\n",
       " '21',\n",
       " '0 18 19',\n",
       " '21',\n",
       " '0 21 25',\n",
       " '0 2 18',\n",
       " '0 1 16',\n",
       " '18 19 25',\n",
       " '0 25',\n",
       " '0 5 12 21',\n",
       " '0 25',\n",
       " '0 11 14 16 17',\n",
       " '0 5 22 25',\n",
       " '11 24',\n",
       " '5 24 25',\n",
       " '0 2 3 5',\n",
       " '0 5',\n",
       " '6 11 25',\n",
       " '0',\n",
       " '0',\n",
       " '21',\n",
       " '23',\n",
       " '6',\n",
       " '0 23 25',\n",
       " '0 2 5 21 25',\n",
       " '11 21',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '21',\n",
       " '0 23',\n",
       " '0 23 25',\n",
       " '0 25',\n",
       " '6 23',\n",
       " '0 2 4 18 25',\n",
       " '5',\n",
       " '23',\n",
       " '6 12 21 25',\n",
       " '2 7',\n",
       " '0 1 7',\n",
       " '6',\n",
       " '20 26',\n",
       " '0 2 5 25',\n",
       " '0 25',\n",
       " '0 7',\n",
       " '21 25',\n",
       " '0 13 21 22',\n",
       " '23',\n",
       " '6 25',\n",
       " '0 14 17 25',\n",
       " '4',\n",
       " '0 25',\n",
       " '0 5',\n",
       " '19 21',\n",
       " '0 3',\n",
       " '18 19 25',\n",
       " '0 3 25',\n",
       " '0 5',\n",
       " '26',\n",
       " '0 7',\n",
       " '14',\n",
       " '21 25',\n",
       " '7',\n",
       " '0 2 25',\n",
       " '7',\n",
       " '23 25',\n",
       " '0',\n",
       " '0 23 25',\n",
       " '0 21 25',\n",
       " '0 3 5',\n",
       " '0 2 21 25',\n",
       " '0 21 25',\n",
       " '2 7 25',\n",
       " '0 25',\n",
       " '0 6 25',\n",
       " '11 19 25',\n",
       " '0 4',\n",
       " '4',\n",
       " '17 18 21 25',\n",
       " '14 16',\n",
       " '0 21',\n",
       " '0 13 21 22 25',\n",
       " '19',\n",
       " '23',\n",
       " '6 21 25',\n",
       " '0 21 25',\n",
       " '21 22',\n",
       " '0 21',\n",
       " '1 21 25',\n",
       " '17 25',\n",
       " '0 2 16',\n",
       " '7',\n",
       " '0 1 25',\n",
       " '2 21 25',\n",
       " '2 26',\n",
       " '0 25',\n",
       " '14 16 17 25',\n",
       " '0 2 3',\n",
       " '0 17 25',\n",
       " '4 21 25',\n",
       " '4 21',\n",
       " '0 25',\n",
       " '0 19 25',\n",
       " '0',\n",
       " '0 18 19',\n",
       " '0 11 25',\n",
       " '25',\n",
       " '7 16 18 19',\n",
       " '12',\n",
       " '7 23',\n",
       " '0 23',\n",
       " '0 25',\n",
       " '0 7',\n",
       " '7',\n",
       " '21 25',\n",
       " '0 11 12 14 25',\n",
       " '0 4 5 25',\n",
       " '0',\n",
       " '0 5 22',\n",
       " '0',\n",
       " '0',\n",
       " '0 16 17 18 25',\n",
       " '0 2 25',\n",
       " '0 21 25',\n",
       " '11 21',\n",
       " '0 7 14 16 21 25',\n",
       " '6',\n",
       " '3',\n",
       " '6 21 25',\n",
       " '0 2 3 5 23',\n",
       " '3',\n",
       " '5 7 18 25',\n",
       " '23',\n",
       " '5',\n",
       " '0 25',\n",
       " '0 2',\n",
       " '0 1 19',\n",
       " '0 2',\n",
       " '0 2',\n",
       " '5 25',\n",
       " '0 25',\n",
       " '0 21 25',\n",
       " '0 25',\n",
       " '23 25',\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['Predicted'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submit\n",
    "# submit = pd.read_csv('../data/sample_submission.csv')\n",
    "# predicted = []\n",
    "# draw_predict = []\n",
    "# # model.load_weights('../cache/InceptionV3.h5')\n",
    "# for name in tqdm(submit['Id']):\n",
    "#     path = os.path.join('../data/test/', name)\n",
    "#     image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "#     score_predict = model.predict(image[np.newaxis])[0]\n",
    "#     draw_predict.append(score_predict)\n",
    "#     label_predict = np.arange(28)[score_predict>=0.2]\n",
    "#     str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "#     predicted.append(str_predict_label)\n",
    "\n",
    "# submit['Predicted'] = predicted\n",
    "# np.save('../cache/draw_predict_InceptionV3-8.npy', score_predict)\n",
    "# submit.to_csv('../submissions/submit_InceptionV3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('../submissions/sub13-a.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/1855095/how-to-create-a-zip-archive-of-a-directory\n",
    "def backup_project_as_zip(project_dir, zip_file):\n",
    "    assert(os.path.isdir(project_dir))\n",
    "    assert(os.path.isdir(os.path.dirname(zip_file)))\n",
    "    shutil.make_archive(zip_file.replace('.zip',''), 'zip', project_dir)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-30 00:48:13.957997\n"
     ]
    }
   ],
   "source": [
    "import datetime, shutil\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "PROJECT_PATH = '/home/watts/lal/Kaggle/kagglehp/scripts_nbs'\n",
    "backup_project_as_zip(PROJECT_PATH, '../cache/code.scripts_nbs.%s.zip'%now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mCache entry deserialization failed, entry ignored\u001b[0m\n",
      "Collecting kaggle\n",
      "  Downloading https://files.pythonhosted.org/packages/83/9b/ac57e15fbb239c6793c8d0b7dfd1a4c4a025eaa9f791b5388a7afb515aed/kaggle-1.5.0.tar.gz (53kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 183kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: urllib3<1.23.0,>=1.15 in /home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages (from kaggle)\n",
      "Requirement already up-to-date: six>=1.10 in /home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages (from kaggle)\n",
      "Collecting certifi (from kaggle)\n",
      "  Downloading https://files.pythonhosted.org/packages/56/9d/1d02dd80bc4cd955f98980f28c5ee2200e1209292d5f9e9cc8d030d18655/certifi-2018.10.15-py2.py3-none-any.whl (146kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 545kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil (from kaggle)\n",
      "  Downloading https://files.pythonhosted.org/packages/74/68/d87d9b36af36f44254a8d512cbfc48369103a3b9e474be9bdfe536abfc45/python_dateutil-2.7.5-py2.py3-none-any.whl (225kB)\n",
      "\u001b[K    100% |████████████████████████████████| 235kB 1.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting requests (from kaggle)\n",
      "  Downloading https://files.pythonhosted.org/packages/f1/ca/10332a30cb25b627192b4ea272c351bce3ca1091e541245cccbace6051d8/requests-2.20.0-py2.py3-none-any.whl (60kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 764kB/s ta 0:00:01\n",
      "\u001b[33mCache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from kaggle)\n",
      "  Downloading https://files.pythonhosted.org/packages/91/55/8cb23a97301b177e9c8e3226dba45bb454411de2cbd25746763267f226c2/tqdm-4.28.1-py2.py3-none-any.whl (45kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 917kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting python-slugify (from kaggle)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/00/ad/c778a6df614b6217c30fe80045b365bfa08b5dd3cb02e8b37a6d25126781/python-slugify-1.2.6.tar.gz\n",
      "Collecting idna<2.8,>=2.5 (from requests->kaggle)\n",
      "  Using cached https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl\n",
      "Requirement already up-to-date: chardet<3.1.0,>=3.0.2 in /home/watts/anaconda3/envs/hpg/lib/python3.6/site-packages (from requests->kaggle)\n",
      "Collecting Unidecode>=0.04.16 (from python-slugify->kaggle)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
      "\u001b[K    100% |████████████████████████████████| 235kB 555kB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: kaggle, python-slugify\n",
      "  Running setup.py bdist_wheel for kaggle ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/watts/.cache/pip/wheels/8b/21/3b/a0076243c6ae12a6215b2da515fe06b539aee7217b406e510e\n",
      "  Running setup.py bdist_wheel for python-slugify ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/watts/.cache/pip/wheels/e3/65/da/2045deea3098ed7471eca0e2460cfbd3fdfe8c1d6fa6fcac92\n",
      "Successfully built kaggle python-slugify\n",
      "Installing collected packages: certifi, python-dateutil, idna, requests, tqdm, Unidecode, python-slugify, kaggle\n",
      "  Found existing installation: certifi 2018.4.16\n",
      "    Uninstalling certifi-2018.4.16:\n",
      "      Successfully uninstalled certifi-2018.4.16\n",
      "  Found existing installation: python-dateutil 2.7.3\n",
      "    Uninstalling python-dateutil-2.7.3:\n",
      "      Successfully uninstalled python-dateutil-2.7.3\n",
      "  Found existing installation: idna 2.6\n",
      "    Uninstalling idna-2.6:\n",
      "      Successfully uninstalled idna-2.6\n",
      "  Found existing installation: requests 2.18.4\n",
      "    Uninstalling requests-2.18.4:\n",
      "      Successfully uninstalled requests-2.18.4\n",
      "  Found existing installation: tqdm 4.22.0\n",
      "    Uninstalling tqdm-4.22.0:\n",
      "      Successfully uninstalled tqdm-4.22.0\n",
      "  Found existing installation: kaggle 1.3.8\n",
      "    Uninstalling kaggle-1.3.8:\n",
      "      Successfully uninstalled kaggle-1.3.8\n",
      "Successfully installed Unidecode-1.0.22 certifi-2018.10.15 idna-2.7 kaggle-1.5.0 python-dateutil-2.7.5 python-slugify-1.2.6 requests-2.20.0 tqdm-4.28.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 492k/492k [00:13<00:00, 37.3kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 349 ms, sys: 224 ms, total: 572 ms\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub13-a.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub13-a.csv   2018-10-29 19:20:40               complete  0.444        None          \r\n",
      "sub12-d.csv   2018-10-26 02:09:32               complete  0.466        None          \r\n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \r\n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \r\n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \r\n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \r\n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \r\n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \r\n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \r\n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \r\n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \r\n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \r\n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \r\n",
      "sub11-g.csv   2018-10-23 09:13:19               complete  0.347        None          \r\n",
      "sub11-f.csv   2018-10-23 09:11:15               complete  0.358        None          \r\n",
      "sub11-b.csv   2018-10-23 05:25:32               complete  0.437        None          \r\n",
      "sub12.csv     2018-10-23 05:18:36               complete  0.436        None          \r\n",
      "sub11.csv     2018-10-23 01:07:18               complete  0.431        None          \r\n",
      "sub10.csv     2018-10-22 17:16:40               complete  0.336        None          \r\n",
      "sub9.csv      2018-10-21 20:04:09               complete  0.098        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 90436.26it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for line in tqdm(sub_class_preds):\n",
    "    label_predict = np.arange(28)[line>=0.25]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub13-b.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-30 00:51:23.536802\n"
     ]
    }
   ],
   "source": [
    "import datetime, shutil\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "PROJECT_PATH = '/home/watts/lal/Kaggle/kagglehp/scripts_nbs'\n",
    "backup_project_as_zip(PROJECT_PATH, '../cache/code.scripts_nbs.%s.zip'%now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 486k/486k [00:13<00:00, 37.7kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 349 ms, sys: 199 ms, total: 548 ms\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub13-b.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub13-b.csv   2018-10-29 19:21:53               complete  0.448        None          \r\n",
      "sub13-a.csv   2018-10-29 19:20:40               complete  0.444        None          \r\n",
      "sub12-d.csv   2018-10-26 02:09:32               complete  0.466        None          \r\n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \r\n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \r\n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \r\n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \r\n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \r\n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \r\n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \r\n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \r\n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \r\n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \r\n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \r\n",
      "sub11-g.csv   2018-10-23 09:13:19               complete  0.347        None          \r\n",
      "sub11-f.csv   2018-10-23 09:11:15               complete  0.358        None          \r\n",
      "sub11-b.csv   2018-10-23 05:25:32               complete  0.437        None          \r\n",
      "sub12.csv     2018-10-23 05:18:36               complete  0.436        None          \r\n",
      "sub11.csv     2018-10-23 01:07:18               complete  0.431        None          \r\n",
      "sub10.csv     2018-10-22 17:16:40               complete  0.336        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {0.3:'bb', 0.35:'c', 0.4:'d', 0.45:'e', 0.5:'f'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 96444.28it/s]\n",
      "100%|██████████| 11702/11702 [00:00<00:00, 107148.54it/s]\n",
      " 99%|█████████▉| 11610/11702 [00:00<00:00, 116077.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub13-bb.csv\n",
      "../submissions/sub13-c.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 114139.61it/s]\n",
      "100%|██████████| 11702/11702 [00:00<00:00, 109594.41it/s]\n",
      "  0%|          | 0/11702 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub13-d.csv\n",
      "../submissions/sub13-e.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 11702/11702 [00:00<00:00, 117229.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub13-f.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for alpha in [0.3, 0.35, 0.4, 0.45, 0.5]:\n",
    "    predicted = []\n",
    "    for line in tqdm(sub_class_preds):\n",
    "        label_predict = np.arange(28)[line>=alpha]\n",
    "        str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "        predicted.append(str_predict_label)\n",
    "    submit['Predicted'] = predicted\n",
    "    name = '../submissions/sub13-' + d[alpha] + '.csv'\n",
    "    print(name)\n",
    "    submit.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 481k/481k [00:16<00:00, 30.2kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 427 ms, sys: 214 ms, total: 641 ms\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub13-bb.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub13-bb.csv  2018-10-29 19:23:01               complete  0.454        None          \r\n",
      "sub13-b.csv   2018-10-29 19:21:53               complete  0.448        None          \r\n",
      "sub13-a.csv   2018-10-29 19:20:40               complete  0.444        None          \r\n",
      "sub12-d.csv   2018-10-26 02:09:32               complete  0.466        None          \r\n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \r\n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \r\n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \r\n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \r\n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \r\n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \r\n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \r\n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \r\n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \r\n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \r\n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \r\n",
      "sub11-g.csv   2018-10-23 09:13:19               complete  0.347        None          \r\n",
      "sub11-f.csv   2018-10-23 09:11:15               complete  0.358        None          \r\n",
      "sub11-b.csv   2018-10-23 05:25:32               complete  0.437        None          \r\n",
      "sub12.csv     2018-10-23 05:18:36               complete  0.436        None          \r\n",
      "sub11.csv     2018-10-23 01:07:18               complete  0.431        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 477k/477k [00:08<00:00, 55.1kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationfileName      date                 description  status    publicScore  privateScore  \n",
      "------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub13-c.csv   2018-10-29 19:24:00               complete  0.459        None          \n",
      "sub13-bb.csv  2018-10-29 19:23:01               complete  0.454        None          \n",
      "sub13-b.csv   2018-10-29 19:21:53               complete  0.448        None          \n",
      "sub13-a.csv   2018-10-29 19:20:40               complete  0.444        None          \n",
      "sub12-d.csv   2018-10-26 02:09:32               complete  0.466        None          \n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \n",
      "sub11-g.csv   2018-10-23 09:13:19               complete  0.347        None          \n",
      "sub11-f.csv   2018-10-23 09:11:15               complete  0.358        None          \n",
      "sub11-b.csv   2018-10-23 05:25:32               complete  0.437        None          \n",
      "sub12.csv     2018-10-23 05:18:36               complete  0.436        None          \n",
      "CPU times: user 420 ms, sys: 259 ms, total: 679 ms\n",
      "Wall time: 28.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub13-c.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpg",
   "language": "python",
   "name": "hpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

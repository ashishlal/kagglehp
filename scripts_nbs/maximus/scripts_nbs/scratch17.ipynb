{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/mathormad/inceptionv3-baseline-lb-0-379/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) + (1 - K.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "path_to_train = '../data/train/'\n",
    "data = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_info = []\n",
    "for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "    train_dataset_info.append({\n",
    "        'path':os.path.join(path_to_train, name),\n",
    "        'labels':np.array([int(label) for label in labels])})\n",
    "train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'path': '../data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0', 'labels': array([16,  0])},\n",
       "       {'path': '../data/train/000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', 'labels': array([7, 1, 2, 0])},\n",
       "       {'path': '../data/train/000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([5])},\n",
       "       ...,\n",
       "       {'path': '../data/train/fff189d8-bbab-11e8-b2ba-ac1f6b6435d0', 'labels': array([7])},\n",
       "       {'path': '../data/train/fffdf7e0-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([25,  2, 21])},\n",
       "       {'path': '../data/train/fffe0ffe-bbc0-11e8-b2bb-ac1f6b6435d0', 'labels': array([2, 0])}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            dataset_info = shuffle(dataset_info)\n",
    "            for start in range(0, len(dataset_info), batch_size):\n",
    "                end = min(start + batch_size, len(dataset_info))\n",
    "                batch_images = []\n",
    "                X_train_batch = dataset_info[start:end]\n",
    "                batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "                for i in range(len(X_train_batch)):\n",
    "                    image = data_generator.load_image(\n",
    "                        X_train_batch[i]['path'], shape)   \n",
    "                    if augument:\n",
    "                        image = data_generator.augment(image)\n",
    "                    batch_images.append(image/255.)\n",
    "                    batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "                yield np.array(batch_images, np.float32), batch_labels\n",
    "\n",
    "    def load_image(path, shape):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image = np.stack((\n",
    "            np.array(image_red_ch),\n",
    "            np.array(image_green_ch), \n",
    "            np.array(image_blue_ch)), -1)\n",
    "        w, h = 512, 512\n",
    "        zero_data = np.zeros((h, w), dtype=np.uint8)\n",
    "#         image2 = np.stack((\n",
    "#             np.array(image_yellow_ch),\n",
    "#             zero_data, zero_data), -1)\n",
    "#         print(image1.shape, image2.shape)\n",
    "#         image = np.vstack((image1, image2))\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "\n",
    "    def augment(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=0),\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "            ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=input_shape)\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = base_model(bn)\n",
    "    x = Conv2D(32, kernel_size=(1,1), activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "# https://github.com/ailias/Focal-Loss-implement-on-Tensorflow/blob/master/focal_loss.py\n",
    "def focal_loss_org(prediction_tensor, target_tensor, weights=None, alpha=0.25, gamma=2):\n",
    "    r\"\"\"Compute focal loss for predictions.\n",
    "        Multi-labels Focal loss formula:\n",
    "            FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                 ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "    Args:\n",
    "     prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing the predicted logits for each class\n",
    "     target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing one-hot encoded classification targets\n",
    "     weights: A float tensor of shape [batch_size, num_anchors]\n",
    "     alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "     gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "    Returns:\n",
    "        loss: A (scalar) tensor representing the value of the loss function\n",
    "    \"\"\"\n",
    "    sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                          - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(weights=None, alpha=0.25, gamma=2):\n",
    "    def focal_loss_my(target_tensor, prediction_tensor, ):\n",
    "        r\"\"\"Compute focal loss for predictions.\n",
    "            Multi-labels Focal loss formula:\n",
    "                FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                     ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "        Args:\n",
    "         prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "            num_classes] representing the predicted logits for each class\n",
    "         target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "            num_classes] representing one-hot encoded classification targets\n",
    "         weights: A float tensor of shape [batch_size, num_anchors]\n",
    "         alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "         gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "        Returns:\n",
    "            loss: A (scalar) tensor representing the value of the loss function\n",
    "        \"\"\"\n",
    "        sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "        zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "\n",
    "        # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "        # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "        pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "\n",
    "        # For negative prediction, only need consider back part loss, front part is 0;\n",
    "        # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "        neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "        per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                              - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "        return tf.reduce_sum(per_entry_cross_ent)\n",
    "#         return K.mean(K.binary_crossentropy(target_tensor, prediction_tensor), axis=-1) + tf.reduce_sum(per_entry_cross_ent)\n",
    "    return focal_loss_my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_fixed(y_true, y_pred):\n",
    "    gamma = 2.\n",
    "    alpha = 0.25\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "#     pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "#     pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def focal_loss(gamma=2., alpha=.25):\n",
    "#     def focal_loss_fixed(y_true, y_pred):\n",
    "#         pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "#         pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "#         pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "#         pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "#         return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "#     return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31072,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1554/1554 [==============================] - 367s 236ms/step - loss: 1.1205 - f1: 0.0377 - val_loss: 1.1683 - val_f1: 0.0203\n",
      "Epoch 2/2\n",
      "1554/1554 [==============================] - 355s 228ms/step - loss: 1.1048 - f1: 0.0499 - val_loss: 1.1664 - val_f1: 0.0230\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 431s 278ms/step - loss: 1.0616 - f1: 0.0932 - val_loss: 0.9940 - val_f1: 0.1531\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.99401, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.9923 - f1: 0.1566 - val_loss: 0.9497 - val_f1: 0.2275\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.99401 to 0.94970, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.9398 - f1: 0.2035 - val_loss: 0.8881 - val_f1: 0.2688\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.94970 to 0.88814, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.9065 - f1: 0.2282 - val_loss: 0.8489 - val_f1: 0.2974\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.88814 to 0.84889, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.8878 - f1: 0.2403 - val_loss: 0.8296 - val_f1: 0.3059\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.84889 to 0.82961, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.8720 - f1: 0.2515 - val_loss: 0.8301 - val_f1: 0.3129\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.82961\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.8605 - f1: 0.2590 - val_loss: 0.8111 - val_f1: 0.3286\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.82961 to 0.81114, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.8496 - f1: 0.2676 - val_loss: 0.8195 - val_f1: 0.3221\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.81114\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.8416 - f1: 0.2718 - val_loss: 0.8123 - val_f1: 0.3214\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.81114\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.8309 - f1: 0.2794 - val_loss: 0.8084 - val_f1: 0.3254\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.81114 to 0.80840, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 409s 263ms/step - loss: 0.8261 - f1: 0.2824 - val_loss: 0.7806 - val_f1: 0.3541\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.80840 to 0.78064, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 411s 264ms/step - loss: 0.8170 - f1: 0.2881 - val_loss: 0.7898 - val_f1: 0.3431\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.78064\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 411s 265ms/step - loss: 0.8094 - f1: 0.2935 - val_loss: 0.8002 - val_f1: 0.3344\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.78064\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 411s 264ms/step - loss: 0.8024 - f1: 0.2979 - val_loss: 0.8120 - val_f1: 0.3352\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.78064\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 411s 264ms/step - loss: 0.7793 - f1: 0.3121 - val_loss: 0.7438 - val_f1: 0.3746\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.78064 to 0.74378, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 411s 264ms/step - loss: 0.7673 - f1: 0.3201 - val_loss: 0.7401 - val_f1: 0.3762\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.74378 to 0.74009, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.7631 - f1: 0.3221 - val_loss: 0.7407 - val_f1: 0.3736\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.74009\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.7599 - f1: 0.3238 - val_loss: 0.7394 - val_f1: 0.3774\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.74009 to 0.73943, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 411s 264ms/step - loss: 0.7572 - f1: 0.3254 - val_loss: 0.7399 - val_f1: 0.3762\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.73943\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.7541 - f1: 0.3263 - val_loss: 0.7372 - val_f1: 0.3786\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.73943 to 0.73723, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 415s 267ms/step - loss: 0.7513 - f1: 0.3282 - val_loss: 0.7360 - val_f1: 0.3790\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.73723 to 0.73600, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 409s 263ms/step - loss: 0.7484 - f1: 0.3306 - val_loss: 0.7337 - val_f1: 0.3819\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.73600 to 0.73371, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 412s 265ms/step - loss: 0.7449 - f1: 0.3323 - val_loss: 0.7356 - val_f1: 0.3813\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.73371\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.7430 - f1: 0.3340 - val_loss: 0.7374 - val_f1: 0.3792\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.73371\n",
      "Epoch 25/120\n",
      "1554/1554 [==============================] - 411s 264ms/step - loss: 0.7375 - f1: 0.3378 - val_loss: 0.7388 - val_f1: 0.3801\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.73371\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 26/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.7369 - f1: 0.3378 - val_loss: 0.7333 - val_f1: 0.3844\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.73371 to 0.73334, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 27/120\n",
      "1554/1554 [==============================] - 411s 264ms/step - loss: 0.7355 - f1: 0.3386 - val_loss: 0.7348 - val_f1: 0.3821\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.73334\n",
      "Epoch 28/120\n",
      "1554/1554 [==============================] - 409s 264ms/step - loss: 0.7346 - f1: 0.3385 - val_loss: 0.7351 - val_f1: 0.3818\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.73334\n",
      "Epoch 29/120\n",
      "1554/1554 [==============================] - 411s 264ms/step - loss: 0.7345 - f1: 0.3386 - val_loss: 0.7358 - val_f1: 0.3808\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.73334\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 30/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.7329 - f1: 0.3396 - val_loss: 0.7347 - val_f1: 0.3816\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.73334\n",
      "Epoch 31/120\n",
      "1554/1554 [==============================] - 410s 264ms/step - loss: 0.7340 - f1: 0.3390 - val_loss: 0.7343 - val_f1: 0.3828\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.73334\n",
      "Epoch 32/120\n",
      "1554/1554 [==============================] - 411s 264ms/step - loss: 0.7337 - f1: 0.3393 - val_loss: 0.7371 - val_f1: 0.3791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6215 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00032: val_loss did not improve from 0.73334\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6215/6215 [27:50<00:00,  3.75it/s]\n",
      "11702it [08:02, 24.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1554/1554 [==============================] - 351s 226ms/step - loss: 1.1199 - f1: 0.0383 - val_loss: 1.1293 - val_f1: 0.0271\n",
      "Epoch 2/2\n",
      "1554/1554 [==============================] - 334s 215ms/step - loss: 1.1055 - f1: 0.0489 - val_loss: 1.2163 - val_f1: 0.0344\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 419s 270ms/step - loss: 1.0527 - f1: 0.1013 - val_loss: 0.9985 - val_f1: 0.1607\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.99850, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.9764 - f1: 0.1724 - val_loss: 0.9163 - val_f1: 0.2417\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.99850 to 0.91633, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.9327 - f1: 0.2094 - val_loss: 0.8611 - val_f1: 0.2886\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.91633 to 0.86106, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.9052 - f1: 0.2292 - val_loss: 0.8867 - val_f1: 0.2722\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.86106\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8870 - f1: 0.2427 - val_loss: 0.8422 - val_f1: 0.2988\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.86106 to 0.84219, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.8757 - f1: 0.2490 - val_loss: 0.8447 - val_f1: 0.3069\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.84219\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8651 - f1: 0.2565 - val_loss: 0.8240 - val_f1: 0.3201\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.84219 to 0.82401, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8557 - f1: 0.2622 - val_loss: 0.8127 - val_f1: 0.3215\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.82401 to 0.81268, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8454 - f1: 0.2695 - val_loss: 0.8022 - val_f1: 0.3358\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.81268 to 0.80215, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8389 - f1: 0.2729 - val_loss: 0.8014 - val_f1: 0.3420\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.80215 to 0.80144, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8293 - f1: 0.2806 - val_loss: 0.8078 - val_f1: 0.3300\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.80144\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.8215 - f1: 0.2851 - val_loss: 0.7766 - val_f1: 0.3578\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.80144 to 0.77662, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 397s 255ms/step - loss: 0.8151 - f1: 0.2896 - val_loss: 0.7805 - val_f1: 0.3502\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.77662\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8109 - f1: 0.2920 - val_loss: 0.8253 - val_f1: 0.3257\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.77662\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8032 - f1: 0.2972 - val_loss: 0.7742 - val_f1: 0.3524\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.77662 to 0.77422, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8002 - f1: 0.2985 - val_loss: 0.7767 - val_f1: 0.3535\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.77422\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7940 - f1: 0.3039 - val_loss: 0.7758 - val_f1: 0.3585\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.77422\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7892 - f1: 0.3057 - val_loss: 0.8228 - val_f1: 0.3315\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.77422\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 397s 255ms/step - loss: 0.7655 - f1: 0.3203 - val_loss: 0.7350 - val_f1: 0.3824\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.77422 to 0.73495, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7546 - f1: 0.3264 - val_loss: 0.7361 - val_f1: 0.3851\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.73495\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7502 - f1: 0.3293 - val_loss: 0.7392 - val_f1: 0.3807\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.73495\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7474 - f1: 0.3309 - val_loss: 0.7352 - val_f1: 0.3857\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.73495\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7422 - f1: 0.3335 - val_loss: 0.7353 - val_f1: 0.3850\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.73495\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7417 - f1: 0.3343 - val_loss: 0.7349 - val_f1: 0.3851\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.73495 to 0.73488, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 25/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7421 - f1: 0.3344 - val_loss: 0.7332 - val_f1: 0.3878\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.73488 to 0.73320, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 26/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7414 - f1: 0.3339 - val_loss: 0.7333 - val_f1: 0.3867\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.73320\n",
      "Epoch 27/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7386 - f1: 0.3367 - val_loss: 0.7323 - val_f1: 0.3864\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.73320 to 0.73228, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 28/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7408 - f1: 0.3340 - val_loss: 0.7341 - val_f1: 0.3861\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.73228\n",
      "Epoch 29/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7391 - f1: 0.3358 - val_loss: 0.7315 - val_f1: 0.3870\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.73228 to 0.73151, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 30/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7398 - f1: 0.3349 - val_loss: 0.7320 - val_f1: 0.3873\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.73151\n",
      "Epoch 31/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7393 - f1: 0.3349 - val_loss: 0.7323 - val_f1: 0.3877\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.73151\n",
      "Epoch 32/120\n",
      "1554/1554 [==============================] - 405s 261ms/step - loss: 0.7385 - f1: 0.3361 - val_loss: 0.7348 - val_f1: 0.3846\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.73151\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 33/120\n",
      "1554/1554 [==============================] - 409s 263ms/step - loss: 0.7397 - f1: 0.3348 - val_loss: 0.7342 - val_f1: 0.3858\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.73151\n",
      "Epoch 34/120\n",
      "1554/1554 [==============================] - 406s 261ms/step - loss: 0.7384 - f1: 0.3361 - val_loss: 0.7322 - val_f1: 0.3880\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.73151\n",
      "Epoch 35/120\n",
      "1554/1554 [==============================] - 395s 255ms/step - loss: 0.7375 - f1: 0.3363 - val_loss: 0.7323 - val_f1: 0.3868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6215 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00035: val_loss did not improve from 0.73151\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6215/6215 [28:01<00:00,  3.71it/s]\n",
      "11702it [08:07, 24.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1554/1554 [==============================] - 357s 230ms/step - loss: 1.1187 - f1: 0.0403 - val_loss: 1.1586 - val_f1: 0.0349\n",
      "Epoch 2/2\n",
      "1554/1554 [==============================] - 342s 220ms/step - loss: 1.1037 - f1: 0.0512 - val_loss: 1.2845 - val_f1: 0.0217\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 414s 266ms/step - loss: 1.0561 - f1: 0.1006 - val_loss: 1.0210 - val_f1: 0.1469\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02102, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.9876 - f1: 0.1633 - val_loss: 0.9481 - val_f1: 0.2157\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02102 to 0.94806, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.9436 - f1: 0.1998 - val_loss: 0.9243 - val_f1: 0.2516\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.94806 to 0.92433, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.9137 - f1: 0.2244 - val_loss: 0.8973 - val_f1: 0.2665\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.92433 to 0.89734, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8942 - f1: 0.2371 - val_loss: 0.8465 - val_f1: 0.2995\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.89734 to 0.84654, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8797 - f1: 0.2461 - val_loss: 0.8434 - val_f1: 0.3047\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.84654 to 0.84338, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8692 - f1: 0.2542 - val_loss: 0.8520 - val_f1: 0.2938\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.84338\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8566 - f1: 0.2638 - val_loss: 0.7993 - val_f1: 0.3291\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.84338 to 0.79929, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8493 - f1: 0.2672 - val_loss: 0.8007 - val_f1: 0.3351\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.79929\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8382 - f1: 0.2753 - val_loss: 0.8135 - val_f1: 0.3274\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.79929\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8315 - f1: 0.2798 - val_loss: 0.7856 - val_f1: 0.3393\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.79929 to 0.78555, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.8238 - f1: 0.2851 - val_loss: 0.8029 - val_f1: 0.3345\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.78555\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.8179 - f1: 0.2880 - val_loss: 0.7978 - val_f1: 0.3357\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.78555\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8107 - f1: 0.2922 - val_loss: 0.7768 - val_f1: 0.3529\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.78555 to 0.77679, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.8035 - f1: 0.2973 - val_loss: 0.7808 - val_f1: 0.3494\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.77679\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7975 - f1: 0.3011 - val_loss: 0.7898 - val_f1: 0.3458\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.77679\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7888 - f1: 0.3065 - val_loss: 0.7978 - val_f1: 0.3398\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.77679\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7667 - f1: 0.3198 - val_loss: 0.7363 - val_f1: 0.3796\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.77679 to 0.73626, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7583 - f1: 0.3255 - val_loss: 0.7361 - val_f1: 0.3780\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.73626 to 0.73611, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7524 - f1: 0.3285 - val_loss: 0.7355 - val_f1: 0.3797\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.73611 to 0.73551, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7500 - f1: 0.3292 - val_loss: 0.7381 - val_f1: 0.3774\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.73551\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7451 - f1: 0.3324 - val_loss: 0.7348 - val_f1: 0.3816\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.73551 to 0.73481, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7414 - f1: 0.3351 - val_loss: 0.7326 - val_f1: 0.3838\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.73481 to 0.73263, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7393 - f1: 0.3357 - val_loss: 0.7331 - val_f1: 0.3851\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.73263\n",
      "Epoch 25/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7371 - f1: 0.3373 - val_loss: 0.7359 - val_f1: 0.3810\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.73263\n",
      "Epoch 26/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7346 - f1: 0.3383 - val_loss: 0.7360 - val_f1: 0.3818\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.73263\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 27/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7304 - f1: 0.3413 - val_loss: 0.7326 - val_f1: 0.3844\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.73263\n",
      "Epoch 28/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7292 - f1: 0.3426 - val_loss: 0.7345 - val_f1: 0.3825\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.73263\n",
      "Epoch 29/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7298 - f1: 0.3406 - val_loss: 0.7325 - val_f1: 0.3846\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.73263 to 0.73248, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 30/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7289 - f1: 0.3413 - val_loss: 0.7337 - val_f1: 0.3833\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.73248\n",
      "Epoch 31/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7283 - f1: 0.3427 - val_loss: 0.7367 - val_f1: 0.3811\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.73248\n",
      "Epoch 32/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7281 - f1: 0.3422 - val_loss: 0.7348 - val_f1: 0.3819\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.73248\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 33/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7275 - f1: 0.3427 - val_loss: 0.7348 - val_f1: 0.3819\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.73248\n",
      "Epoch 34/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7280 - f1: 0.3424 - val_loss: 0.7347 - val_f1: 0.3828\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.73248\n",
      "Epoch 35/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7283 - f1: 0.3424 - val_loss: 0.7314 - val_f1: 0.3870\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.73248 to 0.73135, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 36/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7257 - f1: 0.3445 - val_loss: 0.7331 - val_f1: 0.3850\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.73135\n",
      "Epoch 37/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7265 - f1: 0.3434 - val_loss: 0.7317 - val_f1: 0.3855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00037: val_loss did not improve from 0.73135\n",
      "Epoch 38/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7258 - f1: 0.3447 - val_loss: 0.7329 - val_f1: 0.3849\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.73135\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "Epoch 39/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7256 - f1: 0.3440 - val_loss: 0.7355 - val_f1: 0.3824\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.73135\n",
      "Epoch 40/120\n",
      "1554/1554 [==============================] - 394s 254ms/step - loss: 0.7291 - f1: 0.3422 - val_loss: 0.7353 - val_f1: 0.3822\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.73135\n",
      "Epoch 41/120\n",
      "1554/1554 [==============================] - 395s 254ms/step - loss: 0.7264 - f1: 0.3436 - val_loss: 0.7371 - val_f1: 0.3805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6214 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00041: val_loss did not improve from 0.73135\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6214/6214 [28:09<00:00,  3.67it/s]\n",
      "11702it [08:20, 23.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1554/1554 [==============================] - 365s 235ms/step - loss: 1.1200 - f1: 0.0381 - val_loss: 1.2371 - val_f1: 0.0366\n",
      "Epoch 2/2\n",
      "1554/1554 [==============================] - 346s 223ms/step - loss: 1.1060 - f1: 0.0483 - val_loss: 1.2160 - val_f1: 0.0279\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 422s 271ms/step - loss: 1.0612 - f1: 0.0953 - val_loss: 0.9775 - val_f1: 0.1744\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.97747, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.9820 - f1: 0.1686 - val_loss: 0.9311 - val_f1: 0.2403\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.97747 to 0.93107, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 397s 255ms/step - loss: 0.9348 - f1: 0.2084 - val_loss: 0.8634 - val_f1: 0.2913\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.93107 to 0.86338, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.9049 - f1: 0.2296 - val_loss: 0.8289 - val_f1: 0.3065\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.86338 to 0.82887, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 397s 255ms/step - loss: 0.8888 - f1: 0.2413 - val_loss: 0.8362 - val_f1: 0.3086\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.82887\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 397s 256ms/step - loss: 0.8731 - f1: 0.2509 - val_loss: 0.8263 - val_f1: 0.3142\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.82887 to 0.82627, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 397s 256ms/step - loss: 0.8623 - f1: 0.2589 - val_loss: 0.8188 - val_f1: 0.3184\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.82627 to 0.81883, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.8529 - f1: 0.2648 - val_loss: 0.7960 - val_f1: 0.3306\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.81883 to 0.79603, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 397s 255ms/step - loss: 0.8436 - f1: 0.2708 - val_loss: 0.8194 - val_f1: 0.3263\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.79603\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.8356 - f1: 0.2762 - val_loss: 0.8427 - val_f1: 0.3121\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.79603\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.8262 - f1: 0.2826 - val_loss: 0.7851 - val_f1: 0.3446\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.79603 to 0.78505, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.8232 - f1: 0.2845 - val_loss: 0.7840 - val_f1: 0.3492\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.78505 to 0.78400, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 397s 256ms/step - loss: 0.8177 - f1: 0.2877 - val_loss: 0.7935 - val_f1: 0.3397\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.78400\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.8115 - f1: 0.2921 - val_loss: 0.7930 - val_f1: 0.3409\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.78400\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.8023 - f1: 0.2982 - val_loss: 0.7692 - val_f1: 0.3560\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.78400 to 0.76919, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 397s 256ms/step - loss: 0.7969 - f1: 0.3012 - val_loss: 0.7698 - val_f1: 0.3575\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.76919\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7932 - f1: 0.3040 - val_loss: 0.7969 - val_f1: 0.3358\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.76919\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 397s 255ms/step - loss: 0.7872 - f1: 0.3084 - val_loss: 0.7663 - val_f1: 0.3611\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.76919 to 0.76631, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7832 - f1: 0.3104 - val_loss: 0.7822 - val_f1: 0.3504\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.76631\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 397s 256ms/step - loss: 0.7788 - f1: 0.3122 - val_loss: 0.7812 - val_f1: 0.3548\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.76631\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7729 - f1: 0.3158 - val_loss: 0.7741 - val_f1: 0.3538\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.76631\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 397s 256ms/step - loss: 0.7503 - f1: 0.3292 - val_loss: 0.7367 - val_f1: 0.3806\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.76631 to 0.73673, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 397s 256ms/step - loss: 0.7367 - f1: 0.3388 - val_loss: 0.7319 - val_f1: 0.3843\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.73673 to 0.73186, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 397s 256ms/step - loss: 0.7329 - f1: 0.3400 - val_loss: 0.7339 - val_f1: 0.3829\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.73186\n",
      "Epoch 25/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7277 - f1: 0.3429 - val_loss: 0.7310 - val_f1: 0.3843\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.73186 to 0.73097, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 26/120\n",
      "1554/1554 [==============================] - 397s 256ms/step - loss: 0.7260 - f1: 0.3430 - val_loss: 0.7312 - val_f1: 0.3858\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.73097\n",
      "Epoch 27/120\n",
      "1554/1554 [==============================] - 397s 255ms/step - loss: 0.7232 - f1: 0.3457 - val_loss: 0.7319 - val_f1: 0.3860\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.73097\n",
      "Epoch 28/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7209 - f1: 0.3457 - val_loss: 0.7371 - val_f1: 0.3797\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.73097\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 29/120\n",
      "1554/1554 [==============================] - 396s 255ms/step - loss: 0.7172 - f1: 0.3487 - val_loss: 0.7353 - val_f1: 0.3819\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.73097\n",
      "Epoch 30/120\n",
      "1554/1554 [==============================] - 397s 255ms/step - loss: 0.7157 - f1: 0.3493 - val_loss: 0.7319 - val_f1: 0.3873\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.73097\n",
      "Epoch 31/120\n",
      "1554/1554 [==============================] - 397s 256ms/step - loss: 0.7166 - f1: 0.3490 - val_loss: 0.7350 - val_f1: 0.3828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6214 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: val_loss did not improve from 0.73097\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6214/6214 [28:43<00:00,  3.56it/s]\n",
      "11702it [08:33, 23.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1554/1554 [==============================] - 362s 233ms/step - loss: 1.1205 - f1: 0.0380 - val_loss: 1.1308 - val_f1: 0.0252\n",
      "Epoch 2/2\n",
      "1554/1554 [==============================] - 338s 217ms/step - loss: 1.1038 - f1: 0.0516 - val_loss: 1.1899 - val_f1: 0.0314\n",
      "Epoch 1/120\n",
      "1554/1554 [==============================] - 429s 276ms/step - loss: 1.0548 - f1: 0.1018 - val_loss: 0.9957 - val_f1: 0.1703\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.99568, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 2/120\n",
      "1554/1554 [==============================] - 400s 258ms/step - loss: 0.9812 - f1: 0.1700 - val_loss: 0.9107 - val_f1: 0.2473\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.99568 to 0.91073, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 3/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.9343 - f1: 0.2097 - val_loss: 0.8773 - val_f1: 0.2714\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.91073 to 0.87735, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 4/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.9056 - f1: 0.2294 - val_loss: 0.8639 - val_f1: 0.2838\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.87735 to 0.86385, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 5/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8898 - f1: 0.2395 - val_loss: 0.8381 - val_f1: 0.2994\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.86385 to 0.83812, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 6/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8751 - f1: 0.2501 - val_loss: 0.8256 - val_f1: 0.3131\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.83812 to 0.82565, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 7/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8664 - f1: 0.2559 - val_loss: 0.8203 - val_f1: 0.3183\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.82565 to 0.82035, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 8/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8549 - f1: 0.2644 - val_loss: 0.8142 - val_f1: 0.3263\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.82035 to 0.81424, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 9/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8456 - f1: 0.2694 - val_loss: 0.8184 - val_f1: 0.3239\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.81424\n",
      "Epoch 10/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8387 - f1: 0.2740 - val_loss: 0.7864 - val_f1: 0.3418\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.81424 to 0.78637, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 11/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.8303 - f1: 0.2794 - val_loss: 0.7928 - val_f1: 0.3434\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.78637\n",
      "Epoch 12/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.8215 - f1: 0.2862 - val_loss: 0.7956 - val_f1: 0.3410\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.78637\n",
      "Epoch 13/120\n",
      "1554/1554 [==============================] - 399s 256ms/step - loss: 0.8150 - f1: 0.2900 - val_loss: 0.8184 - val_f1: 0.3261\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.78637\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 14/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7923 - f1: 0.3042 - val_loss: 0.7426 - val_f1: 0.3735\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.78637 to 0.74258, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 15/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7826 - f1: 0.3101 - val_loss: 0.7409 - val_f1: 0.3749\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.74258 to 0.74086, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 16/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7768 - f1: 0.3134 - val_loss: 0.7386 - val_f1: 0.3769\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.74086 to 0.73859, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 17/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7743 - f1: 0.3146 - val_loss: 0.7383 - val_f1: 0.3799\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.73859 to 0.73826, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 18/120\n",
      "1554/1554 [==============================] - 398s 256ms/step - loss: 0.7715 - f1: 0.3159 - val_loss: 0.7392 - val_f1: 0.3768\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.73826\n",
      "Epoch 19/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7668 - f1: 0.3201 - val_loss: 0.7400 - val_f1: 0.3776\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.73826\n",
      "Epoch 20/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7634 - f1: 0.3223 - val_loss: 0.7371 - val_f1: 0.3794\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.73826 to 0.73712, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 21/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7625 - f1: 0.3222 - val_loss: 0.7348 - val_f1: 0.3813\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.73712 to 0.73479, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 22/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7588 - f1: 0.3243 - val_loss: 0.7431 - val_f1: 0.3765\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.73479\n",
      "Epoch 23/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7568 - f1: 0.3256 - val_loss: 0.7394 - val_f1: 0.3797\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.73479\n",
      "Epoch 24/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7554 - f1: 0.3256 - val_loss: 0.7377 - val_f1: 0.3833\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.73479\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 25/120\n",
      "1554/1554 [==============================] - 399s 257ms/step - loss: 0.7502 - f1: 0.3297 - val_loss: 0.7388 - val_f1: 0.3801\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.73479\n",
      "Epoch 26/120\n",
      "1554/1554 [==============================] - 408s 262ms/step - loss: 0.7494 - f1: 0.3293 - val_loss: 0.7359 - val_f1: 0.3813\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.73479\n",
      "Epoch 27/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7481 - f1: 0.3299 - val_loss: 0.7334 - val_f1: 0.3854\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.73479 to 0.73339, saving model to ../cache/InceptionV3.h5\n",
      "Epoch 28/120\n",
      "1554/1554 [==============================] - 407s 262ms/step - loss: 0.7465 - f1: 0.3323 - val_loss: 0.7354 - val_f1: 0.3815\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.73339\n",
      "Epoch 29/120\n",
      "1554/1554 [==============================] - 408s 263ms/step - loss: 0.7470 - f1: 0.3306 - val_loss: 0.7357 - val_f1: 0.3829\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.73339\n",
      "Epoch 30/120\n",
      "1554/1554 [==============================] - 403s 259ms/step - loss: 0.7466 - f1: 0.3318 - val_loss: 0.7363 - val_f1: 0.3820\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.73339\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 31/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7464 - f1: 0.3313 - val_loss: 0.7357 - val_f1: 0.3824\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.73339\n",
      "Epoch 32/120\n",
      "1554/1554 [==============================] - 401s 258ms/step - loss: 0.7468 - f1: 0.3312 - val_loss: 0.7351 - val_f1: 0.3835\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.73339\n",
      "Epoch 33/120\n",
      "1554/1554 [==============================] - 400s 257ms/step - loss: 0.7470 - f1: 0.3313 - val_loss: 0.7378 - val_f1: 0.3795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6214 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: val_loss did not improve from 0.73339\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6214/6214 [29:05<00:00,  3.64it/s]\n",
      "11702it [08:56, 22.73it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split data into train, valid\n",
    "indexes = np.arange(train_dataset_info.shape[0])\n",
    "# np.random.shuffle(indexes)\n",
    "# train_indexes, valid_indexes = train_test_split(indexes, test_size=0.15, random_state=8)\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "# train_generator = data_generator.create_train(\n",
    "#     train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "# validation_generator = data_generator.create_train(\n",
    "#     train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)\n",
    "\n",
    "oof_class_preds = np.zeros((train_dataset_info.shape[0], 28))\n",
    "sub_class_preds = np.zeros((submit.shape[0], 28))\n",
    "\n",
    "fold_ = 0\n",
    "epochs = 10; batch_size = 16\n",
    "for train_indexes, valid_indexes in kf.split(indexes):\n",
    "    \n",
    "    checkpoint = ModelCheckpoint('../cache/InceptionV3.h5', monitor='val_loss', verbose=1, \n",
    "                                 save_best_only=True, mode='min', save_weights_only = True)\n",
    "    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
    "                                       verbose=1, mode='auto', epsilon=0.0001)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", \n",
    "                          mode=\"min\", \n",
    "                          patience=6)\n",
    "    callbacks_list = [checkpoint, early, reduceLROnPlat]\n",
    "\n",
    "\n",
    "    _preds = []\n",
    "    # create train and valid datagens\n",
    "    train_generator = data_generator.create_train(\n",
    "        train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "    validation_generator = data_generator.create_train(\n",
    "        train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)\n",
    "\n",
    "    # warm up model\n",
    "    model = create_model(\n",
    "        input_shape=(SIZE,SIZE,3), \n",
    "        n_out=28)\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.layers[-1].trainable = True\n",
    "    model.layers[-2].trainable = True\n",
    "    model.layers[-3].trainable = True\n",
    "    model.layers[-4].trainable = True\n",
    "    model.layers[-5].trainable = True\n",
    "    model.layers[-6].trainable = True\n",
    "    \n",
    "    model.compile(\n",
    "        loss=f1_loss, \n",
    "        optimizer=Adam(1e-03),\n",
    "        metrics=[f1])\n",
    "#     model.summary()\n",
    "\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "        epochs=2, \n",
    "        verbose=1)\n",
    "    \n",
    "    # train all layers\n",
    "    epochs=120\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    model.compile(loss=f1_loss,\n",
    "                optimizer=Adam(lr=1e-4),\n",
    "                metrics=[f1])\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "        epochs=epochs, \n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list)\n",
    "    \n",
    "    for idx in tqdm(valid_indexes):\n",
    "        item = train_dataset_info[idx]\n",
    "        path = item['path']\n",
    "        labels = item['labels']\n",
    "        image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "        model.load_weights('../cache/InceptionV3.h5')\n",
    "        score_predict = model.predict(image[np.newaxis])[0]\n",
    "        oof_class_preds[idx] = score_predict\n",
    "        np.save('../cache/oof_class_preds-17.npy', oof_class_preds)\n",
    "        \n",
    "    for idx, name in tqdm(enumerate(submit['Id'])):\n",
    "        path = os.path.join('../data/test/', name)\n",
    "        image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "        score_predict = model.predict(image[np.newaxis])[0]\n",
    "        sub_class_preds[idx] += score_predict\n",
    "        np.save('../cache/sub_class_preds-17.npy', sub_class_preds)\n",
    "        \n",
    "    fold_ += 1\n",
    "sub_class_preds /= n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../cache/oof_class_preds-17-1.npy', oof_class_preds)\n",
    "np.save('../cache/sub_class_preds-17-1.npy', sub_class_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof_class_preds = np.zeros((train_dataset_info.shape[0], 28))\n",
    "# sub_class_preds = np.zeros((submit.shape[0], 28))\n",
    "# score_predict = model.predict(image[np.newaxis])[0]\n",
    "# oof_class_preds[idx] = score_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train_index, test_index in kf.split(indexes):\n",
    "# ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11702, 28)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_class_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 89053.01it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for line in tqdm(sub_class_preds):\n",
    "    label_predict = np.arange(28)[line>=0.2]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11702"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " '5 25',\n",
       " '0 5 25',\n",
       " '0 25',\n",
       " '0 7 25',\n",
       " '4 21',\n",
       " '0 4 23 25',\n",
       " '0 23 25',\n",
       " '0',\n",
       " '25',\n",
       " '18 25',\n",
       " '3 5',\n",
       " '0 25',\n",
       " '6 7 9 20',\n",
       " '23',\n",
       " '4 18 25',\n",
       " '2 14',\n",
       " '0 5',\n",
       " '14 21',\n",
       " '0 5',\n",
       " '6',\n",
       " '3 5 24',\n",
       " '0 11 16 25',\n",
       " '0',\n",
       " '0 4',\n",
       " '0 11 12 25 26',\n",
       " '0',\n",
       " '0 1',\n",
       " '0 25',\n",
       " '0',\n",
       " '13 21',\n",
       " '0 25',\n",
       " '14 16 17 18 21 25',\n",
       " '0 5 25',\n",
       " '0 7',\n",
       " '13',\n",
       " '0 25',\n",
       " '0 3',\n",
       " '0 12 21 25',\n",
       " '1',\n",
       " '0 16 17 25',\n",
       " '6 25',\n",
       " '0 5 21 25',\n",
       " '18 19 25',\n",
       " '0 16 17 21 22 25',\n",
       " '6',\n",
       " '0',\n",
       " '0',\n",
       " '6 23 25',\n",
       " '0',\n",
       " '0 17 25',\n",
       " '0 5',\n",
       " '20 23',\n",
       " '0 25',\n",
       " '3',\n",
       " '0 25',\n",
       " '0 17 25',\n",
       " '6 11 23',\n",
       " '0 25',\n",
       " '21 25',\n",
       " '2 21 22 23',\n",
       " '0 5',\n",
       " '14 16 21 25',\n",
       " '21 25',\n",
       " '23',\n",
       " '0 18 19 25',\n",
       " '3 6 21 25',\n",
       " '0 25',\n",
       " '0 16',\n",
       " '21 25',\n",
       " '2 3',\n",
       " '0 2',\n",
       " '14',\n",
       " '4',\n",
       " '0 21',\n",
       " '0',\n",
       " '4',\n",
       " '0 1',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '6 25',\n",
       " '0 25',\n",
       " '0 21 25',\n",
       " '0 17 21 25',\n",
       " '17 18 19',\n",
       " '0 23 25',\n",
       " '23',\n",
       " '0 21',\n",
       " '14 16 17 25',\n",
       " '14',\n",
       " '0 25',\n",
       " '11 21',\n",
       " '23',\n",
       " '12 13',\n",
       " '0 16 17 25',\n",
       " '0 25',\n",
       " '7 18 23 25',\n",
       " '0 7 19 25',\n",
       " '24',\n",
       " '0 23 25',\n",
       " '0 11 25',\n",
       " '23',\n",
       " '21 23',\n",
       " '0 23',\n",
       " '7 11 25',\n",
       " '19 21 22 25',\n",
       " '0 14 16',\n",
       " '0 11 16 24',\n",
       " '7 20 26',\n",
       " '0 25',\n",
       " '2',\n",
       " '1',\n",
       " '16 17 18 25',\n",
       " '0 22 25',\n",
       " '21 25',\n",
       " '21 23',\n",
       " '0 2',\n",
       " '4 25',\n",
       " '6 14 25',\n",
       " '26',\n",
       " '0 18 23 25',\n",
       " '21 25',\n",
       " '2 11 21 25',\n",
       " '7 8 9 20',\n",
       " '0 2 4',\n",
       " '0',\n",
       " '0 21 25',\n",
       " '25',\n",
       " '0 4',\n",
       " '19',\n",
       " '16 17 25',\n",
       " '0 21',\n",
       " '0',\n",
       " '0 23',\n",
       " '0 11',\n",
       " '5',\n",
       " '0 14 16',\n",
       " '0',\n",
       " '5 21',\n",
       " '21 25',\n",
       " '0 19',\n",
       " '21 25',\n",
       " '0 1',\n",
       " '0 5',\n",
       " '0 25',\n",
       " '4 22 26',\n",
       " '0 12 21 25',\n",
       " '0 6 7 25',\n",
       " '0',\n",
       " '6 25',\n",
       " '0 23',\n",
       " '7 16 17 18',\n",
       " '0 7',\n",
       " '0 25',\n",
       " '6 7 25',\n",
       " '6',\n",
       " '0 7',\n",
       " '0 16 17 25',\n",
       " '1 21 25',\n",
       " '25',\n",
       " '0 19 25',\n",
       " '21 25',\n",
       " '0 11 19 25',\n",
       " '4 25',\n",
       " '5',\n",
       " '23',\n",
       " '0 19 22 25',\n",
       " '19',\n",
       " '17 21 25',\n",
       " '7 16 25',\n",
       " '5 25',\n",
       " '0 6 21 25',\n",
       " '0 16 17 25',\n",
       " '21 23',\n",
       " '0 2',\n",
       " '0 11 23',\n",
       " '21',\n",
       " '0 7 18 25',\n",
       " '0 12 21 25',\n",
       " '0 2 25',\n",
       " '0',\n",
       " '23',\n",
       " '0',\n",
       " '0 24 26',\n",
       " '14',\n",
       " '0 14 16 17 25',\n",
       " '0 16 25',\n",
       " '0 5 25',\n",
       " '23',\n",
       " '0 14 16 17 21 25',\n",
       " '14 25',\n",
       " '0 25',\n",
       " '5 25 26',\n",
       " '25 26',\n",
       " '5 25',\n",
       " '0 13 22',\n",
       " '0 25',\n",
       " '16 19 25 26',\n",
       " '23 25',\n",
       " '0 4 25',\n",
       " '2 21',\n",
       " '0 3',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '0 3 5 19 25',\n",
       " '7 21',\n",
       " '0 25',\n",
       " '0 5',\n",
       " '0',\n",
       " '0 21',\n",
       " '0 14 16',\n",
       " '0 5',\n",
       " '7',\n",
       " '0 5',\n",
       " '18 19 25',\n",
       " '0 21',\n",
       " '24 26',\n",
       " '7 25',\n",
       " '21 22',\n",
       " '3',\n",
       " '0 2 3',\n",
       " '14 16 17 25',\n",
       " '0 26',\n",
       " '2 3 12 21',\n",
       " '6 8 20 23',\n",
       " '7 23',\n",
       " '21',\n",
       " '12 25',\n",
       " '0 19',\n",
       " '0 1 5',\n",
       " '11 25',\n",
       " '0 11',\n",
       " '0 2',\n",
       " '23',\n",
       " '0 23 25',\n",
       " '11',\n",
       " '13 20 26',\n",
       " '0 12',\n",
       " '0 18 21',\n",
       " '16 17 18',\n",
       " '21',\n",
       " '2 7',\n",
       " '21',\n",
       " '0 21 25',\n",
       " '2 4 7 11 14 16 17 25',\n",
       " '12 23',\n",
       " '25',\n",
       " '4',\n",
       " '13',\n",
       " '0 1',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '2 6 25',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '11',\n",
       " '23',\n",
       " '3 5',\n",
       " '25',\n",
       " '12 21 25',\n",
       " '14 16 17 18 21 25',\n",
       " '19',\n",
       " '4 26',\n",
       " '0 2 5 19',\n",
       " '13 21 22',\n",
       " '0',\n",
       " '12',\n",
       " '0',\n",
       " '0 23 25',\n",
       " '0 25',\n",
       " '0 19',\n",
       " '0 21 25',\n",
       " '0 12',\n",
       " '0 2 25',\n",
       " '2',\n",
       " '0 18 19 25',\n",
       " '0 2',\n",
       " '11',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '0 21 25',\n",
       " '2',\n",
       " '3',\n",
       " '8 9 10 20 26',\n",
       " '0 19',\n",
       " '0 12 21',\n",
       " '0 2',\n",
       " '7',\n",
       " '1 2 6 25',\n",
       " '7 24',\n",
       " '0 5',\n",
       " '0 14 16',\n",
       " '0 18 21 25',\n",
       " '7',\n",
       " '0 4 19',\n",
       " '0 21 25',\n",
       " '0 23 25',\n",
       " '0 2 25',\n",
       " '0 5',\n",
       " '0 2',\n",
       " '26',\n",
       " '0 3 5 18',\n",
       " '11 25',\n",
       " '0 14',\n",
       " '25',\n",
       " '0 19',\n",
       " '1',\n",
       " '14 16 17 25',\n",
       " '0 21',\n",
       " '0',\n",
       " '11 23',\n",
       " '6 11',\n",
       " '0 4 7 25',\n",
       " '0',\n",
       " '0',\n",
       " '21 25',\n",
       " '14 16 17 18 25',\n",
       " '0 3 25',\n",
       " '0 25',\n",
       " '0 2 25',\n",
       " '19',\n",
       " '6',\n",
       " '0 25',\n",
       " '0 2',\n",
       " '0',\n",
       " '0 2 25',\n",
       " '21 22',\n",
       " '0 2',\n",
       " '0 2 3',\n",
       " '0 14 25',\n",
       " '5',\n",
       " '3 6 9 10 24 25',\n",
       " '25',\n",
       " '0 1 21',\n",
       " '11 21 25',\n",
       " '7',\n",
       " '24',\n",
       " '14 17 25',\n",
       " '0 25',\n",
       " '6',\n",
       " '21 25',\n",
       " '0 7 18',\n",
       " '0 2 4',\n",
       " '14 16 17',\n",
       " '4 12 21 25',\n",
       " '0 2 3 5 19 25',\n",
       " '18 24',\n",
       " '3 7',\n",
       " '0 2 21 25',\n",
       " '0 25',\n",
       " '0 7',\n",
       " '8 9 10 20 26',\n",
       " '25',\n",
       " '0 5 19',\n",
       " '0 2 25',\n",
       " '3',\n",
       " '0 7 24',\n",
       " '0 5 21',\n",
       " '25',\n",
       " '2 4 23',\n",
       " '11 24 25',\n",
       " '2 6 25',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '7 25',\n",
       " '11',\n",
       " '11',\n",
       " '0',\n",
       " '0 5 25',\n",
       " '0 5',\n",
       " '14 16 17 18 25',\n",
       " '0 1 25',\n",
       " '0 21 25',\n",
       " '21 25',\n",
       " '0 23',\n",
       " '0 21',\n",
       " '0 3',\n",
       " '0 3 25',\n",
       " '12 21',\n",
       " '4 21 25',\n",
       " '0 21',\n",
       " '2 7',\n",
       " '0',\n",
       " '2 25',\n",
       " '4 5',\n",
       " '14 16 25',\n",
       " '0',\n",
       " '2 3',\n",
       " '18 19',\n",
       " '0 25',\n",
       " '14 16 17 18 25',\n",
       " '25',\n",
       " '0 14',\n",
       " '21 25',\n",
       " '0 5',\n",
       " '0 25',\n",
       " '0 21',\n",
       " '0 11',\n",
       " '14 16 17 21',\n",
       " '0 25',\n",
       " '0 21 23 25',\n",
       " '0 5 25',\n",
       " '21 23 25',\n",
       " '0 14 16 21',\n",
       " '0 21 22 25',\n",
       " '0 2 23',\n",
       " '0 20 23',\n",
       " '23 26',\n",
       " '21 25',\n",
       " '1',\n",
       " '6 25',\n",
       " '0',\n",
       " '23',\n",
       " '7 25',\n",
       " '0 25',\n",
       " '0',\n",
       " '5 21 25',\n",
       " '21 25',\n",
       " '0 21 25',\n",
       " '0 7 24',\n",
       " '0 7 25',\n",
       " '0 2 5',\n",
       " '21 23',\n",
       " '0 3 4 25',\n",
       " '0 21 25',\n",
       " '0 7 25',\n",
       " '18 25',\n",
       " '0 18 19',\n",
       " '0 3 24',\n",
       " '4 21 25',\n",
       " '0 12 21 25',\n",
       " '0 23 25',\n",
       " '23',\n",
       " '0 14 16',\n",
       " '0 18 25',\n",
       " '0 7 23 25',\n",
       " '0 4',\n",
       " '0 12 21 25',\n",
       " '2',\n",
       " '7',\n",
       " '1 25',\n",
       " '21 25',\n",
       " '17 25',\n",
       " '0 2',\n",
       " '0 25',\n",
       " '0 22 25 26',\n",
       " '0 19 25',\n",
       " '21',\n",
       " '14 16 17 18 25',\n",
       " '17 18 21 25',\n",
       " '12 21',\n",
       " '0 2 3',\n",
       " '0 4 21 25',\n",
       " '0',\n",
       " '0 3',\n",
       " '0',\n",
       " '0 3 5 25',\n",
       " '25',\n",
       " '6 11',\n",
       " '4',\n",
       " '0 25',\n",
       " '2 21 25',\n",
       " '0 2',\n",
       " '0 2 25',\n",
       " '7 9 10 20',\n",
       " '21',\n",
       " '23',\n",
       " '3 5 23',\n",
       " '0 2 11',\n",
       " '7 11',\n",
       " '3',\n",
       " '18 23 25',\n",
       " '0 2',\n",
       " '0 25',\n",
       " '14 17 25',\n",
       " '3 4',\n",
       " '0 11 25',\n",
       " '0 25',\n",
       " '6 23 25',\n",
       " '4',\n",
       " '14',\n",
       " '0 21 22 25',\n",
       " '4',\n",
       " '0',\n",
       " '0 21 22',\n",
       " '0 21 25',\n",
       " '2 25',\n",
       " '0 13',\n",
       " '4',\n",
       " '0 1',\n",
       " '0 25',\n",
       " '21 25',\n",
       " '0 25',\n",
       " '2 25',\n",
       " '0 25',\n",
       " '7',\n",
       " '21 25',\n",
       " '0 7',\n",
       " '0 23',\n",
       " '0 19 23',\n",
       " '16 17 25',\n",
       " '2',\n",
       " '0',\n",
       " '21',\n",
       " '21',\n",
       " '0',\n",
       " '0 21',\n",
       " '2 7 25',\n",
       " '0 25',\n",
       " '0 7',\n",
       " '0 21 25',\n",
       " '0 7 21 25',\n",
       " '12 13',\n",
       " '0 2 3 25',\n",
       " '0 13 16 21 22 25',\n",
       " '21 25',\n",
       " '7',\n",
       " '0',\n",
       " '13 16 25',\n",
       " '3',\n",
       " '0 19',\n",
       " '0 3 25',\n",
       " '1 25',\n",
       " '0 11 25',\n",
       " '0',\n",
       " '19 25',\n",
       " '0 4',\n",
       " '7',\n",
       " '0 25',\n",
       " '23',\n",
       " '18 19',\n",
       " '0 21 25',\n",
       " '0 16 17 25',\n",
       " '0 16 17 18 21',\n",
       " '4 25',\n",
       " '0 14',\n",
       " '2 25',\n",
       " '19',\n",
       " '0 25',\n",
       " '18 19',\n",
       " '12',\n",
       " '0 21',\n",
       " '5 26',\n",
       " '0',\n",
       " '0 25',\n",
       " '7',\n",
       " '4',\n",
       " '0 5',\n",
       " '2 7 21',\n",
       " '2',\n",
       " '23',\n",
       " '14',\n",
       " '0 2 25',\n",
       " '0 11 12 25',\n",
       " '2 7 21 25',\n",
       " '21',\n",
       " '12 21 25',\n",
       " '0 25',\n",
       " '0 18 21 25',\n",
       " '0 21 25',\n",
       " '7',\n",
       " '0',\n",
       " '21 23',\n",
       " '2 7',\n",
       " '0 25',\n",
       " '0 21 25',\n",
       " '18 21 25',\n",
       " '0 25',\n",
       " '6 25',\n",
       " '0 23 25',\n",
       " '18 21 25',\n",
       " '0 25',\n",
       " '14 16 17 25',\n",
       " '0 5 18',\n",
       " '7 25',\n",
       " '11 24',\n",
       " '0 16 17 25',\n",
       " '7 23',\n",
       " '0',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '20 21 23 26',\n",
       " '6 11 25',\n",
       " '5 18 19 21 25',\n",
       " '21',\n",
       " '0 25',\n",
       " '21',\n",
       " '4',\n",
       " '23',\n",
       " '21',\n",
       " '0 7',\n",
       " '5 23',\n",
       " '0 13 20 22',\n",
       " '6 25',\n",
       " '21 22 25',\n",
       " '0 25',\n",
       " '5 19 25',\n",
       " '2 3',\n",
       " '2 14 16',\n",
       " '6 11 14',\n",
       " '0 2 25',\n",
       " '1 2',\n",
       " '23',\n",
       " '0 25',\n",
       " '0 21 25',\n",
       " '0 5 21',\n",
       " '0',\n",
       " '19',\n",
       " '0 4 7',\n",
       " '0 12 21',\n",
       " '0 25',\n",
       " '7 21 25',\n",
       " '14 16',\n",
       " '14 21',\n",
       " '5 19',\n",
       " '0 21',\n",
       " '23',\n",
       " '1 6 25',\n",
       " '0 5 25',\n",
       " '7',\n",
       " '23 25',\n",
       " '3 5',\n",
       " '0 12',\n",
       " '14',\n",
       " '23 25',\n",
       " '0 21',\n",
       " '5 21 22',\n",
       " '21 22',\n",
       " '0 25',\n",
       " '25',\n",
       " '0 12 21',\n",
       " '0 25',\n",
       " '0 5 7',\n",
       " '0 25',\n",
       " '3 5 25',\n",
       " '14 16 17',\n",
       " '0 7 18',\n",
       " '0 19',\n",
       " '5',\n",
       " '7',\n",
       " '21',\n",
       " '13',\n",
       " '0',\n",
       " '23 25',\n",
       " '0 7',\n",
       " '11 12 21 25',\n",
       " '0 1 25',\n",
       " '25',\n",
       " '0 5 7 25',\n",
       " '25 26',\n",
       " '21 25',\n",
       " '14 21 25',\n",
       " '0 21 25',\n",
       " '0 2 3 7',\n",
       " '2',\n",
       " '5 21',\n",
       " '0 23 25',\n",
       " '0 1 4',\n",
       " '0 1 18 19 21 25',\n",
       " '25',\n",
       " '0 18 19',\n",
       " '0 2',\n",
       " '0 19',\n",
       " '0 16 25',\n",
       " '0 25',\n",
       " '2 4 26',\n",
       " '7',\n",
       " '5 25',\n",
       " '5 25',\n",
       " '21 22',\n",
       " '14 16 17 25',\n",
       " '0 21 22',\n",
       " '0 2 5 25',\n",
       " '7',\n",
       " '5 25',\n",
       " '18 19',\n",
       " '0 1 6 25',\n",
       " '0 21',\n",
       " '19 26',\n",
       " '4 18 19 25',\n",
       " '23 25',\n",
       " '1 25',\n",
       " '0 2 14 16',\n",
       " '22',\n",
       " '0 6 21 25',\n",
       " '0 25',\n",
       " '0 22',\n",
       " '0 21',\n",
       " '0 25',\n",
       " '0 16 17 25',\n",
       " '0 25',\n",
       " '23',\n",
       " '0 5 21',\n",
       " '4',\n",
       " '13',\n",
       " '0 25',\n",
       " '6 11 21 25',\n",
       " '0 12 23 25',\n",
       " '0 1 6 21 25',\n",
       " '0 5 14 16 21 25',\n",
       " '5',\n",
       " '0',\n",
       " '21 23',\n",
       " '14',\n",
       " '0 2 3 11 25',\n",
       " '5 25',\n",
       " '11',\n",
       " '0 7',\n",
       " '0 22 25',\n",
       " '3',\n",
       " '0 21',\n",
       " '0 25',\n",
       " '21 25',\n",
       " '1 6 21 25',\n",
       " '25',\n",
       " '0 21 25',\n",
       " '11 12 21 24',\n",
       " '2',\n",
       " '3 4 5',\n",
       " '0 21',\n",
       " '0 2 18 19',\n",
       " '21',\n",
       " '25',\n",
       " '0 25',\n",
       " '19',\n",
       " '0 3',\n",
       " '0 2 25',\n",
       " '0 23',\n",
       " '0 1 2',\n",
       " '0',\n",
       " '0 11 25',\n",
       " '0 13 22 25',\n",
       " '5',\n",
       " '0 17 18 21',\n",
       " '0 2 7',\n",
       " '0 1 2',\n",
       " '7 21',\n",
       " '0 25',\n",
       " '0 18 19 25',\n",
       " '23',\n",
       " '7 25',\n",
       " '0 19 21 25',\n",
       " '23 25',\n",
       " '23',\n",
       " '21',\n",
       " '21 24',\n",
       " '0 13',\n",
       " '0 25',\n",
       " '2 16 17 25',\n",
       " '0 21 25',\n",
       " '0',\n",
       " '23',\n",
       " '0 2 21',\n",
       " '0 3',\n",
       " '0 25',\n",
       " '0 5',\n",
       " '21 25',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '23',\n",
       " '13 25',\n",
       " '23 25',\n",
       " '0 2 21 25',\n",
       " '14',\n",
       " '5 7',\n",
       " '14 21 25',\n",
       " '0 7 21 25',\n",
       " '18 19',\n",
       " '7 23',\n",
       " '7 11',\n",
       " '7',\n",
       " '18 19',\n",
       " '23',\n",
       " '0 21 25',\n",
       " '0 25',\n",
       " '0',\n",
       " '14 16 17 25',\n",
       " '6 23 25',\n",
       " '0 2 16',\n",
       " '21 25',\n",
       " '0 25',\n",
       " '6 11 23',\n",
       " '14 16 17',\n",
       " '0 16 21',\n",
       " '14 16 17 25',\n",
       " '14 25',\n",
       " '0 18 19 25',\n",
       " '6 23',\n",
       " '5',\n",
       " '2 7',\n",
       " '7 23',\n",
       " '0',\n",
       " '0',\n",
       " '0 7 20',\n",
       " '0 25',\n",
       " '25',\n",
       " '23 25',\n",
       " '0 25',\n",
       " '21 22 25 26',\n",
       " '0 2 3 25',\n",
       " '0 18 19 25',\n",
       " '12 25',\n",
       " '0 21',\n",
       " '0 16 17 25',\n",
       " '0 4 21',\n",
       " '0 21',\n",
       " '23',\n",
       " '4 17 18 25',\n",
       " '0 25',\n",
       " '0 19 23',\n",
       " '23',\n",
       " '11',\n",
       " '2',\n",
       " '0 23 25',\n",
       " '0 7',\n",
       " '5 25',\n",
       " '0',\n",
       " '14 16 17 18',\n",
       " '0 5 7 21 25',\n",
       " '0 21 25',\n",
       " '21 22 25',\n",
       " '0 13 14 25',\n",
       " '0 25',\n",
       " '25',\n",
       " '0 1 21 25',\n",
       " '0 6 11 25',\n",
       " '25',\n",
       " '25',\n",
       " '6 25',\n",
       " '0 21',\n",
       " '25',\n",
       " '21 25',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '0 1 2',\n",
       " '0 25',\n",
       " '3',\n",
       " '0 19',\n",
       " '0 5',\n",
       " '0 2 21',\n",
       " '0 25',\n",
       " '0 5',\n",
       " '0 25',\n",
       " '0 1 3 5 19',\n",
       " '7 9 10',\n",
       " '26',\n",
       " '0 23 25',\n",
       " '0 14 16 17 21 25',\n",
       " '18 19 21 25',\n",
       " '0 12 21',\n",
       " '0 23 25',\n",
       " '0 21 25',\n",
       " '14 16 17',\n",
       " '14 21 25',\n",
       " '0 21 25',\n",
       " '0 19',\n",
       " '6 25',\n",
       " '0',\n",
       " '21',\n",
       " '0 18 19',\n",
       " '21',\n",
       " '0 23',\n",
       " '0 18',\n",
       " '0 1',\n",
       " '19 25',\n",
       " '0 25',\n",
       " '0 5 12 21',\n",
       " '0 17 18 25',\n",
       " '0 11 14 16 17',\n",
       " '0 5 21 22',\n",
       " '11',\n",
       " '3 5 22 25',\n",
       " '0 3 5',\n",
       " '0 5',\n",
       " '6 11 25',\n",
       " '0',\n",
       " '0',\n",
       " '',\n",
       " '23',\n",
       " '6',\n",
       " '0 23 25',\n",
       " '0 2 25',\n",
       " '14 21',\n",
       " '0 25',\n",
       " '0 5 26',\n",
       " '12 21',\n",
       " '0 23',\n",
       " '0 20 23 25',\n",
       " '0 25',\n",
       " '21',\n",
       " '0 2 18 25',\n",
       " '5',\n",
       " '23',\n",
       " '12 21 25',\n",
       " '2 7 25',\n",
       " '0 1 7',\n",
       " '6',\n",
       " '20 26',\n",
       " '0 25',\n",
       " '0 25',\n",
       " '0 7',\n",
       " '0 21 25',\n",
       " '0 13 22',\n",
       " '23',\n",
       " '25',\n",
       " '14 25',\n",
       " '4',\n",
       " '0 25',\n",
       " '0 5',\n",
       " '19 21 25',\n",
       " '0 3',\n",
       " '18 19 25',\n",
       " '0 21 25',\n",
       " '0 5',\n",
       " '19 26',\n",
       " '0 7',\n",
       " '14',\n",
       " '25',\n",
       " '7',\n",
       " '2 25',\n",
       " '7',\n",
       " '23 25',\n",
       " '0',\n",
       " '23 25',\n",
       " '0 21 25',\n",
       " '0 3 5',\n",
       " '0 2 21 25',\n",
       " '0 25',\n",
       " '0 2 7 25',\n",
       " '0 25',\n",
       " '0 6 25',\n",
       " '11 19 25',\n",
       " '0 4',\n",
       " '4',\n",
       " '17 21',\n",
       " '14 16 17',\n",
       " '0 21',\n",
       " '0 3 13 21 22 25',\n",
       " '19',\n",
       " '21 23',\n",
       " '19 25',\n",
       " '0 21 25',\n",
       " '21 22',\n",
       " '0 21 25',\n",
       " '1 21 25',\n",
       " '25',\n",
       " '0 2 16',\n",
       " '7',\n",
       " '0 1 6 25',\n",
       " '21 25',\n",
       " '2 26',\n",
       " '0 25',\n",
       " '14 25',\n",
       " '0 2 3',\n",
       " '17 25',\n",
       " '4 21',\n",
       " '4 21',\n",
       " '0 25',\n",
       " '0 19',\n",
       " '0',\n",
       " '0 18 19',\n",
       " '0 11 23 25',\n",
       " '25',\n",
       " '7 16 18',\n",
       " '12',\n",
       " '7 19 23',\n",
       " '0 23',\n",
       " '0 25',\n",
       " '0 7',\n",
       " '7',\n",
       " '21 25',\n",
       " '0 12 14 25',\n",
       " '0 4 25',\n",
       " '0 4',\n",
       " '0 5 21 22',\n",
       " '0',\n",
       " '0 4',\n",
       " '0 16 17 18 25',\n",
       " '0 25',\n",
       " '0 18 21',\n",
       " '11',\n",
       " '0 7 14',\n",
       " '21 25',\n",
       " '3',\n",
       " '6 21 25',\n",
       " '0 2 3 5 23',\n",
       " '3',\n",
       " '18 25',\n",
       " '23',\n",
       " '5',\n",
       " '0 25',\n",
       " '0 2',\n",
       " '0 1 19',\n",
       " '0 2',\n",
       " '0 2',\n",
       " '5 7 25',\n",
       " '0 25 26',\n",
       " '0 21',\n",
       " '0 25',\n",
       " '23 25',\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['Predicted'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submit\n",
    "# submit = pd.read_csv('../data/sample_submission.csv')\n",
    "# predicted = []\n",
    "# draw_predict = []\n",
    "# # model.load_weights('../cache/InceptionV3.h5')\n",
    "# for name in tqdm(submit['Id']):\n",
    "#     path = os.path.join('../data/test/', name)\n",
    "#     image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "#     score_predict = model.predict(image[np.newaxis])[0]\n",
    "#     draw_predict.append(score_predict)\n",
    "#     label_predict = np.arange(28)[score_predict>=0.2]\n",
    "#     str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "#     predicted.append(str_predict_label)\n",
    "\n",
    "# submit['Predicted'] = predicted\n",
    "# np.save('../cache/draw_predict_InceptionV3-8.npy', score_predict)\n",
    "# submit.to_csv('../submissions/submit_InceptionV3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('../submissions/sub17-a.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/1855095/how-to-create-a-zip-archive-of-a-directory\n",
    "def backup_project_as_zip(project_dir, zip_file):\n",
    "    assert(os.path.isdir(project_dir))\n",
    "    assert(os.path.isdir(os.path.dirname(zip_file)))\n",
    "    shutil.make_archive(zip_file.replace('.zip',''), 'zip', project_dir)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-02 11:04:00.238809\n"
     ]
    }
   ],
   "source": [
    "import datetime, shutil\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "PROJECT_PATH = '/home/watts/lal/Kaggle/kagglehp/scripts_nbs'\n",
    "backup_project_as_zip(PROJECT_PATH, '../cache/code.scripts_nbs.%s.zip'%now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 492k/492k [00:13<00:00, 37.3kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 492 ms, sys: 222 ms, total: 714 ms\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub17-a.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub17-a.csv   2018-11-02 05:34:31               complete  0.448        None          \r\n",
      "sub15-e.csv   2018-10-31 17:17:32               complete  0.453        None          \r\n",
      "sub15-d.csv   2018-10-31 17:16:24               complete  0.461        None          \r\n",
      "sub15-c.csv   2018-10-31 17:15:49               complete  0.457        None          \r\n",
      "sub15-b.csv   2018-10-31 17:14:03               complete  0.454        None          \r\n",
      "sub15-a.csv   2018-10-31 17:13:02               complete  0.446        None          \r\n",
      "sub14-d.csv   2018-10-30 16:36:00               complete  0.464        None          \r\n",
      "sub14-c.csv   2018-10-30 16:35:09               complete  0.466        None          \r\n",
      "sub14-bb.csv  2018-10-30 16:34:35               complete  0.461        None          \r\n",
      "sub14-b.csv   2018-10-30 16:33:35               complete  0.459        None          \r\n",
      "sub14-a.csv   2018-10-30 16:32:37               complete  0.452        None          \r\n",
      "sub13-c.csv   2018-10-29 19:24:00               complete  0.459        None          \r\n",
      "sub13-bb.csv  2018-10-29 19:23:01               complete  0.454        None          \r\n",
      "sub13-b.csv   2018-10-29 19:21:53               complete  0.448        None          \r\n",
      "sub13-a.csv   2018-10-29 19:20:40               complete  0.444        None          \r\n",
      "sub12-d.csv   2018-10-26 02:09:32               complete  0.466        None          \r\n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \r\n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \r\n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \r\n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(30)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 98820.65it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for line in tqdm(sub_class_preds):\n",
    "    label_predict = np.arange(28)[line>=0.25]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub17-b.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-02 11:05:04.665317\n"
     ]
    }
   ],
   "source": [
    "import datetime, shutil\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "PROJECT_PATH = '/home/watts/lal/Kaggle/kagglehp/scripts_nbs'\n",
    "backup_project_as_zip(PROJECT_PATH, '../cache/code.scripts_nbs.%s.zip'%now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 486k/486k [00:12<00:00, 39.4kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 386 ms, sys: 145 ms, total: 530 ms\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub17-b.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub17-b.csv   2018-11-02 05:35:30               complete  0.456        None          \r\n",
      "sub17-a.csv   2018-11-02 05:34:31               complete  0.448        None          \r\n",
      "sub15-e.csv   2018-10-31 17:17:32               complete  0.453        None          \r\n",
      "sub15-d.csv   2018-10-31 17:16:24               complete  0.461        None          \r\n",
      "sub15-c.csv   2018-10-31 17:15:49               complete  0.457        None          \r\n",
      "sub15-b.csv   2018-10-31 17:14:03               complete  0.454        None          \r\n",
      "sub15-a.csv   2018-10-31 17:13:02               complete  0.446        None          \r\n",
      "sub14-d.csv   2018-10-30 16:36:00               complete  0.464        None          \r\n",
      "sub14-c.csv   2018-10-30 16:35:09               complete  0.466        None          \r\n",
      "sub14-bb.csv  2018-10-30 16:34:35               complete  0.461        None          \r\n",
      "sub14-b.csv   2018-10-30 16:33:35               complete  0.459        None          \r\n",
      "sub14-a.csv   2018-10-30 16:32:37               complete  0.452        None          \r\n",
      "sub13-c.csv   2018-10-29 19:24:00               complete  0.459        None          \r\n",
      "sub13-bb.csv  2018-10-29 19:23:01               complete  0.454        None          \r\n",
      "sub13-b.csv   2018-10-29 19:21:53               complete  0.448        None          \r\n",
      "sub13-a.csv   2018-10-29 19:20:40               complete  0.444        None          \r\n",
      "sub12-d.csv   2018-10-26 02:09:32               complete  0.466        None          \r\n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \r\n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \r\n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(30)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {0.3:'bb', 0.35:'c', 0.4:'d', 0.45:'e', 0.5:'f'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 97846.66it/s]\n",
      "100%|██████████| 11702/11702 [00:00<00:00, 107801.36it/s]\n",
      "  0%|          | 0/11702 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub17-bb.csv\n",
      "../submissions/sub17-c.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 109296.67it/s]\n",
      "100%|██████████| 11702/11702 [00:00<00:00, 118110.74it/s]\n",
      "  0%|          | 0/11702 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub17-d.csv\n",
      "../submissions/sub17-e.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 11702/11702 [00:00<00:00, 119991.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub17-f.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for alpha in [0.3, 0.35, 0.4, 0.45, 0.5]:\n",
    "    predicted = []\n",
    "    for line in tqdm(sub_class_preds):\n",
    "        label_predict = np.arange(28)[line>=alpha]\n",
    "        str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "        predicted.append(str_predict_label)\n",
    "    submit['Predicted'] = predicted\n",
    "    name = '../submissions/sub17-' + d[alpha] + '.csv'\n",
    "    print(name)\n",
    "    submit.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 481k/481k [00:13<00:00, 37.2kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 506 ms, sys: 201 ms, total: 707 ms\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub17-bb.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub17-bb.csv  2018-11-02 05:36:24               complete  0.464        None          \r\n",
      "sub17-b.csv   2018-11-02 05:35:30               complete  0.456        None          \r\n",
      "sub17-a.csv   2018-11-02 05:34:31               complete  0.448        None          \r\n",
      "sub15-e.csv   2018-10-31 17:17:32               complete  0.453        None          \r\n",
      "sub15-d.csv   2018-10-31 17:16:24               complete  0.461        None          \r\n",
      "sub15-c.csv   2018-10-31 17:15:49               complete  0.457        None          \r\n",
      "sub15-b.csv   2018-10-31 17:14:03               complete  0.454        None          \r\n",
      "sub15-a.csv   2018-10-31 17:13:02               complete  0.446        None          \r\n",
      "sub14-d.csv   2018-10-30 16:36:00               complete  0.464        None          \r\n",
      "sub14-c.csv   2018-10-30 16:35:09               complete  0.466        None          \r\n",
      "sub14-bb.csv  2018-10-30 16:34:35               complete  0.461        None          \r\n",
      "sub14-b.csv   2018-10-30 16:33:35               complete  0.459        None          \r\n",
      "sub14-a.csv   2018-10-30 16:32:37               complete  0.452        None          \r\n",
      "sub13-c.csv   2018-10-29 19:24:00               complete  0.459        None          \r\n",
      "sub13-bb.csv  2018-10-29 19:23:01               complete  0.454        None          \r\n",
      "sub13-b.csv   2018-10-29 19:21:53               complete  0.448        None          \r\n",
      "sub13-a.csv   2018-10-29 19:20:40               complete  0.444        None          \r\n",
      "sub12-d.csv   2018-10-26 02:09:32               complete  0.466        None          \r\n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \r\n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(30)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 477k/477k [00:08<00:00, 50.1kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationfileName      date                 description  status    publicScore  privateScore  \n",
      "------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub17-c.csv   2018-11-02 05:37:12               complete  0.463        None          \n",
      "sub17-bb.csv  2018-11-02 05:36:24               complete  0.464        None          \n",
      "sub17-b.csv   2018-11-02 05:35:30               complete  0.456        None          \n",
      "sub17-a.csv   2018-11-02 05:34:31               complete  0.448        None          \n",
      "sub15-e.csv   2018-10-31 17:17:32               complete  0.453        None          \n",
      "sub15-d.csv   2018-10-31 17:16:24               complete  0.461        None          \n",
      "sub15-c.csv   2018-10-31 17:15:49               complete  0.457        None          \n",
      "sub15-b.csv   2018-10-31 17:14:03               complete  0.454        None          \n",
      "sub15-a.csv   2018-10-31 17:13:02               complete  0.446        None          \n",
      "sub14-d.csv   2018-10-30 16:36:00               complete  0.464        None          \n",
      "sub14-c.csv   2018-10-30 16:35:09               complete  0.466        None          \n",
      "sub14-bb.csv  2018-10-30 16:34:35               complete  0.461        None          \n",
      "sub14-b.csv   2018-10-30 16:33:35               complete  0.459        None          \n",
      "sub14-a.csv   2018-10-30 16:32:37               complete  0.452        None          \n",
      "sub13-c.csv   2018-10-29 19:24:00               complete  0.459        None          \n",
      "sub13-bb.csv  2018-10-29 19:23:01               complete  0.454        None          \n",
      "sub13-b.csv   2018-10-29 19:21:53               complete  0.448        None          \n",
      "sub13-a.csv   2018-10-29 19:20:40               complete  0.444        None          \n",
      "sub12-d.csv   2018-10-26 02:09:32               complete  0.466        None          \n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \n",
      "CPU times: user 434 ms, sys: 288 ms, total: 721 ms\n",
      "Wall time: 48.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub17-c.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(30)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31072, 28)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_class_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31072, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31072, 28)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = 28\n",
    "y_train = np.zeros(oof_class_preds.shape)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31072/31072 [00:00<00:00, 741901.66it/s]\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for labels in tqdm(data['Target'].str.split(' ')):\n",
    "#     print(labels)\n",
    "    for label in labels:\n",
    "        y_train[idx][int(label)] = 1\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 1., 0., 0.],\n",
       "       [1., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.6174081518752018\n",
      "0.01 0.6174081494907646\n",
      "0.1 0.6174079419635514\n",
      "0.5 0.6174049197331472\n",
      "0.75 0.6174023910597577\n",
      "1.0 0.6173997536697944\n",
      "------------------\n",
      "0.001 0.7042131450176463\n",
      "0.01 0.7042131449021258\n",
      "0.1 0.7042131341504188\n",
      "0.5 0.7042129258072632\n",
      "0.75 0.7042126913081677\n",
      "1.0 0.7042123865546952\n",
      "------------------\n",
      "0.001 0.6405293650278552\n",
      "0.01 0.6405293643199262\n",
      "0.1 0.6405293035770623\n",
      "0.5 0.6405284696126301\n",
      "0.75 0.6405278129689744\n",
      "1.0 0.6405271539313317\n",
      "------------------\n",
      "0.001 0.5206868082561187\n",
      "0.01 0.5206868064937693\n",
      "0.1 0.5206866550090807\n",
      "0.5 0.5206845661735036\n",
      "0.75 0.5206829204240566\n",
      "1.0 0.5206812723936773\n",
      "------------------\n",
      "0.001 0.6225629182679313\n",
      "0.01 0.6225628870674288\n",
      "0.1 0.6225602493284829\n",
      "0.5 0.6225265347174933\n",
      "0.75 0.6225024258166685\n",
      "1.0 0.6224802007531789\n",
      "------------------\n",
      "0.001 0.47625033808763034\n",
      "0.01 0.4762503310360723\n",
      "0.1 0.47624973372408597\n",
      "0.5 0.4762420258070793\n",
      "0.75 0.4762364393569697\n",
      "1.0 0.47623122503579396\n",
      "------------------\n",
      "0.001 0.37015383088158105\n",
      "0.01 0.37015383051625794\n",
      "0.1 0.37015379589885844\n",
      "0.5 0.370153120483382\n",
      "0.75 0.3701523989904022\n",
      "1.0 0.3701515220047934\n",
      "------------------\n",
      "0.001 0.6278499957612576\n",
      "0.01 0.6278499933196181\n",
      "0.1 0.6278497675145762\n",
      "0.5 0.6278457789099899\n",
      "0.75 0.6278419420023433\n",
      "1.0 0.627837666925348\n",
      "------------------\n",
      "0.001 0.23972549325385814\n",
      "0.01 0.23972546015299\n",
      "0.1 0.23972230652354265\n",
      "0.5 0.2396593031542591\n",
      "0.75 0.23959034757832143\n",
      "1.0 0.23950497349498\n",
      "------------------\n",
      "0.001 0.31331172323399825\n",
      "0.01 0.3133113149919019\n",
      "0.1 0.3132732609784179\n",
      "0.5 0.3125863700441769\n",
      "0.75 0.3119170265340445\n",
      "1.0 0.31116713928425954\n",
      "------------------\n",
      "0.001 0.2558623247962669\n",
      "0.01 0.255861948626774\n",
      "0.1 0.2558268879333644\n",
      "0.5 0.2551944259969803\n",
      "0.75 0.254578676028139\n",
      "1.0 0.2538894622998168\n",
      "------------------\n",
      "0.001 0.5968983087672983\n",
      "0.01 0.5968983084398413\n",
      "0.1 0.5968982782239397\n",
      "0.5 0.5968977292368627\n",
      "0.75 0.5968971633273094\n",
      "1.0 0.5968964809073818\n",
      "------------------\n",
      "0.001 0.49290871329017727\n",
      "0.01 0.49290871298539585\n",
      "0.1 0.4929086845398815\n",
      "0.5 0.49290813804804207\n",
      "0.75 0.49290753815710786\n",
      "1.0 0.49290677775529046\n",
      "------------------\n",
      "0.001 0.3787539379466851\n",
      "0.01 0.37875392012227116\n",
      "0.1 0.37875241060554954\n",
      "0.5 0.3787329277123833\n",
      "0.75 0.37871876766614176\n",
      "1.0 0.37870547804838306\n",
      "------------------\n",
      "0.001 0.7506973147934757\n",
      "0.01 0.7506973136319354\n",
      "0.1 0.7506972086330781\n",
      "0.5 0.7506954269694857\n",
      "0.75 0.7506936901241192\n",
      "1.0 0.7506916736611128\n",
      "------------------\n",
      "0.001 0.01983763392161686\n",
      "0.01 0.019837535325928135\n",
      "0.1 0.019828450862805713\n",
      "0.5 0.019665777176268495\n",
      "0.75 0.01950321718332293\n",
      "1.0 0.019314882518966292\n",
      "------------------\n",
      "0.001 0.1751297622786967\n",
      "0.01 0.17512974759033484\n",
      "0.1 0.17512846022244855\n",
      "0.5 0.17510915339406974\n",
      "0.75 0.17509248192232651\n",
      "1.0 0.17507468614395694\n",
      "------------------\n",
      "0.001 0.1667931781498947\n",
      "0.01 0.16679317509693092\n",
      "0.1 0.1667928880710675\n",
      "0.5 0.1667873460571777\n",
      "0.75 0.1667814074139947\n",
      "1.0 0.16677415822966601\n",
      "------------------\n",
      "0.001 0.3087568657202341\n",
      "0.01 0.3087568642759011\n",
      "0.1 0.3087567290489941\n",
      "0.5 0.3087542387474458\n",
      "0.75 0.30875175512550346\n",
      "1.0 0.3087489153085835\n",
      "------------------\n",
      "0.001 0.38082130482961174\n",
      "0.01 0.380821304671176\n",
      "0.1 0.38082128970689655\n",
      "0.5 0.38082100099719085\n",
      "0.75 0.38082069536892904\n",
      "1.0 0.3808203259166289\n",
      "------------------\n",
      "0.001 0.1538312914148493\n",
      "0.01 0.15383122528024018\n",
      "0.1 0.1538250686849436\n",
      "0.5 0.15371444612700225\n",
      "0.75 0.15360709884995571\n",
      "1.0 0.15348721808287602\n",
      "------------------\n",
      "0.001 0.48557824579010384\n",
      "0.01 0.48557824528334803\n",
      "0.1 0.4855781985932974\n",
      "0.5 0.48557738349917456\n",
      "0.75 0.48557660601055197\n",
      "1.0 0.48557574340418097\n",
      "------------------\n",
      "0.001 0.3646582386899322\n",
      "0.01 0.3646582372378981\n",
      "0.1 0.364658112376793\n",
      "0.5 0.36465637881526125\n",
      "0.75 0.3646549894024714\n",
      "1.0 0.364653566661458\n",
      "------------------\n",
      "0.001 0.6577178864298491\n",
      "0.01 0.6577178855069651\n",
      "0.1 0.6577177994042775\n",
      "0.5 0.6577162388191071\n",
      "0.75 0.657714710592422\n",
      "1.0 0.6577129906824652\n",
      "------------------\n",
      "0.001 0.48347736718545864\n",
      "0.01 0.4834773624770783\n",
      "0.1 0.4834769405428032\n",
      "0.5 0.4834700745036658\n",
      "0.75 0.48346367708679355\n",
      "1.0 0.483456474479732\n",
      "------------------\n",
      "0.001 0.43786458026883995\n",
      "0.01 0.43786457972979137\n",
      "0.1 0.4378645337127519\n",
      "0.5 0.4378639170133599\n",
      "0.75 0.4378634463465395\n",
      "1.0 0.43786298621777675\n",
      "------------------\n",
      "0.001 0.21837253521077882\n",
      "0.01 0.2183724949425644\n",
      "0.1 0.21836905357883252\n",
      "0.5 0.21832290898991366\n",
      "0.75 0.21828791269733872\n",
      "1.0 0.21825408221249284\n",
      "------------------\n",
      "0.001 0.0019063946880274463\n",
      "0.01 0.0019063937044974109\n",
      "0.1 0.0019063073388934093\n",
      "0.5 0.0019050204660372438\n",
      "0.75 0.0019039365114243976\n",
      "1.0 0.0019028140324179876\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "sub_ridge_preds = np.zeros(sub_class_preds.shape)\n",
    "from sklearn.linear_model import Ridge\n",
    "for cls in np.arange(n_classes):\n",
    "    for alpha in [0.001, 0.01, 0.1, 0.5, 0.75, 1.0]:\n",
    "        y = y_train[:, cls]\n",
    "        X = oof_class_preds\n",
    "        clf = Ridge(alpha=alpha)\n",
    "        clf.fit(X,y)\n",
    "        score_ = clf.score(X, y)\n",
    "        print(alpha, score_)\n",
    "    print('------------------')\n",
    "#         X_test = sub_class_preds[:, cls]\n",
    "#         preds_ = clf.predict(X_test)\n",
    "#         sub_ridge_preds[:,cls] = preds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ridge_preds = np.zeros(sub_class_preds.shape)\n",
    "from sklearn.linear_model import Ridge\n",
    "for cls in np.arange(n_classes):\n",
    "    y = y_train[:, cls]\n",
    "    X = oof_class_preds\n",
    "    clf = Ridge(alpha=0.1)\n",
    "    clf.fit(X,y)\n",
    "    X_test = sub_class_preds\n",
    "    preds_ = clf.predict(X_test)\n",
    "    sub_ridge_preds[:,cls] = preds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.38001867e-02, 1.59430779e-04, 9.98871672e-01, ...,\n",
       "        1.77784316e-03, 1.12266340e-04, 5.20836841e-09],\n",
       "       [2.47062426e-02, 2.68127583e-04, 7.85995722e-04, ...,\n",
       "        6.51888692e-01, 8.71524611e-04, 3.26655725e-05],\n",
       "       [8.41529155e-01, 2.72278007e-04, 3.96186303e-03, ...,\n",
       "        9.23864961e-01, 2.01543609e-03, 1.92988443e-05],\n",
       "       ...,\n",
       "       [6.59056642e-04, 5.12143007e-05, 3.49444263e-05, ...,\n",
       "        1.77463120e-03, 5.20864920e-08, 5.91902866e-09],\n",
       "       [5.01914832e-01, 9.99162483e-01, 2.73049554e-03, ...,\n",
       "        1.16331837e-02, 1.53418808e-04, 1.20560289e-06],\n",
       "       [5.07521251e-01, 3.52088286e-04, 3.01849514e-03, ...,\n",
       "        6.92711103e-01, 1.99362053e-03, 1.05607675e-06]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_class_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11702,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_class_preds[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.26424064e-01,  8.84581981e-03,  8.85833838e-01, ...,\n",
       "         3.20592330e-02,  4.54385428e-03, -6.92529375e-05],\n",
       "       [ 5.51528260e-02,  2.13827355e-03,  1.63897473e-02, ...,\n",
       "         5.38178358e-01,  1.06812025e-03,  1.39571143e-03],\n",
       "       [ 7.77233272e-01, -1.67743788e-03,  1.87791802e-02, ...,\n",
       "         7.51411388e-01, -1.45306321e-03,  1.27131495e-03],\n",
       "       ...,\n",
       "       [ 2.62432560e-02,  4.66381707e-04,  7.97892029e-03, ...,\n",
       "         2.92185935e-02,  3.12140745e-03,  1.76712611e-04],\n",
       "       [ 4.95625470e-01,  8.71181686e-01,  1.99755452e-02, ...,\n",
       "         3.70837162e-02,  3.03441473e-03,  3.03600332e-05],\n",
       "       [ 4.96285545e-01,  1.04218405e-02,  2.67882221e-02, ...,\n",
       "         5.85205684e-01,  4.37290352e-03,  1.52204091e-06]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_ridge_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 88242.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub12-g.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "alpha = 0.35\n",
    "for line in tqdm(sub_ridge_preds):\n",
    "    label_predict = np.arange(28)[line>=alpha]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "name = '../submissions/sub12-g.csv'\n",
    "print(name)\n",
    "submit.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.3.8)\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 372 ms, sys: 201 ms, total: 573 ms\n",
      "Wall time: 17.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub12-g.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.3.8)\r\n",
      "fileName      date                 description  status    publicScore  privateScore  \r\n",
      "------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \r\n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \r\n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \r\n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \r\n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \r\n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \r\n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \r\n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \r\n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \r\n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \r\n",
      "sub11-g.csv   2018-10-23 09:13:19               complete  0.347        None          \r\n",
      "sub11-f.csv   2018-10-23 09:11:15               complete  0.358        None          \r\n",
      "sub11-b.csv   2018-10-23 05:25:32               complete  0.437        None          \r\n",
      "sub12.csv     2018-10-23 05:18:36               complete  0.436        None          \r\n",
      "sub11.csv     2018-10-23 01:07:18               complete  0.431        None          \r\n",
      "sub10.csv     2018-10-22 17:16:40               complete  0.336        None          \r\n",
      "sub9.csv      2018-10-21 20:04:09               complete  0.098        None          \r\n",
      "sub9.csv      2018-10-21 19:44:17               complete  0.073        None          \r\n",
      "sub9.csv      2018-10-21 19:37:30               complete  0.043        None          \r\n",
      "sub9.csv      2018-10-21 19:27:51               complete  0.000        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11702/11702 [00:00<00:00, 79991.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submissions/sub12-h.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "alpha = 0.4\n",
    "for line in tqdm(sub_ridge_preds):\n",
    "    label_predict = np.arange(28)[line>=alpha]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "name = '../submissions/sub12-h.csv'\n",
    "print(name)\n",
    "submit.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.0 / client 1.3.8)\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationWarning: Looks like you're using an outdated API Version, please consider updating (server 1.5.0 / client 1.3.8)\n",
      "fileName      date                 description  status    publicScore  privateScore  \n",
      "------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \n",
      "sub11-g.csv   2018-10-23 09:13:19               complete  0.347        None          \n",
      "sub11-f.csv   2018-10-23 09:11:15               complete  0.358        None          \n",
      "sub11-b.csv   2018-10-23 05:25:32               complete  0.437        None          \n",
      "sub12.csv     2018-10-23 05:18:36               complete  0.436        None          \n",
      "sub11.csv     2018-10-23 01:07:18               complete  0.431        None          \n",
      "sub10.csv     2018-10-22 17:16:40               complete  0.336        None          \n",
      "sub9.csv      2018-10-21 20:04:09               complete  0.098        None          \n",
      "sub9.csv      2018-10-21 19:44:17               complete  0.073        None          \n",
      "sub9.csv      2018-10-21 19:37:30               complete  0.043        None          \n",
      "CPU times: user 328 ms, sys: 282 ms, total: 610 ms\n",
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub12-h.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.0 / client 1.3.8)\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationWarning: Looks like you're using an outdated API Version, please consider updating (server 1.5.0 / client 1.3.8)\n",
      "fileName      date                 description  status    publicScore  privateScore  \n",
      "------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub12-d.csv   2018-10-26 02:09:32               complete  0.466        None          \n",
      "sub12-h.csv   2018-10-26 02:07:56               complete  0.389        None          \n",
      "sub12-g.csv   2018-10-25 00:55:10               complete  0.433        None          \n",
      "sub12-c.csv   2018-10-25 00:45:32               complete  0.469        None          \n",
      "sub12-bb.csv  2018-10-25 00:43:34               complete  0.466        None          \n",
      "sub12-b.csv   2018-10-25 00:41:50               complete  0.457        None          \n",
      "sub12-a.csv   2018-10-25 00:40:56               complete  0.449        None          \n",
      "sub11-k.csv   2018-10-24 00:35:39               complete  0.346        None          \n",
      "sub11-j.csv   2018-10-24 00:34:46               complete  0.366        None          \n",
      "sub11-j.csv   2018-10-24 00:33:17               complete  0.000        None          \n",
      "sub11-i.csv   2018-10-24 00:24:24               complete  0.389        None          \n",
      "sub11-h.csv   2018-10-24 00:21:18               complete  0.371        None          \n",
      "sub11-g.csv   2018-10-23 09:13:19               complete  0.347        None          \n",
      "sub11-f.csv   2018-10-23 09:11:15               complete  0.358        None          \n",
      "sub11-b.csv   2018-10-23 05:25:32               complete  0.437        None          \n",
      "sub12.csv     2018-10-23 05:18:36               complete  0.436        None          \n",
      "sub11.csv     2018-10-23 01:07:18               complete  0.431        None          \n",
      "sub10.csv     2018-10-22 17:16:40               complete  0.336        None          \n",
      "sub9.csv      2018-10-21 20:04:09               complete  0.098        None          \n",
      "sub9.csv      2018-10-21 19:44:17               complete  0.073        None          \n",
      "CPU times: user 352 ms, sys: 328 ms, total: 680 ms\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub12-d.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(10)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpg",
   "language": "python",
   "name": "hpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/mathormad/inceptionv3-baseline-lb-0-379/code\n",
    "# fork of scratch8, 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "import keras_metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) + (1 - K.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "path_to_train = '../data/train/'\n",
    "data = pd.read_csv('../data/train.csv')\n",
    "# path_to_external_data = '../data/external_data/external_data_1/'\n",
    "# edata = pd.read_csv('../data/external_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31072, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_info = []\n",
    "\n",
    "for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "    train_dataset_info.append({\n",
    "        'path':os.path.join(path_to_train, name),\n",
    "        'labels':np.array([int(label) for label in labels])})\n",
    "    \n",
    "# for name, labels in zip(edata['id'], edata['labels'].str.strip('[]')):\n",
    "#     labels = labels.split(',')\n",
    "#     train_dataset_info.append({\n",
    "#         'path':os.path.join(path_to_external_data, name),\n",
    "#         'labels':np.array([int(label) for label in labels])})\n",
    "    \n",
    "train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31072,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# class data_generator:\n",
    "    \n",
    "class threadsafe_iter:\n",
    "    \"\"\"\n",
    "    Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return self.it.__next__()\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"\n",
    "    A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "@threadsafe_generator\n",
    "def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "    assert shape[2] == 3\n",
    "    while True:\n",
    "        dataset_info = shuffle(dataset_info)\n",
    "        for start in range(0, len(dataset_info), batch_size):\n",
    "            end = min(start + batch_size, len(dataset_info))\n",
    "            batch_images = []\n",
    "            X_train_batch = dataset_info[start:end]\n",
    "            batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "            for i in range(len(X_train_batch)):\n",
    "                image = load_image2(\n",
    "                    X_train_batch[i]['path'], shape)\n",
    "#                     image = tdi[i+start]\n",
    "#                     image = cv2.resize(image, (shape[0], shape[1]))\n",
    "                if augument:\n",
    "                    image = augment2(image)\n",
    "\n",
    "                batch_images.append(image/255.)\n",
    "                batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "            yield np.array(batch_images, np.float32), batch_labels\n",
    "\n",
    "def load_image(path, shape):\n",
    "\n",
    "    if len(path.split('/')[3]) != 36:\n",
    "        print(path)\n",
    "    image_red_ch = Image.open(path+'_red.png')\n",
    "    image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "    image_green_ch = Image.open(path+'_green.png')\n",
    "    image_blue_ch = Image.open(path+'_blue.png')\n",
    "    image1 = np.stack((\n",
    "        np.array(image_red_ch),\n",
    "        np.array(image_green_ch), \n",
    "        np.array(image_blue_ch)), -1)\n",
    "    if len(path.split('/')[3]) != 36:\n",
    "        print(image1.shape)\n",
    "    w, h = 512, 512\n",
    "#         zero_data = np.zeros((h, w), dtype=np.uint8)\n",
    "#         image2 = np.stack((\n",
    "#             np.array(image_red_ch),\n",
    "#             np.array(image_green_ch), \n",
    "#             np.array(image_yellow_ch)), -1)\n",
    "#         image3 = np.stack((\n",
    "#             np.array(image_yellow_ch),\n",
    "#             np.array(image_green_ch), \n",
    "#             np.array(image_blue_ch)), -1)\n",
    "# #         print(image1.shape, image2.shape)\n",
    "#         image = np.vstack((image1, image2, image3))\n",
    "#         print(image.shape)\n",
    "    image =image1\n",
    "#         image = canny_image4(image1)\n",
    "    image = cv2.resize(image, (shape[0], shape[1]))\n",
    "    if len(path.split('/')[3]) != 36:\n",
    "        print(image.shape)\n",
    "    return image\n",
    "\n",
    "def load_image2(path, shape):\n",
    "    colors = ['red','green','blue']\n",
    "    flags = cv2.IMREAD_GRAYSCALE\n",
    "    img = [cv2.imread(path+'_'+color+'.png', flags).astype(np.float32)\n",
    "       for color in colors]\n",
    "    return np.stack(img, axis=-1)\n",
    "\n",
    "\n",
    "def augment2(image):\n",
    "    augment_img = iaa.Sequential([\n",
    "        iaa.OneOf([\n",
    "            iaa.Affine(rotate=0),\n",
    "            iaa.Affine(rotate=90),\n",
    "            iaa.Affine(rotate=180),\n",
    "            iaa.Affine(rotate=270),\n",
    "            iaa.Fliplr(0.5),\n",
    "            iaa.Flipud(0.5),\n",
    "        ])], random_order=True)\n",
    "\n",
    "    image_aug = augment_img.augment_image(image)\n",
    "    return image_aug\n",
    "def augment(image):\n",
    "    augment_img = iaa.Sequential([\n",
    "        iaa.OneOf([\n",
    "                iaa.Fliplr(0.5), # horizontal flips\n",
    "                iaa.Affine(rotate=0),\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Flipud(0.5),\n",
    "                iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "                # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "                # But we only blur about 50% of all images.\n",
    "                iaa.Sometimes(0.5,\n",
    "                    iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "                ),\n",
    "                # Strengthen or weaken the contrast in each image.\n",
    "                iaa.ContrastNormalization((0.75, 1.5)),\n",
    "                # Add gaussian noise.\n",
    "                # For 50% of all images, we sample the noise once per pixel.\n",
    "                # For the other 50% of all images, we sample the noise per pixel AND\n",
    "                # channel. This can change the color (not only brightness) of the\n",
    "                # pixels.\n",
    "                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "                # Make some images brighter and some darker.\n",
    "                # In 20% of all cases, we sample the multiplier once per channel,\n",
    "                # which can end up changing the color of the images.\n",
    "                iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "                # Apply affine transformations to each image.\n",
    "                # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "                iaa.Affine(\n",
    "                    scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                    translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                    rotate=(-180, 180),\n",
    "                    shear=(-8, 8)\n",
    "                )\n",
    "            ])], random_order=True)\n",
    "\n",
    "    image_aug = augment_img.augment_image(image)\n",
    "    return image_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Lambda, multiply\n",
    "from keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D, BatchNormalization, Input, Conv2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    in_lay = Input(input_shape)\n",
    "    base_pretrained_model = ResNet50(input_shape =  input_shape, include_top = False, weights = 'imagenet')\n",
    "    base_pretrained_model.trainable = False\n",
    "    pt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\n",
    "    pt_features = base_pretrained_model(in_lay)\n",
    "    from keras.layers import BatchNormalization\n",
    "    bn_features = BatchNormalization()(pt_features)\n",
    "\n",
    "    # here we do an attention mechanism to turn pixels in the GAP on an off\n",
    "\n",
    "    attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(Dropout(0.5)(bn_features))\n",
    "    attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n",
    "    attn_layer = Conv2D(8, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n",
    "    attn_layer = Conv2D(1, kernel_size = (1,1), padding = 'valid', activation = 'sigmoid')(attn_layer)\n",
    "    # fan it out to all of the channels\n",
    "    up_c2_w = np.ones((1, 1, 1, pt_depth))\n",
    "    up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n",
    "                   activation = 'linear', use_bias = False, weights = [up_c2_w])\n",
    "    up_c2.trainable = False\n",
    "    attn_layer = up_c2(attn_layer)\n",
    "\n",
    "    mask_features = multiply([attn_layer, bn_features])\n",
    "    gap_features = GlobalAveragePooling2D()(mask_features)\n",
    "    gap_mask = GlobalAveragePooling2D()(attn_layer)\n",
    "    # to account for missing values from the attention model\n",
    "    gap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\n",
    "    gap_dr = Dropout(0.25)(gap)\n",
    "    dr_steps = Dropout(0.25)(Dense(128, activation = 'relu')(gap_dr))\n",
    "    out_layer = Dense(n_out, activation = 'sigmoid')(dr_steps)\n",
    "    model = Model(inputs = [in_lay], outputs = [out_layer])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "resnet50 (Model)                (None, 16, 16, 2048) 23587712    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 16, 2048) 8192        resnet50[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16, 16, 2048) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 64)   131136      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 16)   1040        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 8)    136         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 1)    9           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 2048) 2048        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 16, 16, 2048) 0           conv2d_5[0][0]                   \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 2048)         0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "RescaleGAP (Lambda)             (None, 2048)         0           global_average_pooling2d_1[0][0] \n",
      "                                                                 global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2048)         0           RescaleGAP[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          262272      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 28)           3612        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,996,157\n",
      "Trainable params: 402,301\n",
      "Non-trainable params: 23,593,856\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# warm up model\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3), \n",
    "    n_out=28)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=f1_loss,\n",
    "            optimizer=Adam(lr=1e-3),\n",
    "            metrics=[f1])\n",
    "# model.load_weights('../cache/R50-57-maximus.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epochs = 10; batch_size = 16\n",
    "checkpoint = ModelCheckpoint('../cache/R50-58-maximus.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
    "                                   verbose=1, mode='auto', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=6)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, valid\n",
    "indexes = np.arange(train_dataset_info.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "train_indexes, valid_indexes = train_test_split(indexes, test_size=0.15, random_state=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "\n",
    "# create train and valid datagens\n",
    "# train_generator = data_generator.create_train(\n",
    "#     train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "# validation_generator = data_generator.create_train(\n",
    "#     train_dataset_info[valid_indexes], 16, (SIZE,SIZE,3), augument=True)\n",
    "\n",
    "train_generator = create_train(\n",
    "    train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "validation_generator = create_train(\n",
    "    train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.layers[0].trainable = False\n",
    "model.layers[1].trainable = False\n",
    "model.layers[2].trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1651/1651 [==============================] - 546s 331ms/step - loss: 1.0292 - f1: 0.1292 - val_loss: 1.3080 - val_f1: 0.0282\n",
      "Epoch 2/2\n",
      "1651/1651 [==============================] - 508s 308ms/step - loss: 0.9807 - f1: 0.1676 - val_loss: 1.2734 - val_f1: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f51ae6a22e8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=2, \n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train all layers\n",
    "epochs=120\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.compile(loss=f1_loss,\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=[f1])\n",
    "# model.fit_generator(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "#     epochs=epochs, \n",
    "#     verbose=1,\n",
    "#     workers=10,\n",
    "#     callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "3302/3302 [==============================] - 1233s 374ms/step - loss: 0.9778 - f1: 0.1515 - val_loss: 0.9010 - val_f1: 0.2161\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.90104, saving model to ../cache/R50-58-maximus.h5\n",
      "Epoch 2/120\n",
      "3302/3302 [==============================] - 1222s 370ms/step - loss: 0.9432 - f1: 0.1730 - val_loss: 0.8680 - val_f1: 0.2437\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.90104 to 0.86798, saving model to ../cache/R50-58-maximus.h5\n",
      "Epoch 3/120\n",
      "3302/3302 [==============================] - 1245s 377ms/step - loss: 0.9284 - f1: 0.1813 - val_loss: 0.8658 - val_f1: 0.2459\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.86798 to 0.86584, saving model to ../cache/R50-58-maximus.h5\n",
      "Epoch 4/120\n",
      "3302/3302 [==============================] - 1248s 378ms/step - loss: 0.9166 - f1: 0.1882 - val_loss: 0.8539 - val_f1: 0.2523\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.86584 to 0.85394, saving model to ../cache/R50-58-maximus.h5\n",
      "Epoch 5/120\n",
      "3302/3302 [==============================] - 1246s 377ms/step - loss: 0.9089 - f1: 0.1920 - val_loss: 0.8598 - val_f1: 0.2574\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.85394\n",
      "Epoch 6/120\n",
      "3302/3302 [==============================] - 1237s 375ms/step - loss: 0.9022 - f1: 0.1953 - val_loss: 0.8452 - val_f1: 0.2666\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.85394 to 0.84524, saving model to ../cache/R50-58-maximus.h5\n",
      "Epoch 7/120\n",
      "3302/3302 [==============================] - 1214s 368ms/step - loss: 0.8971 - f1: 0.1989 - val_loss: 0.8444 - val_f1: 0.2625\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.84524 to 0.84438, saving model to ../cache/R50-58-maximus.h5\n",
      "Epoch 8/120\n",
      "3302/3302 [==============================] - 1209s 366ms/step - loss: 0.8906 - f1: 0.2020 - val_loss: 0.8497 - val_f1: 0.2619\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.84438\n",
      "Epoch 9/120\n",
      "3302/3302 [==============================] - 1210s 366ms/step - loss: 0.8858 - f1: 0.2046 - val_loss: 0.8294 - val_f1: 0.2735\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.84438 to 0.82939, saving model to ../cache/R50-58-maximus.h5\n",
      "Epoch 10/120\n",
      "3302/3302 [==============================] - 1208s 366ms/step - loss: 0.8798 - f1: 0.2080 - val_loss: 0.8261 - val_f1: 0.2764\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.82939 to 0.82606, saving model to ../cache/R50-58-maximus.h5\n",
      "Epoch 11/120\n",
      "3302/3302 [==============================] - 1223s 370ms/step - loss: 0.8770 - f1: 0.2098 - val_loss: 0.8227 - val_f1: 0.2785\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.82606 to 0.82270, saving model to ../cache/R50-58-maximus.h5\n",
      "Epoch 12/120\n",
      "3302/3302 [==============================] - 1372s 416ms/step - loss: 0.8719 - f1: 0.2129 - val_loss: 0.8215 - val_f1: 0.2805\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.82270 to 0.82152, saving model to ../cache/R50-58-maximus.h5\n",
      "Epoch 13/120\n",
      "3302/3302 [==============================] - 1370s 415ms/step - loss: 0.8683 - f1: 0.2144 - val_loss: 0.8251 - val_f1: 0.2782\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.82152\n",
      "Epoch 14/120\n",
      "3302/3302 [==============================] - 1394s 422ms/step - loss: 0.8644 - f1: 0.2158 - val_loss: 0.8166 - val_f1: 0.2837\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.82152 to 0.81662, saving model to ../cache/R50-58-maximus.h5\n",
      "Epoch 15/120\n",
      "3302/3302 [==============================] - 1408s 426ms/step - loss: 0.8607 - f1: 0.2184 - val_loss: 0.8204 - val_f1: 0.2827\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.81662\n",
      "Epoch 16/120\n",
      "3302/3302 [==============================] - 1261s 382ms/step - loss: 0.8552 - f1: 0.2217 - val_loss: 0.8146 - val_f1: 0.2857\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.81662 to 0.81463, saving model to ../cache/R50-58-maximus.h5\n",
      "Epoch 17/120\n",
      "3302/3302 [==============================] - 1207s 365ms/step - loss: 0.8545 - f1: 0.2209 - val_loss: 0.8123 - val_f1: 0.2879\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.81463 to 0.81227, saving model to ../cache/R50-58-maximus.h5\n",
      "Epoch 18/120\n",
      "3302/3302 [==============================] - 1214s 368ms/step - loss: 0.8494 - f1: 0.2245 - val_loss: 0.8239 - val_f1: 0.2781\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.81227\n",
      "Epoch 19/120\n",
      "3302/3302 [==============================] - 1211s 367ms/step - loss: 0.8455 - f1: 0.2266 - val_loss: 0.8074 - val_f1: 0.2913\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.81227 to 0.80741, saving model to ../cache/R50-58-maximus.h5\n",
      "Epoch 20/120\n",
      "3302/3302 [==============================] - 1210s 366ms/step - loss: 0.8424 - f1: 0.2283 - val_loss: 0.8087 - val_f1: 0.2931\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.80741\n",
      "Epoch 21/120\n",
      "3302/3302 [==============================] - 1209s 366ms/step - loss: 0.8399 - f1: 0.2290 - val_loss: 0.8129 - val_f1: 0.2882\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.80741\n",
      "Epoch 22/120\n",
      "3302/3302 [==============================] - 1210s 367ms/step - loss: 0.8362 - f1: 0.2311 - val_loss: 0.8153 - val_f1: 0.2856\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.80741\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 23/120\n",
      "3302/3302 [==============================] - 1212s 367ms/step - loss: 0.8190 - f1: 0.2393 - val_loss: 0.8002 - val_f1: 0.2962\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.80741 to 0.80020, saving model to ../cache/R50-58-maximus.h5\n",
      "Epoch 24/120\n",
      "3302/3302 [==============================] - 1292s 391ms/step - loss: 0.8142 - f1: 0.2410 - val_loss: 0.7974 - val_f1: 0.2970\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.80020 to 0.79744, saving model to ../cache/R50-58-maximus.h5\n",
      "Epoch 25/120\n",
      "3302/3302 [==============================] - 1396s 423ms/step - loss: 0.8088 - f1: 0.2438 - val_loss: 0.7981 - val_f1: 0.2985\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.79744\n",
      "Epoch 26/120\n",
      "3302/3302 [==============================] - 1459s 442ms/step - loss: 0.8057 - f1: 0.2455 - val_loss: 0.8007 - val_f1: 0.2970\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.79744\n",
      "Epoch 27/120\n",
      "3302/3302 [==============================] - 1338s 405ms/step - loss: 0.8049 - f1: 0.2460 - val_loss: 0.8021 - val_f1: 0.2952\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.79744\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 28/120\n",
      "3302/3302 [==============================] - 1239s 375ms/step - loss: 0.8008 - f1: 0.2477 - val_loss: 0.8003 - val_f1: 0.2965\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.79744\n",
      "Epoch 29/120\n",
      "3302/3302 [==============================] - 1245s 377ms/step - loss: 0.8007 - f1: 0.2482 - val_loss: 0.7994 - val_f1: 0.2974\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.79744\n",
      "Epoch 30/120\n",
      "3302/3302 [==============================] - 1245s 377ms/step - loss: 0.7992 - f1: 0.2490 - val_loss: 0.8018 - val_f1: 0.2956\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.79744\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f51afda99e8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "\n",
    "# create train and valid datagens\n",
    "# train_generator = data_generator.create_train(\n",
    "#     train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "# validation_generator = data_generator.create_train(\n",
    "#     train_dataset_info[valid_indexes], 16, (SIZE,SIZE,3), augument=True)\n",
    "\n",
    "train_generator = create_train(\n",
    "    train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "validation_generator = create_train(\n",
    "    train_dataset_info[valid_indexes], 16, (SIZE,SIZE,3), augument=False)\n",
    "\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=epochs, \n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8595f1d70be41158b7c275182c2f608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11702), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create submit\n",
    "from tqdm import tqdm_notebook\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "predicted = []\n",
    "draw_predict = []\n",
    "# model = create_model(\n",
    "#     input_shape=(SIZE,SIZE,3), \n",
    "#     n_out=28)\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = True\n",
    "# model.compile(loss=f1_loss,\n",
    "#             optimizer=Adam(lr=1e-4),\n",
    "#             metrics=[f1])\n",
    "model.load_weights('../cache/R50-58-maximus.h5')\n",
    "for name in tqdm_notebook(submit['Id']):\n",
    "    path = os.path.join('../data/test/', name)\n",
    "    image = load_image2(path, (SIZE,SIZE,3))/255.\n",
    "    score_predict = model.predict(image[np.newaxis])[0]\n",
    "    draw_predict.append(score_predict)\n",
    "    label_predict = np.arange(28)[score_predict>=0.5]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('../submissions/sub58-max-a.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 469k/469k [00:18<00:00, 25.6kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationfileName             date                 description  status    publicScore  privateScore  \n",
      "-------------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub58-max-a.csv      2018-12-22 21:48:49               complete  0.446        None          \n",
      "sub57-max-e.csv      2018-12-22 02:08:14               complete  0.075        None          \n",
      "sub57-max-d.csv      2018-12-22 01:57:31               complete  0.464        None          \n",
      "sub57-max-c.csv      2018-12-22 01:47:16               complete  0.463        None          \n",
      "sub57-max-b.csv      2018-12-22 01:45:44               complete  0.456        None          \n",
      "sub57-max-a.csv      2018-12-21 23:46:32               complete  0.455        None          \n",
      "sub57-max.csv        2018-12-21 08:43:25               complete  0.000        None          \n",
      "sub50-max-055.csv    2018-12-18 20:29:16               complete  0.510        None          \n",
      "sub50-max-f.csv      2018-12-18 20:15:51               complete  0.513        None          \n",
      "sub50-max-f-1-l.csv  2018-12-18 16:40:56               complete  0.546        None          \n",
      "sub50-max-f-1.csv    2018-12-18 16:38:26               complete  0.525        None          \n",
      "sub50-max-f-1.csv    2018-12-18 15:53:31               complete  0.061        None          \n",
      "sub55-max-045-l.csv  2018-12-17 20:34:48               complete  0.574        None          \n",
      "sub55-max-05-l.csv   2018-12-17 20:32:23               complete  0.581        None          \n",
      "sub55-max-045.csv    2018-12-17 20:31:53               complete  0.567        None          \n",
      "sub55-max-05.csv     2018-12-17 20:31:18               complete  0.559        None          \n",
      "sub55-max-05.csv     2018-12-17 20:30:37               complete  0.559        None          \n",
      "sub50_22.csv         2018-12-16 02:56:59               complete  0.579        None          \n",
      "sub50_22.csv         2018-12-16 02:32:16               complete  0.578        None          \n",
      "sub50_22.csv         2018-12-16 02:16:49               complete  0.059        None          \n",
      "CPU times: user 529 ms, sys: 348 ms, total: 877 ms\n",
      "Wall time: 46.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub58-max-a.csv -m \"\"\n",
    "\n",
    "from time import sleep\n",
    "sleep(20)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "predicted = []\n",
    "for ii, name in tqdm_notebook(enumerate(submit['Id'])):\n",
    "    \n",
    "    score_predict = draw_predict[ii]\n",
    "    label_predict = np.arange(28)[score_predict>=0.45]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub578-max-b.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub58-max-b.csv -m \"\"\n",
    "\n",
    "from time import sleep\n",
    "sleep(20)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "predicted = []\n",
    "for ii, name in tqdm_notebook(enumerate(submit['Id'])):\n",
    "    \n",
    "    score_predict = draw_predict[ii]\n",
    "    label_predict = np.arange(28)[score_predict>=0.40]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub58-max-c.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub58-max-c.csv -m \"\"\n",
    "\n",
    "from time import sleep\n",
    "sleep(20)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "predicted = []\n",
    "for ii, name in tqdm_notebook(enumerate(submit['Id'])):\n",
    "    \n",
    "    score_predict = draw_predict[ii]\n",
    "    label_predict = np.arange(28)[score_predict>=0.35]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub57-max-d.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub58-max-d.csv -m \"\"\n",
    "\n",
    "from time import sleep\n",
    "sleep(20)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpg",
   "language": "python",
   "name": "hpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/mathormad/inceptionv3-baseline-lb-0-379/code\n",
    "# fork of scratch8, 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "import keras_metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) + (1 - K.mean(f1))\n",
    "\n",
    "def get_weighted_loss(weights):\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "        tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "        fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "        fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "        p = tp / (tp + fp + K.epsilon())\n",
    "        r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "        f1 = 2*p*r / (p+r+K.epsilon())\n",
    "        f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1) +\\\n",
    "            (1-K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*f1, axis=-1))\n",
    "    \n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "path_to_train = '../data/train/'\n",
    "data = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_info = []\n",
    "for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "    train_dataset_info.append({\n",
    "        'path':os.path.join(path_to_train, name),\n",
    "        'labels':np.array([int(label) for label in labels])})\n",
    "train_dataset_info = np.array(train_dataset_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'path': '../data/train/00070df0-bbc3-11e8-b2bc-ac1f6b6435d0', 'labels': array([16,  0])},\n",
       "       {'path': '../data/train/000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', 'labels': array([7, 1, 2, 0])},\n",
       "       {'path': '../data/train/000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([5])},\n",
       "       ...,\n",
       "       {'path': '../data/train/fff189d8-bbab-11e8-b2ba-ac1f6b6435d0', 'labels': array([7])},\n",
       "       {'path': '../data/train/fffdf7e0-bbc4-11e8-b2bc-ac1f6b6435d0', 'labels': array([25,  2, 21])},\n",
       "       {'path': '../data/train/fffe0ffe-bbc0-11e8-b2bb-ac1f6b6435d0', 'labels': array([2, 0])}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "        assert shape[2] == 3\n",
    "        while True:\n",
    "            dataset_info = shuffle(dataset_info)\n",
    "            for start in range(0, len(dataset_info), batch_size):\n",
    "                end = min(start + batch_size, len(dataset_info))\n",
    "                batch_images = []\n",
    "                X_train_batch = dataset_info[start:end]\n",
    "                batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "                for i in range(len(X_train_batch)):\n",
    "                    image = data_generator.load_image(\n",
    "                        X_train_batch[i]['path'], shape)\n",
    "#                     image = tdi[i+start]\n",
    "#                     image = cv2.resize(image, (shape[0], shape[1]))\n",
    "                    if augument:\n",
    "                        image = data_generator.augment(image)\n",
    "                    batch_images.append(image/255.)\n",
    "                    batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "                yield np.array(batch_images, np.float32), batch_labels\n",
    "\n",
    "    def load_image(path, shape):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image1 = np.stack((\n",
    "            np.array(image_red_ch),\n",
    "            np.array(image_green_ch), \n",
    "            np.array(image_blue_ch)), -1)\n",
    "        w, h = 512, 512\n",
    "#         zero_data = np.zeros((h, w), dtype=np.uint8)\n",
    "#         image2 = np.stack((\n",
    "#             np.array(image_red_ch),\n",
    "#             np.array(image_green_ch), \n",
    "#             np.array(image_yellow_ch)), -1)\n",
    "#         image3 = np.stack((\n",
    "#             np.array(image_yellow_ch),\n",
    "#             np.array(image_green_ch), \n",
    "#             np.array(image_blue_ch)), -1)\n",
    "# #         print(image1.shape, image2.shape)\n",
    "#         image = np.vstack((image1, image2, image3))\n",
    "#         print(image.shape)\n",
    "        image =image1\n",
    "#         image = canny_image4(image1)\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    \n",
    "    def load_image2(path, shape):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image1 = np.stack((\n",
    "            np.array(image_red_ch),\n",
    "            np.array(image_green_ch), \n",
    "            np.array(image_blue_ch)), -1)\n",
    "        w, h = 512, 512\n",
    "#         zero_data = np.zeros((h, w), dtype=np.uint8)\n",
    "#         image2 = np.stack((\n",
    "#             np.array(image_red_ch),\n",
    "#             np.array(image_green_ch), \n",
    "#             np.array(image_yellow_ch)), -1)\n",
    "#         image3 = np.stack((\n",
    "#             np.array(image_yellow_ch),\n",
    "#             np.array(image_green_ch), \n",
    "#             np.array(image_blue_ch)), -1)\n",
    "# #         print(image1.shape, image2.shape)\n",
    "#         image = np.vstack((image1, image2, image3))\n",
    "#         print(image.shape)\n",
    "        image =image1\n",
    "#         image = canny_image4(image1)\n",
    "        image = cv2.resize(image, (shape[0], shape[1]))\n",
    "        return image\n",
    "    \n",
    "    def augment(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=0),\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "            ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n",
    "    def augment2(image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                    iaa.Fliplr(0.5), # horizontal flips\n",
    "                    iaa.Affine(rotate=0),\n",
    "                    iaa.Affine(rotate=90),\n",
    "                    iaa.Affine(rotate=180),\n",
    "                    iaa.Affine(rotate=270),\n",
    "                    iaa.Flipud(0.5),\n",
    "                    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "                    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "                    # But we only blur about 50% of all images.\n",
    "                    iaa.Sometimes(0.5,\n",
    "                        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "                    ),\n",
    "                    # Strengthen or weaken the contrast in each image.\n",
    "                    iaa.ContrastNormalization((0.75, 1.5)),\n",
    "                    # Add gaussian noise.\n",
    "                    # For 50% of all images, we sample the noise once per pixel.\n",
    "                    # For the other 50% of all images, we sample the noise per pixel AND\n",
    "                    # channel. This can change the color (not only brightness) of the\n",
    "                    # pixels.\n",
    "                    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "                    # Make some images brighter and some darker.\n",
    "                    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "                    # which can end up changing the color of the images.\n",
    "                    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "                    # Apply affine transformations to each image.\n",
    "                    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "                    iaa.Affine(\n",
    "                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                        rotate=(-180, 180),\n",
    "                        shear=(-8, 8)\n",
    "                    )\n",
    "                ])], random_order=True)\n",
    "\n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras.layers import Input, Conv2D, Reshape, UpSampling2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = InceptionV3(include_top=False,\n",
    "#                    weights='imagenet',\n",
    "#                    input_shape=(512,512,3))\n",
    "# #     bn = BatchNormalization()(input_tensor)\n",
    "# model.summary()\n",
    "# Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "# ==================================================================================================\n",
    "# input_4 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_408 (Conv2D)             (None, 255, 255, 32) 864         input_4[0][0]                    \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_408 (BatchN (None, 255, 255, 32) 96          conv2d_408[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_407 (Activation)     (None, 255, 255, 32) 0           batch_normalization_408[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_409 (Conv2D)             (None, 253, 253, 32) 9216        activation_407[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_409 (BatchN (None, 253, 253, 32) 96          conv2d_409[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_408 (Activation)     (None, 253, 253, 32) 0           batch_normalization_409[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_410 (Conv2D)             (None, 253, 253, 64) 18432       activation_408[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_410 (BatchN (None, 253, 253, 64) 192         conv2d_410[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_409 (Activation)     (None, 253, 253, 64) 0           batch_normalization_410[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# max_pooling2d_9 (MaxPooling2D)  (None, 126, 126, 64) 0           activation_409[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_411 (Conv2D)             (None, 126, 126, 80) 5120        max_pooling2d_9[0][0]            \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_411 (BatchN (None, 126, 126, 80) 240         conv2d_411[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_410 (Activation)     (None, 126, 126, 80) 0           batch_normalization_411[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_412 (Conv2D)             (None, 124, 124, 192 138240      activation_410[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_412 (BatchN (None, 124, 124, 192 576         conv2d_412[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_411 (Activation)     (None, 124, 124, 192 0           batch_normalization_412[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# max_pooling2d_10 (MaxPooling2D) (None, 61, 61, 192)  0           activation_411[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_416 (Conv2D)             (None, 61, 61, 64)   12288       max_pooling2d_10[0][0]           \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_416 (BatchN (None, 61, 61, 64)   192         conv2d_416[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_415 (Activation)     (None, 61, 61, 64)   0           batch_normalization_416[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_414 (Conv2D)             (None, 61, 61, 48)   9216        max_pooling2d_10[0][0]           \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_417 (Conv2D)             (None, 61, 61, 96)   55296       activation_415[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_414 (BatchN (None, 61, 61, 48)   144         conv2d_414[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_417 (BatchN (None, 61, 61, 96)   288         conv2d_417[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_413 (Activation)     (None, 61, 61, 48)   0           batch_normalization_414[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_416 (Activation)     (None, 61, 61, 96)   0           batch_normalization_417[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_3 (AveragePoo (None, 61, 61, 192)  0           max_pooling2d_10[0][0]           \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_413 (Conv2D)             (None, 61, 61, 64)   12288       max_pooling2d_10[0][0]           \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_415 (Conv2D)             (None, 61, 61, 64)   76800       activation_413[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_418 (Conv2D)             (None, 61, 61, 96)   82944       activation_416[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_419 (Conv2D)             (None, 61, 61, 32)   6144        average_pooling2d_3[0][0]        \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_413 (BatchN (None, 61, 61, 64)   192         conv2d_413[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_415 (BatchN (None, 61, 61, 64)   192         conv2d_415[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_418 (BatchN (None, 61, 61, 96)   288         conv2d_418[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_419 (BatchN (None, 61, 61, 32)   96          conv2d_419[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_412 (Activation)     (None, 61, 61, 64)   0           batch_normalization_413[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_414 (Activation)     (None, 61, 61, 64)   0           batch_normalization_415[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_417 (Activation)     (None, 61, 61, 96)   0           batch_normalization_418[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_418 (Activation)     (None, 61, 61, 32)   0           batch_normalization_419[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed0 (Concatenate)            (None, 61, 61, 256)  0           activation_412[0][0]             \n",
    "#                                                                  activation_414[0][0]             \n",
    "#                                                                  activation_417[0][0]             \n",
    "#                                                                  activation_418[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_423 (Conv2D)             (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_423 (BatchN (None, 61, 61, 64)   192         conv2d_423[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_422 (Activation)     (None, 61, 61, 64)   0           batch_normalization_423[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_421 (Conv2D)             (None, 61, 61, 48)   12288       mixed0[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_424 (Conv2D)             (None, 61, 61, 96)   55296       activation_422[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_421 (BatchN (None, 61, 61, 48)   144         conv2d_421[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_424 (BatchN (None, 61, 61, 96)   288         conv2d_424[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_420 (Activation)     (None, 61, 61, 48)   0           batch_normalization_421[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_423 (Activation)     (None, 61, 61, 96)   0           batch_normalization_424[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_4 (AveragePoo (None, 61, 61, 256)  0           mixed0[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_420 (Conv2D)             (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_422 (Conv2D)             (None, 61, 61, 64)   76800       activation_420[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_425 (Conv2D)             (None, 61, 61, 96)   82944       activation_423[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_426 (Conv2D)             (None, 61, 61, 64)   16384       average_pooling2d_4[0][0]        \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_420 (BatchN (None, 61, 61, 64)   192         conv2d_420[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_422 (BatchN (None, 61, 61, 64)   192         conv2d_422[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_425 (BatchN (None, 61, 61, 96)   288         conv2d_425[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_426 (BatchN (None, 61, 61, 64)   192         conv2d_426[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_419 (Activation)     (None, 61, 61, 64)   0           batch_normalization_420[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_421 (Activation)     (None, 61, 61, 64)   0           batch_normalization_422[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_424 (Activation)     (None, 61, 61, 96)   0           batch_normalization_425[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_425 (Activation)     (None, 61, 61, 64)   0           batch_normalization_426[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed1 (Concatenate)            (None, 61, 61, 288)  0           activation_419[0][0]             \n",
    "#                                                                  activation_421[0][0]             \n",
    "#                                                                  activation_424[0][0]             \n",
    "#                                                                  activation_425[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_430 (Conv2D)             (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_430 (BatchN (None, 61, 61, 64)   192         conv2d_430[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_429 (Activation)     (None, 61, 61, 64)   0           batch_normalization_430[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_428 (Conv2D)             (None, 61, 61, 48)   13824       mixed1[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_431 (Conv2D)             (None, 61, 61, 96)   55296       activation_429[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_428 (BatchN (None, 61, 61, 48)   144         conv2d_428[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_431 (BatchN (None, 61, 61, 96)   288         conv2d_431[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_427 (Activation)     (None, 61, 61, 48)   0           batch_normalization_428[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_430 (Activation)     (None, 61, 61, 96)   0           batch_normalization_431[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_5 (AveragePoo (None, 61, 61, 288)  0           mixed1[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_427 (Conv2D)             (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_429 (Conv2D)             (None, 61, 61, 64)   76800       activation_427[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_432 (Conv2D)             (None, 61, 61, 96)   82944       activation_430[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_433 (Conv2D)             (None, 61, 61, 64)   18432       average_pooling2d_5[0][0]        \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_427 (BatchN (None, 61, 61, 64)   192         conv2d_427[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_429 (BatchN (None, 61, 61, 64)   192         conv2d_429[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_432 (BatchN (None, 61, 61, 96)   288         conv2d_432[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_433 (BatchN (None, 61, 61, 64)   192         conv2d_433[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_426 (Activation)     (None, 61, 61, 64)   0           batch_normalization_427[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_428 (Activation)     (None, 61, 61, 64)   0           batch_normalization_429[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_431 (Activation)     (None, 61, 61, 96)   0           batch_normalization_432[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_432 (Activation)     (None, 61, 61, 64)   0           batch_normalization_433[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed2 (Concatenate)            (None, 61, 61, 288)  0           activation_426[0][0]             \n",
    "#                                                                  activation_428[0][0]             \n",
    "#                                                                  activation_431[0][0]             \n",
    "#                                                                  activation_432[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_435 (Conv2D)             (None, 61, 61, 64)   18432       mixed2[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_435 (BatchN (None, 61, 61, 64)   192         conv2d_435[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_434 (Activation)     (None, 61, 61, 64)   0           batch_normalization_435[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_436 (Conv2D)             (None, 61, 61, 96)   55296       activation_434[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_436 (BatchN (None, 61, 61, 96)   288         conv2d_436[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_435 (Activation)     (None, 61, 61, 96)   0           batch_normalization_436[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_434 (Conv2D)             (None, 30, 30, 384)  995328      mixed2[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_437 (Conv2D)             (None, 30, 30, 96)   82944       activation_435[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_434 (BatchN (None, 30, 30, 384)  1152        conv2d_434[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_437 (BatchN (None, 30, 30, 96)   288         conv2d_437[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_433 (Activation)     (None, 30, 30, 384)  0           batch_normalization_434[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_436 (Activation)     (None, 30, 30, 96)   0           batch_normalization_437[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# max_pooling2d_11 (MaxPooling2D) (None, 30, 30, 288)  0           mixed2[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed3 (Concatenate)            (None, 30, 30, 768)  0           activation_433[0][0]             \n",
    "#                                                                  activation_436[0][0]             \n",
    "#                                                                  max_pooling2d_11[0][0]           \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_442 (Conv2D)             (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_442 (BatchN (None, 30, 30, 128)  384         conv2d_442[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_441 (Activation)     (None, 30, 30, 128)  0           batch_normalization_442[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_443 (Conv2D)             (None, 30, 30, 128)  114688      activation_441[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_443 (BatchN (None, 30, 30, 128)  384         conv2d_443[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_442 (Activation)     (None, 30, 30, 128)  0           batch_normalization_443[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_439 (Conv2D)             (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_444 (Conv2D)             (None, 30, 30, 128)  114688      activation_442[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_439 (BatchN (None, 30, 30, 128)  384         conv2d_439[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_444 (BatchN (None, 30, 30, 128)  384         conv2d_444[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_438 (Activation)     (None, 30, 30, 128)  0           batch_normalization_439[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_443 (Activation)     (None, 30, 30, 128)  0           batch_normalization_444[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_440 (Conv2D)             (None, 30, 30, 128)  114688      activation_438[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_445 (Conv2D)             (None, 30, 30, 128)  114688      activation_443[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_440 (BatchN (None, 30, 30, 128)  384         conv2d_440[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_445 (BatchN (None, 30, 30, 128)  384         conv2d_445[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_439 (Activation)     (None, 30, 30, 128)  0           batch_normalization_440[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_444 (Activation)     (None, 30, 30, 128)  0           batch_normalization_445[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_6 (AveragePoo (None, 30, 30, 768)  0           mixed3[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_438 (Conv2D)             (None, 30, 30, 192)  147456      mixed3[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_441 (Conv2D)             (None, 30, 30, 192)  172032      activation_439[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_446 (Conv2D)             (None, 30, 30, 192)  172032      activation_444[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_447 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_6[0][0]        \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_438 (BatchN (None, 30, 30, 192)  576         conv2d_438[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_441 (BatchN (None, 30, 30, 192)  576         conv2d_441[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_446 (BatchN (None, 30, 30, 192)  576         conv2d_446[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_447 (BatchN (None, 30, 30, 192)  576         conv2d_447[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_437 (Activation)     (None, 30, 30, 192)  0           batch_normalization_438[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_440 (Activation)     (None, 30, 30, 192)  0           batch_normalization_441[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_445 (Activation)     (None, 30, 30, 192)  0           batch_normalization_446[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_446 (Activation)     (None, 30, 30, 192)  0           batch_normalization_447[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed4 (Concatenate)            (None, 30, 30, 768)  0           activation_437[0][0]             \n",
    "#                                                                  activation_440[0][0]             \n",
    "#                                                                  activation_445[0][0]             \n",
    "#                                                                  activation_446[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_452 (Conv2D)             (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_452 (BatchN (None, 30, 30, 160)  480         conv2d_452[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_451 (Activation)     (None, 30, 30, 160)  0           batch_normalization_452[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_453 (Conv2D)             (None, 30, 30, 160)  179200      activation_451[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_453 (BatchN (None, 30, 30, 160)  480         conv2d_453[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_452 (Activation)     (None, 30, 30, 160)  0           batch_normalization_453[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_449 (Conv2D)             (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_454 (Conv2D)             (None, 30, 30, 160)  179200      activation_452[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_449 (BatchN (None, 30, 30, 160)  480         conv2d_449[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_454 (BatchN (None, 30, 30, 160)  480         conv2d_454[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_448 (Activation)     (None, 30, 30, 160)  0           batch_normalization_449[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_453 (Activation)     (None, 30, 30, 160)  0           batch_normalization_454[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_450 (Conv2D)             (None, 30, 30, 160)  179200      activation_448[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_455 (Conv2D)             (None, 30, 30, 160)  179200      activation_453[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_450 (BatchN (None, 30, 30, 160)  480         conv2d_450[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_455 (BatchN (None, 30, 30, 160)  480         conv2d_455[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_449 (Activation)     (None, 30, 30, 160)  0           batch_normalization_450[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_454 (Activation)     (None, 30, 30, 160)  0           batch_normalization_455[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_7 (AveragePoo (None, 30, 30, 768)  0           mixed4[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_448 (Conv2D)             (None, 30, 30, 192)  147456      mixed4[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_451 (Conv2D)             (None, 30, 30, 192)  215040      activation_449[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_456 (Conv2D)             (None, 30, 30, 192)  215040      activation_454[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_457 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_7[0][0]        \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_448 (BatchN (None, 30, 30, 192)  576         conv2d_448[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_451 (BatchN (None, 30, 30, 192)  576         conv2d_451[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_456 (BatchN (None, 30, 30, 192)  576         conv2d_456[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_457 (BatchN (None, 30, 30, 192)  576         conv2d_457[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_447 (Activation)     (None, 30, 30, 192)  0           batch_normalization_448[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_450 (Activation)     (None, 30, 30, 192)  0           batch_normalization_451[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_455 (Activation)     (None, 30, 30, 192)  0           batch_normalization_456[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_456 (Activation)     (None, 30, 30, 192)  0           batch_normalization_457[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed5 (Concatenate)            (None, 30, 30, 768)  0           activation_447[0][0]             \n",
    "#                                                                  activation_450[0][0]             \n",
    "#                                                                  activation_455[0][0]             \n",
    "#                                                                  activation_456[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_462 (Conv2D)             (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_462 (BatchN (None, 30, 30, 160)  480         conv2d_462[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_461 (Activation)     (None, 30, 30, 160)  0           batch_normalization_462[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_463 (Conv2D)             (None, 30, 30, 160)  179200      activation_461[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_463 (BatchN (None, 30, 30, 160)  480         conv2d_463[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_462 (Activation)     (None, 30, 30, 160)  0           batch_normalization_463[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_459 (Conv2D)             (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_464 (Conv2D)             (None, 30, 30, 160)  179200      activation_462[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_459 (BatchN (None, 30, 30, 160)  480         conv2d_459[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_464 (BatchN (None, 30, 30, 160)  480         conv2d_464[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_458 (Activation)     (None, 30, 30, 160)  0           batch_normalization_459[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_463 (Activation)     (None, 30, 30, 160)  0           batch_normalization_464[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_460 (Conv2D)             (None, 30, 30, 160)  179200      activation_458[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_465 (Conv2D)             (None, 30, 30, 160)  179200      activation_463[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_460 (BatchN (None, 30, 30, 160)  480         conv2d_460[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_465 (BatchN (None, 30, 30, 160)  480         conv2d_465[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_459 (Activation)     (None, 30, 30, 160)  0           batch_normalization_460[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_464 (Activation)     (None, 30, 30, 160)  0           batch_normalization_465[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_8 (AveragePoo (None, 30, 30, 768)  0           mixed5[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_458 (Conv2D)             (None, 30, 30, 192)  147456      mixed5[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_461 (Conv2D)             (None, 30, 30, 192)  215040      activation_459[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_466 (Conv2D)             (None, 30, 30, 192)  215040      activation_464[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_467 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_8[0][0]        \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_458 (BatchN (None, 30, 30, 192)  576         conv2d_458[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_461 (BatchN (None, 30, 30, 192)  576         conv2d_461[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_466 (BatchN (None, 30, 30, 192)  576         conv2d_466[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_467 (BatchN (None, 30, 30, 192)  576         conv2d_467[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_457 (Activation)     (None, 30, 30, 192)  0           batch_normalization_458[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_460 (Activation)     (None, 30, 30, 192)  0           batch_normalization_461[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_465 (Activation)     (None, 30, 30, 192)  0           batch_normalization_466[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_466 (Activation)     (None, 30, 30, 192)  0           batch_normalization_467[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed6 (Concatenate)            (None, 30, 30, 768)  0           activation_457[0][0]             \n",
    "#                                                                  activation_460[0][0]             \n",
    "#                                                                  activation_465[0][0]             \n",
    "#                                                                  activation_466[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_472 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_472 (BatchN (None, 30, 30, 192)  576         conv2d_472[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_471 (Activation)     (None, 30, 30, 192)  0           batch_normalization_472[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_473 (Conv2D)             (None, 30, 30, 192)  258048      activation_471[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_473 (BatchN (None, 30, 30, 192)  576         conv2d_473[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_472 (Activation)     (None, 30, 30, 192)  0           batch_normalization_473[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_469 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_474 (Conv2D)             (None, 30, 30, 192)  258048      activation_472[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_469 (BatchN (None, 30, 30, 192)  576         conv2d_469[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_474 (BatchN (None, 30, 30, 192)  576         conv2d_474[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_468 (Activation)     (None, 30, 30, 192)  0           batch_normalization_469[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_473 (Activation)     (None, 30, 30, 192)  0           batch_normalization_474[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_470 (Conv2D)             (None, 30, 30, 192)  258048      activation_468[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_475 (Conv2D)             (None, 30, 30, 192)  258048      activation_473[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_470 (BatchN (None, 30, 30, 192)  576         conv2d_470[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_475 (BatchN (None, 30, 30, 192)  576         conv2d_475[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_469 (Activation)     (None, 30, 30, 192)  0           batch_normalization_470[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_474 (Activation)     (None, 30, 30, 192)  0           batch_normalization_475[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_9 (AveragePoo (None, 30, 30, 768)  0           mixed6[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_468 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_471 (Conv2D)             (None, 30, 30, 192)  258048      activation_469[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_476 (Conv2D)             (None, 30, 30, 192)  258048      activation_474[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_477 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_9[0][0]        \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_468 (BatchN (None, 30, 30, 192)  576         conv2d_468[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_471 (BatchN (None, 30, 30, 192)  576         conv2d_471[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_476 (BatchN (None, 30, 30, 192)  576         conv2d_476[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_477 (BatchN (None, 30, 30, 192)  576         conv2d_477[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_467 (Activation)     (None, 30, 30, 192)  0           batch_normalization_468[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_470 (Activation)     (None, 30, 30, 192)  0           batch_normalization_471[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_475 (Activation)     (None, 30, 30, 192)  0           batch_normalization_476[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_476 (Activation)     (None, 30, 30, 192)  0           batch_normalization_477[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed7 (Concatenate)            (None, 30, 30, 768)  0           activation_467[0][0]             \n",
    "#                                                                  activation_470[0][0]             \n",
    "#                                                                  activation_475[0][0]             \n",
    "#                                                                  activation_476[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_480 (Conv2D)             (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_480 (BatchN (None, 30, 30, 192)  576         conv2d_480[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_479 (Activation)     (None, 30, 30, 192)  0           batch_normalization_480[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_481 (Conv2D)             (None, 30, 30, 192)  258048      activation_479[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_481 (BatchN (None, 30, 30, 192)  576         conv2d_481[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_480 (Activation)     (None, 30, 30, 192)  0           batch_normalization_481[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_478 (Conv2D)             (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_482 (Conv2D)             (None, 30, 30, 192)  258048      activation_480[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_478 (BatchN (None, 30, 30, 192)  576         conv2d_478[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_482 (BatchN (None, 30, 30, 192)  576         conv2d_482[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_477 (Activation)     (None, 30, 30, 192)  0           batch_normalization_478[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_481 (Activation)     (None, 30, 30, 192)  0           batch_normalization_482[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_479 (Conv2D)             (None, 14, 14, 320)  552960      activation_477[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_483 (Conv2D)             (None, 14, 14, 192)  331776      activation_481[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_479 (BatchN (None, 14, 14, 320)  960         conv2d_479[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_483 (BatchN (None, 14, 14, 192)  576         conv2d_483[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_478 (Activation)     (None, 14, 14, 320)  0           batch_normalization_479[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_482 (Activation)     (None, 14, 14, 192)  0           batch_normalization_483[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# max_pooling2d_12 (MaxPooling2D) (None, 14, 14, 768)  0           mixed7[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed8 (Concatenate)            (None, 14, 14, 1280) 0           activation_478[0][0]             \n",
    "#                                                                  activation_482[0][0]             \n",
    "#                                                                  max_pooling2d_12[0][0]           \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_488 (Conv2D)             (None, 14, 14, 448)  573440      mixed8[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_488 (BatchN (None, 14, 14, 448)  1344        conv2d_488[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_487 (Activation)     (None, 14, 14, 448)  0           batch_normalization_488[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_485 (Conv2D)             (None, 14, 14, 384)  491520      mixed8[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_489 (Conv2D)             (None, 14, 14, 384)  1548288     activation_487[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_485 (BatchN (None, 14, 14, 384)  1152        conv2d_485[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_489 (BatchN (None, 14, 14, 384)  1152        conv2d_489[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_484 (Activation)     (None, 14, 14, 384)  0           batch_normalization_485[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_488 (Activation)     (None, 14, 14, 384)  0           batch_normalization_489[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_486 (Conv2D)             (None, 14, 14, 384)  442368      activation_484[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_487 (Conv2D)             (None, 14, 14, 384)  442368      activation_484[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_490 (Conv2D)             (None, 14, 14, 384)  442368      activation_488[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_491 (Conv2D)             (None, 14, 14, 384)  442368      activation_488[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_10 (AveragePo (None, 14, 14, 1280) 0           mixed8[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_484 (Conv2D)             (None, 14, 14, 320)  409600      mixed8[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_486 (BatchN (None, 14, 14, 384)  1152        conv2d_486[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_487 (BatchN (None, 14, 14, 384)  1152        conv2d_487[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_490 (BatchN (None, 14, 14, 384)  1152        conv2d_490[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_491 (BatchN (None, 14, 14, 384)  1152        conv2d_491[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_492 (Conv2D)             (None, 14, 14, 192)  245760      average_pooling2d_10[0][0]       \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_484 (BatchN (None, 14, 14, 320)  960         conv2d_484[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_485 (Activation)     (None, 14, 14, 384)  0           batch_normalization_486[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_486 (Activation)     (None, 14, 14, 384)  0           batch_normalization_487[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_489 (Activation)     (None, 14, 14, 384)  0           batch_normalization_490[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_490 (Activation)     (None, 14, 14, 384)  0           batch_normalization_491[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_492 (BatchN (None, 14, 14, 192)  576         conv2d_492[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_483 (Activation)     (None, 14, 14, 320)  0           batch_normalization_484[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed9_0 (Concatenate)          (None, 14, 14, 768)  0           activation_485[0][0]             \n",
    "#                                                                  activation_486[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# concatenate_1 (Concatenate)     (None, 14, 14, 768)  0           activation_489[0][0]             \n",
    "#                                                                  activation_490[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_491 (Activation)     (None, 14, 14, 192)  0           batch_normalization_492[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed9 (Concatenate)            (None, 14, 14, 2048) 0           activation_483[0][0]             \n",
    "#                                                                  mixed9_0[0][0]                   \n",
    "#                                                                  concatenate_1[0][0]              \n",
    "#                                                                  activation_491[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_497 (Conv2D)             (None, 14, 14, 448)  917504      mixed9[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_497 (BatchN (None, 14, 14, 448)  1344        conv2d_497[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_496 (Activation)     (None, 14, 14, 448)  0           batch_normalization_497[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_494 (Conv2D)             (None, 14, 14, 384)  786432      mixed9[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_498 (Conv2D)             (None, 14, 14, 384)  1548288     activation_496[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_494 (BatchN (None, 14, 14, 384)  1152        conv2d_494[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_498 (BatchN (None, 14, 14, 384)  1152        conv2d_498[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_493 (Activation)     (None, 14, 14, 384)  0           batch_normalization_494[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_497 (Activation)     (None, 14, 14, 384)  0           batch_normalization_498[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_495 (Conv2D)             (None, 14, 14, 384)  442368      activation_493[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_496 (Conv2D)             (None, 14, 14, 384)  442368      activation_493[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_499 (Conv2D)             (None, 14, 14, 384)  442368      activation_497[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_500 (Conv2D)             (None, 14, 14, 384)  442368      activation_497[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# average_pooling2d_11 (AveragePo (None, 14, 14, 2048) 0           mixed9[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_493 (Conv2D)             (None, 14, 14, 320)  655360      mixed9[0][0]                     \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_495 (BatchN (None, 14, 14, 384)  1152        conv2d_495[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_496 (BatchN (None, 14, 14, 384)  1152        conv2d_496[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_499 (BatchN (None, 14, 14, 384)  1152        conv2d_499[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_500 (BatchN (None, 14, 14, 384)  1152        conv2d_500[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# conv2d_501 (Conv2D)             (None, 14, 14, 192)  393216      average_pooling2d_11[0][0]       \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_493 (BatchN (None, 14, 14, 320)  960         conv2d_493[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_494 (Activation)     (None, 14, 14, 384)  0           batch_normalization_495[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_495 (Activation)     (None, 14, 14, 384)  0           batch_normalization_496[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_498 (Activation)     (None, 14, 14, 384)  0           batch_normalization_499[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_499 (Activation)     (None, 14, 14, 384)  0           batch_normalization_500[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# batch_normalization_501 (BatchN (None, 14, 14, 192)  576         conv2d_501[0][0]                 \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_492 (Activation)     (None, 14, 14, 320)  0           batch_normalization_493[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed9_1 (Concatenate)          (None, 14, 14, 768)  0           activation_494[0][0]             \n",
    "#                                                                  activation_495[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# concatenate_2 (Concatenate)     (None, 14, 14, 768)  0           activation_498[0][0]             \n",
    "#                                                                  activation_499[0][0]             \n",
    "# __________________________________________________________________________________________________\n",
    "# activation_500 (Activation)     (None, 14, 14, 192)  0           batch_normalization_501[0][0]    \n",
    "# __________________________________________________________________________________________________\n",
    "# mixed10 (Concatenate)           (None, 14, 14, 2048) 0           activation_492[0][0]             \n",
    "#                                                                  mixed9_1[0][0]                   \n",
    "#                                                                  concatenate_2[0][0]              \n",
    "#                                                                  activation_500[0][0]             \n",
    "# ==================================================================================================\n",
    "# Total params: 21,802,784\n",
    "# Trainable params: 21,768,352\n",
    "# Non-trainable params: 34,432\n",
    "# __________________________________________________________________________________________________\n",
    "\n",
    "# # model = InceptionResNetV2(include_top=False,\n",
    "# #                    weights='imagenet',\n",
    "# #                    input_shape=(512,512,3))\n",
    "# # #     bn = BatchNormalization()(input_tensor)\n",
    "# # model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(x,blocks=5,start_filters=256):\n",
    "    for i in range(blocks):\n",
    "        x = Conv2D(start_filters//(2**i),(3,3), activation='relu', padding='same')(x)\n",
    "        x = Conv2D(start_filters//(2**i),(3,3), activation='relu', padding='same')(x)\n",
    "        x = UpSampling2D()(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=input_shape)\n",
    "    bn = BatchNormalization(name='bn1')(input_tensor)\n",
    "    x = base_model(bn)\n",
    "    x1 = x\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "#     print(x.shape)\n",
    "#     x = Conv2D(32, kernel_size=(1,1), activation='relu', name='conv1')(x)\n",
    "#     y = Dense(14 * 14 * 2048)(x)\n",
    "#     x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "#     y = Reshape((16, 16, 1568))(x1)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     y = Reshape(16,16,1568)(y)\n",
    "#     y = decoder_block(y)\n",
    "#     y = Dense(3, activation='relu')(y)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape1, input_shape2, input_shape3, input_shape4, n_out):\n",
    "    input_tensor1 = Input(shape=input_shape1)\n",
    "    \n",
    "    input_tensor2 = Input(shape=input_shape2)\n",
    "    \n",
    "    input_tensor3 = Input(shape=input_shape3)\n",
    "    \n",
    "    input_tensor4 = Input(shape=input_shape4)\n",
    "    \n",
    "    x1 = Conv2D(64, kernel_size=(5,5), activation='relu', name='conv1a')(input_tensor1)\n",
    "    x1 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='same', data_format=None)(x1)\n",
    "    \n",
    "    x1 = Conv2D(128, kernel_size=(3,3), activation='relu', name='conv2a')(x1)\n",
    "    x1 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='same', data_format=None)(x1)\n",
    "    \n",
    "    x1 = Conv2D(256, kernel_size=(3,3), activation='relu', name='conv3a')(x1)\n",
    "    x1 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='same', data_format=None)(x1)\n",
    "    \n",
    "    x2 = Conv2D(64, kernel_size=(5,5), activation='relu', name='conv1b')(input_tensor2)\n",
    "    x2 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='same', data_format=None)(x2)\n",
    "    \n",
    "    x2 = Conv2D(128, kernel_size=(3,3), activation='relu', name='conv2b')(x2)\n",
    "    x2 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='same', data_format=None)(x2)\n",
    "    \n",
    "    x2 = Conv2D(256, kernel_size=(3,3), activation='relu', name='conv3b')(x2)\n",
    "    x2 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='same', data_format=None)(x2)\n",
    "    \n",
    "    x3 = Conv2D(64, kernel_size=(5,5), activation='relu', name='conv1c')(input_tensor3)\n",
    "    x3 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='same', data_format=None)(x3)\n",
    "    \n",
    "    x3 = Conv2D(128, kernel_size=(3,3), activation='relu', name='conv2c')(x3)\n",
    "    x3 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='same', data_format=None)(x3)\n",
    "    \n",
    "    x3 = Conv2D(256, kernel_size=(3,3), activation='relu', name='conv3c')(x3)\n",
    "    x3 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='same', data_format=None)(x3)\n",
    "    \n",
    "    x4 = Conv2D(64, kernel_size=(5,5), activation='relu', name='conv1d')(input_tensor4)\n",
    "    x4 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='same', data_format=None)(x4)\n",
    "    \n",
    "    x4 = Conv2D(128, kernel_size=(3,3), activation='relu', name='conv2d')(x4)\n",
    "    x4 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='same', data_format=None)(x4)\n",
    "    \n",
    "    x4 = Conv2D(256, kernel_size=(3,3), activation='relu', name='conv3d')(x4)\n",
    "    x4 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='same', data_format=None)(x4)\n",
    "    \n",
    "    x = Concatenate([x1, x2, x3, x4])\n",
    "    \n",
    "    x = Conv2D(256, kernel_size=(3,3), activation='relu', name='conv4')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='same', data_format=None)(x)\n",
    "    \n",
    "    output = Conv2D(n_out, kernel_size=(3,3), activation='sigmoid', name='confidence_map')(x)\n",
    "    \n",
    "    model = Model([input_tensor1, input_tensor2, input_tensor3, input_tensor4], output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401408"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14*14*2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "401408/(16*16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512, 512, 3)       0         \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 512, 512, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 14, 14, 2048)      21802784  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 23,929,672\n",
      "Trainable params: 23,895,234\n",
      "Non-trainable params: 34,438\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# warm up model\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3), \n",
    "    n_out=28)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "# https://github.com/ailias/Focal-Loss-implement-on-Tensorflow/blob/master/focal_loss.py\n",
    "def focal_loss_org(prediction_tensor, target_tensor, weights=None, alpha=0.25, gamma=2):\n",
    "    r\"\"\"Compute focal loss for predictions.\n",
    "        Multi-labels Focal loss formula:\n",
    "            FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                 ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "    Args:\n",
    "     prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing the predicted logits for each class\n",
    "     target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing one-hot encoded classification targets\n",
    "     weights: A float tensor of shape [batch_size, num_anchors]\n",
    "     alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "     gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "    Returns:\n",
    "        loss: A (scalar) tensor representing the value of the loss function\n",
    "    \"\"\"\n",
    "    sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                          - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(weights=None, alpha=0.25, gamma=2):\n",
    "    def focal_loss_my(target_tensor, prediction_tensor, ):\n",
    "        r\"\"\"Compute focal loss for predictions.\n",
    "            Multi-labels Focal loss formula:\n",
    "                FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                     ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "        Args:\n",
    "         prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "            num_classes] representing the predicted logits for each class\n",
    "         target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "            num_classes] representing one-hot encoded classification targets\n",
    "         weights: A float tensor of shape [batch_size, num_anchors]\n",
    "         alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "         gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "        Returns:\n",
    "            loss: A (scalar) tensor representing the value of the loss function\n",
    "        \"\"\"\n",
    "        sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "        zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "\n",
    "        # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "        # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "        pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "\n",
    "        # For negative prediction, only need consider back part loss, front part is 0;\n",
    "        # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "        neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "        per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                              - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "        return tf.reduce_sum(per_entry_cross_ent)\n",
    "#         return K.mean(K.binary_crossentropy(target_tensor, prediction_tensor), axis=-1) + tf.reduce_sum(per_entry_cross_ent)\n",
    "    return focal_loss_my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_fixed(y_true, y_pred):\n",
    "    gamma = 2.\n",
    "    alpha = 0.25\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "#     pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "#     pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def focal_loss(gamma=2., alpha=.25):\n",
    "#     def focal_loss_fixed(y_true, y_pred):\n",
    "#         pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "#         pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "#         pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "#         pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "#         return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "#     return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epochs = 10; batch_size = 16\n",
    "checkpoint = ModelCheckpoint('../cache/IV3-75-maximus.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
    "                                   verbose=1, mode='auto', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=6)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# split data into train, valid\n",
    "indexes = np.arange(train_dataset_info.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "train_indexes, valid_indexes = train_test_split(indexes, test_size=0.15, random_state=8)\n",
    "\n",
    "# create train and valid datagens\n",
    "# train_generator = data_generator.create_train(\n",
    "#     train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "# validation_generator = data_generator.create_train(\n",
    "#     train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 512, 512, 3)       0         \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 512, 512, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 14, 14, 2048)      21802784  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 23,929,672\n",
      "Trainable params: 23,895,234\n",
      "Non-trainable params: 34,438\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# warm up model\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3), \n",
    "    n_out=28)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_class_weights(y_true):\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    number_dim = np.shape(y_true)[1]\n",
    "    weights = np.empty([number_dim, 2])\n",
    "    for i in range(number_dim):\n",
    "        weights[i] = compute_class_weight('balanced', [0.,1.], y_true[:, i])\n",
    "        print(i, weights[i])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_true_ohe = []\n",
    "for ii in range(len(train_dataset_info)):\n",
    "#     if ii != 0:\n",
    "#         continue\n",
    "    labels = train_dataset_info[ii]['labels']\n",
    "#     print(labels)\n",
    "    y_true.append(labels)\n",
    "    ohe = np.zeros(28, dtype='int')\n",
    "    for label in labels:\n",
    "#         print(label)\n",
    "        ohe[label] = 1\n",
    "    y_true_ohe.append(list(ohe))\n",
    "y_true_ohe = np.array(y_true_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.85423654 1.20574311]\n",
      "1 [ 0.52102757 12.3891547 ]\n",
      "2 [0.56595388 4.29052748]\n",
      "3 [0.52644777 9.95259449]\n",
      "4 [0.53179982 8.36167922]\n",
      "5 [0.54399664 6.18225229]\n",
      "6 [ 0.51676424 15.41269841]\n",
      "7 [0.5499469  5.50531538]\n",
      "8 [  0.50085432 293.13207547]\n",
      "9 [  0.50072517 345.24444444]\n",
      "10 [5.00450973e-01 5.54857143e+02]\n",
      "11 [ 0.51822943 14.21408966]\n",
      "12 [ 0.51132175 22.58139535]\n",
      "13 [ 0.50879319 28.9310987 ]\n",
      "14 [ 0.51776311 14.57410882]\n",
      "15 [5.00338153e-01 7.39809524e+02]\n",
      "16 [ 0.50867658 29.31320755]\n",
      "17 [ 0.50340224 73.98095238]\n",
      "18 [ 0.51494862 17.22394678]\n",
      "19 [ 0.52504224 10.4831309 ]\n",
      "20 [ 0.50278317 90.3255814 ]\n",
      "21 [0.5691885  4.11331745]\n",
      "22 [ 0.51324744 19.37157107]\n",
      "23 [0.55274487 5.23979764]\n",
      "24 [ 0.50523577 48.2484472 ]\n",
      "25 [0.68009105 1.88818668]\n",
      "26 [ 0.50533437 47.36585366]\n",
      "27 [5.00177071e-01 1.41236364e+03]\n"
     ]
    }
   ],
   "source": [
    "weights = calculating_class_weights(y_true_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.layers[0].trainable = False\n",
    "model.layers[1].trainable = False\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "# model.layers[-1].trainable = True\n",
    "# model.layers[-2].trainable = True\n",
    "# model.layers[-3].trainable = True\n",
    "# model.layers[-4].trainable = True\n",
    "# model.layers[-5].trainable = True\n",
    "# model.layers[-6].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = np.zeros((28))\n",
    "# labels[0] = 1\n",
    "model.compile(\n",
    "    loss=[get_weighted_loss(weights)],\n",
    "    optimizer=Adam(1e-03),\n",
    "    metrics=[f1])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1651/1651 [==============================] - 496s 300ms/step - loss: 1.6240 - f1: 0.0886 - val_loss: 1.5698 - val_f1: 0.0949\n",
      "Epoch 2/2\n",
      "1651/1651 [==============================] - 458s 277ms/step - loss: 1.5683 - f1: 0.0938 - val_loss: 1.5701 - val_f1: 0.0968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe6168df748>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# create train and valid datagens\n",
    "train_generator = data_generator.create_train(\n",
    "    train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "validation_generator = data_generator.create_train(\n",
    "    train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=True)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=2, \n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "1651/1651 [==============================] - 944s 572ms/step - loss: 1.5734 - f1: 0.0921 - val_loss: 1.7044 - val_f1: 0.0860\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.70440, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 2/120\n",
      "1651/1651 [==============================] - 916s 555ms/step - loss: 1.5724 - f1: 0.0920 - val_loss: 1.5845 - val_f1: 0.0898\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.70440 to 1.58446, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 3/120\n",
      "1651/1651 [==============================] - 920s 557ms/step - loss: 1.5788 - f1: 0.0920 - val_loss: 1.5810 - val_f1: 0.0958\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.58446 to 1.58104, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 4/120\n",
      "1651/1651 [==============================] - 919s 557ms/step - loss: 1.5598 - f1: 0.0959 - val_loss: 1.5677 - val_f1: 0.0913\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.58104 to 1.56769, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 5/120\n",
      "1651/1651 [==============================] - 912s 552ms/step - loss: 1.5720 - f1: 0.0973 - val_loss: 1.6470 - val_f1: 0.0991\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.56769\n",
      "Epoch 6/120\n",
      "1651/1651 [==============================] - 918s 556ms/step - loss: 1.5600 - f1: 0.1009 - val_loss: 1.5650 - val_f1: 0.1014\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.56769 to 1.56498, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 7/120\n",
      "1651/1651 [==============================] - 913s 553ms/step - loss: 1.5533 - f1: 0.1070 - val_loss: 1.5629 - val_f1: 0.1112\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.56498 to 1.56292, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 8/120\n",
      "1651/1651 [==============================] - 923s 559ms/step - loss: 1.5613 - f1: 0.1082 - val_loss: 2.3806 - val_f1: 0.1020\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.56292\n",
      "Epoch 9/120\n",
      "1651/1651 [==============================] - 910s 551ms/step - loss: 1.5523 - f1: 0.1076 - val_loss: 1.5573 - val_f1: 0.1099\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.56292 to 1.55731, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 10/120\n",
      "1651/1651 [==============================] - 921s 558ms/step - loss: 1.5429 - f1: 0.1113 - val_loss: 1.6789 - val_f1: 0.1173\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.55731\n",
      "Epoch 11/120\n",
      "1651/1651 [==============================] - 911s 552ms/step - loss: 1.5357 - f1: 0.1123 - val_loss: 1.5831 - val_f1: 0.1157\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.55731\n",
      "Epoch 12/120\n",
      "1651/1651 [==============================] - 912s 552ms/step - loss: 1.5280 - f1: 0.1137 - val_loss: 1.5522 - val_f1: 0.1187\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.55731 to 1.55215, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 13/120\n",
      "1651/1651 [==============================] - 923s 559ms/step - loss: 1.5282 - f1: 0.1141 - val_loss: 1.5311 - val_f1: 0.1196\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.55215 to 1.53106, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 14/120\n",
      "1651/1651 [==============================] - 918s 556ms/step - loss: 1.5195 - f1: 0.1150 - val_loss: 1.9246 - val_f1: 0.1150\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.53106\n",
      "Epoch 15/120\n",
      "1651/1651 [==============================] - 927s 561ms/step - loss: 1.5196 - f1: 0.1175 - val_loss: 1.5707 - val_f1: 0.1040\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.53106\n",
      "Epoch 16/120\n",
      "1651/1651 [==============================] - 904s 548ms/step - loss: 1.5206 - f1: 0.1177 - val_loss: 1.5119 - val_f1: 0.1242\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.53106 to 1.51191, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 17/120\n",
      "1651/1651 [==============================] - 928s 562ms/step - loss: 1.5113 - f1: 0.1187 - val_loss: 1.5298 - val_f1: 0.1277\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.51191\n",
      "Epoch 18/120\n",
      "1651/1651 [==============================] - 916s 555ms/step - loss: 1.5022 - f1: 0.1212 - val_loss: 1.4954 - val_f1: 0.1287\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.51191 to 1.49539, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 19/120\n",
      "1651/1651 [==============================] - 926s 561ms/step - loss: 1.4997 - f1: 0.1219 - val_loss: 1.5551 - val_f1: 0.1344\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.49539\n",
      "Epoch 20/120\n",
      "1651/1651 [==============================] - 919s 557ms/step - loss: 1.4847 - f1: 0.1230 - val_loss: 1.6888 - val_f1: 0.0993\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.49539\n",
      "Epoch 21/120\n",
      "1651/1651 [==============================] - 925s 560ms/step - loss: 1.4848 - f1: 0.1248 - val_loss: 1.5746 - val_f1: 0.1196\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.49539\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 22/120\n",
      "1651/1651 [==============================] - 915s 554ms/step - loss: 1.4408 - f1: 0.1279 - val_loss: 1.4597 - val_f1: 0.1398\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.49539 to 1.45966, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 23/120\n",
      "1651/1651 [==============================] - 913s 553ms/step - loss: 1.4404 - f1: 0.1303 - val_loss: 1.4588 - val_f1: 0.1409\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.45966 to 1.45884, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 24/120\n",
      "1651/1651 [==============================] - 920s 557ms/step - loss: 1.4334 - f1: 0.1299 - val_loss: 1.4468 - val_f1: 0.1424\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.45884 to 1.44679, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 25/120\n",
      "1651/1651 [==============================] - 909s 550ms/step - loss: 1.4215 - f1: 0.1307 - val_loss: 1.4383 - val_f1: 0.1433\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.44679 to 1.43830, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 26/120\n",
      "1651/1651 [==============================] - 918s 556ms/step - loss: 1.4132 - f1: 0.1320 - val_loss: 1.4367 - val_f1: 0.1409\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.43830 to 1.43667, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 27/120\n",
      "1651/1651 [==============================] - 907s 549ms/step - loss: 1.4112 - f1: 0.1322 - val_loss: 1.4372 - val_f1: 0.1442\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.43667\n",
      "Epoch 28/120\n",
      "1651/1651 [==============================] - 920s 557ms/step - loss: 1.4147 - f1: 0.1330 - val_loss: 1.4219 - val_f1: 0.1467\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.43667 to 1.42189, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 29/120\n",
      "1651/1651 [==============================] - 920s 557ms/step - loss: 1.4067 - f1: 0.1340 - val_loss: 1.4348 - val_f1: 0.1480\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.42189\n",
      "Epoch 30/120\n",
      "1651/1651 [==============================] - 905s 548ms/step - loss: 1.4000 - f1: 0.1342 - val_loss: 1.4387 - val_f1: 0.1466\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.42189\n",
      "Epoch 31/120\n",
      "1651/1651 [==============================] - 906s 549ms/step - loss: 1.3948 - f1: 0.1341 - val_loss: 1.4189 - val_f1: 0.1469\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.42189 to 1.41886, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 32/120\n",
      "1651/1651 [==============================] - 906s 549ms/step - loss: 1.4048 - f1: 0.1346 - val_loss: 1.4156 - val_f1: 0.1472\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.41886 to 1.41562, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 33/120\n",
      "1651/1651 [==============================] - 914s 553ms/step - loss: 1.3931 - f1: 0.1353 - val_loss: 1.4433 - val_f1: 0.1439\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.41562\n",
      "Epoch 34/120\n",
      "1651/1651 [==============================] - 917s 556ms/step - loss: 1.3917 - f1: 0.1358 - val_loss: 1.4493 - val_f1: 0.1491\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.41562\n",
      "Epoch 35/120\n",
      "1651/1651 [==============================] - 928s 562ms/step - loss: 1.3778 - f1: 0.1367 - val_loss: 1.4213 - val_f1: 0.1505\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.41562\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 36/120\n",
      "1651/1651 [==============================] - 918s 556ms/step - loss: 1.3811 - f1: 0.1364 - val_loss: 1.4179 - val_f1: 0.1501\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.41562\n",
      "Epoch 37/120\n",
      "1651/1651 [==============================] - 930s 563ms/step - loss: 1.3874 - f1: 0.1366 - val_loss: 1.4054 - val_f1: 0.1503\n",
      "\n",
      "Epoch 00037: val_loss improved from 1.41562 to 1.40544, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 38/120\n",
      "1651/1651 [==============================] - 905s 548ms/step - loss: 1.3789 - f1: 0.1376 - val_loss: 1.4071 - val_f1: 0.1522\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.40544\n",
      "Epoch 39/120\n",
      "1651/1651 [==============================] - 915s 554ms/step - loss: 1.3738 - f1: 0.1370 - val_loss: 1.4090 - val_f1: 0.1506\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.40544\n",
      "Epoch 40/120\n",
      "1651/1651 [==============================] - 908s 550ms/step - loss: 1.3818 - f1: 0.1375 - val_loss: 1.4104 - val_f1: 0.1517\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.40544\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 41/120\n",
      "1651/1651 [==============================] - 914s 554ms/step - loss: 1.3810 - f1: 0.1382 - val_loss: 1.4205 - val_f1: 0.1513\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.40544\n",
      "Epoch 42/120\n",
      "1651/1651 [==============================] - 910s 551ms/step - loss: 1.3855 - f1: 0.1373 - val_loss: 1.4147 - val_f1: 0.1507\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.40544\n",
      "Epoch 43/120\n",
      "1651/1651 [==============================] - 909s 551ms/step - loss: 1.3820 - f1: 0.1378 - val_loss: 1.4021 - val_f1: 0.1518\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.40544 to 1.40214, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 44/120\n",
      "1651/1651 [==============================] - 911s 552ms/step - loss: 1.3772 - f1: 0.1368 - val_loss: 1.4029 - val_f1: 0.1519\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.40214\n",
      "Epoch 45/120\n",
      "1651/1651 [==============================] - 905s 548ms/step - loss: 1.3798 - f1: 0.1374 - val_loss: 1.3999 - val_f1: 0.1522\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.40214 to 1.39986, saving model to ../cache/IV3-75-maximus.h5\n",
      "Epoch 46/120\n",
      "1651/1651 [==============================] - 911s 552ms/step - loss: 1.3807 - f1: 0.1377 - val_loss: 1.4104 - val_f1: 0.1505\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.39986\n",
      "Epoch 47/120\n",
      "1651/1651 [==============================] - 896s 542ms/step - loss: 1.3811 - f1: 0.1376 - val_loss: 1.4072 - val_f1: 0.1516\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.39986\n",
      "Epoch 48/120\n",
      "1651/1651 [==============================] - 898s 544ms/step - loss: 1.3806 - f1: 0.1379 - val_loss: 1.4092 - val_f1: 0.1516\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.39986\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "Epoch 49/120\n",
      "1651/1651 [==============================] - 917s 556ms/step - loss: 1.3899 - f1: 0.1373 - val_loss: 1.4013 - val_f1: 0.1516\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.39986\n",
      "Epoch 50/120\n",
      "1651/1651 [==============================] - 911s 552ms/step - loss: 1.3866 - f1: 0.1371 - val_loss: 1.4047 - val_f1: 0.1518\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.39986\n",
      "Epoch 51/120\n",
      "1650/1651 [============================>.] - ETA: 0s - loss: 1.3846 - f1: 0.1380"
     ]
    }
   ],
   "source": [
    "# train all layers\n",
    "epochs=120\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.compile(loss=[get_weighted_loss(weights)],\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=[f1])\n",
    "# model.load_weights('../cache/IV3-35-maximus.h5')\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
    "    epochs=epochs, \n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 11702/11702 [09:08<00:00, 20.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create submit\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "predicted = []\n",
    "draw_predict = []\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3), \n",
    "    n_out=28)\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.compile(loss=f1_loss,\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=[f1])\n",
    "model.load_weights('../cache/IV3-37-maximus.h5')\n",
    "for name in tqdm(submit['Id']):\n",
    "    path = os.path.join('../data/test/', name)\n",
    "    image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
    "    score_predict = model.predict(image[np.newaxis])[0]\n",
    "    draw_predict.append(score_predict)\n",
    "    label_predict = np.arange(28)[score_predict[0]>=0.2]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted\n",
    "# np.save('../cache/draw_predict_InceptionV3-30.npy', score_predict)\n",
    "# submit.to_csv('../submissions/submit_InceptionV3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('../submissions/sub35-max.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/1855095/how-to-create-a-zip-archive-of-a-directory\n",
    "def backup_project_as_zip(project_dir, zip_file):\n",
    "    assert(os.path.isdir(project_dir))\n",
    "    assert(os.path.isdir(os.path.dirname(zip_file)))\n",
    "    shutil.make_archive(zip_file.replace('.zip',''), 'zip', project_dir)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-26 06:28:07.734445\n"
     ]
    }
   ],
   "source": [
    "import datetime, shutil\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "PROJECT_PATH = '/home/watts/lal/Kaggle/kagglehp/scripts_nbs'\n",
    "backup_project_as_zip(PROJECT_PATH, '../cache/code.scripts_nbs.%s.zip'%now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 481k/481k [00:14<00:00, 34.5kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationCPU times: user 366 ms, sys: 172 ms, total: 538 ms\n",
      "Wall time: 17.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub35-max.csv -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 11702/11702 [00:00<00:00, 101300.98it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for score_predict in tqdm(draw_predict):\n",
    "    \n",
    "    label_predict = np.arange(28)[score_predict[0]>=0.5]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub35-max-a.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 467k/467k [00:13<00:00, 35.6kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationfileName         date                 description  status    publicScore  privateScore  \n",
      "---------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub35-max-a.csv  2018-11-27 09:45:10               complete  0.456        None          \n",
      "sub36-max.csv    2018-11-27 06:40:12               complete  0.469        None          \n",
      "sub37-c.csv      2018-11-26 21:04:50               complete  0.437        None          \n",
      "sub37-b.csv      2018-11-26 21:04:21               complete  0.457        None          \n",
      "sub37-a.csv      2018-11-26 21:03:26               complete  0.464        None          \n",
      "sub35-max.csv    2018-11-26 00:58:38               complete  0.473        None          \n",
      "sub34b-max.csv   2018-11-24 19:03:59               complete  0.459        None          \n",
      "sub34a-max.csv   2018-11-24 18:50:22               complete  0.469        None          \n",
      "sub34-max.csv    2018-11-24 17:27:36               complete  0.473        None          \n",
      "sub33-h.csv      2018-11-24 06:48:41               complete  0.464        None          \n",
      "sub33-g.csv      2018-11-24 06:46:19               complete  0.472        None          \n",
      "sub33-c.csv      2018-11-23 11:48:41               complete  0.493        None          \n",
      "sub33-bb.csv     2018-11-23 11:47:32               complete  0.493        None          \n",
      "sub33-b.csv      2018-11-23 11:46:26               complete  0.498        None          \n",
      "sub33-a.csv      2018-11-23 11:45:09               complete  0.496        None          \n",
      "sub36-a.csv      2018-11-23 10:12:45               complete  0.287        None          \n",
      "sub35b-c.csv     2018-11-22 08:00:23               complete  0.417        None          \n",
      "sub35b-b.csv     2018-11-22 07:59:44               complete  0.415        None          \n",
      "sub35b-a.csv     2018-11-22 07:58:57               complete  0.432        None          \n",
      "sub35-a.csv      2018-11-20 06:54:01               complete  0.315        None          \n",
      "CPU times: user 1.2 s, sys: 507 ms, total: 1.71 s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub35-max-a.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(20)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 11702/11702 [00:00<00:00, 81479.84it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for score_predict in tqdm(draw_predict):\n",
    "    \n",
    "    label_predict = np.arange(28)[score_predict[0]>=0.45]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub35-max-b.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 467k/467k [00:12<00:00, 39.1kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationfileName         date                 description  status    publicScore  privateScore  \n",
      "---------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub35-max-b.csv  2018-11-27 09:45:48               complete  0.456        None          \n",
      "sub35-max-a.csv  2018-11-27 09:45:10               complete  0.456        None          \n",
      "sub36-max.csv    2018-11-27 06:40:12               complete  0.469        None          \n",
      "sub37-c.csv      2018-11-26 21:04:50               complete  0.437        None          \n",
      "sub37-b.csv      2018-11-26 21:04:21               complete  0.457        None          \n",
      "sub37-a.csv      2018-11-26 21:03:26               complete  0.464        None          \n",
      "sub35-max.csv    2018-11-26 00:58:38               complete  0.473        None          \n",
      "sub34b-max.csv   2018-11-24 19:03:59               complete  0.459        None          \n",
      "sub34a-max.csv   2018-11-24 18:50:22               complete  0.469        None          \n",
      "sub34-max.csv    2018-11-24 17:27:36               complete  0.473        None          \n",
      "sub33-h.csv      2018-11-24 06:48:41               complete  0.464        None          \n",
      "sub33-g.csv      2018-11-24 06:46:19               complete  0.472        None          \n",
      "sub33-c.csv      2018-11-23 11:48:41               complete  0.493        None          \n",
      "sub33-bb.csv     2018-11-23 11:47:32               complete  0.493        None          \n",
      "sub33-b.csv      2018-11-23 11:46:26               complete  0.498        None          \n",
      "sub33-a.csv      2018-11-23 11:45:09               complete  0.496        None          \n",
      "sub36-a.csv      2018-11-23 10:12:45               complete  0.287        None          \n",
      "sub35b-c.csv     2018-11-22 08:00:23               complete  0.417        None          \n",
      "sub35b-b.csv     2018-11-22 07:59:44               complete  0.415        None          \n",
      "sub35b-a.csv     2018-11-22 07:58:57               complete  0.432        None          \n",
      "CPU times: user 451 ms, sys: 200 ms, total: 650 ms\n",
      "Wall time: 38.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub35-max-b.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(20)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 11702/11702 [00:00<00:00, 84763.56it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for score_predict in tqdm(draw_predict):\n",
    "    \n",
    "    label_predict = np.arange(28)[score_predict[0]>=0.35]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub35-max-c.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 472k/472k [00:13<00:00, 37.0kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationfileName         date                 description  status    publicScore  privateScore  \n",
      "---------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub35-max-c.csv  2018-11-27 09:47:12               complete  0.458        None          \n",
      "sub35-max-b.csv  2018-11-27 09:45:48               complete  0.456        None          \n",
      "sub35-max-a.csv  2018-11-27 09:45:10               complete  0.456        None          \n",
      "sub36-max.csv    2018-11-27 06:40:12               complete  0.469        None          \n",
      "sub37-c.csv      2018-11-26 21:04:50               complete  0.437        None          \n",
      "sub37-b.csv      2018-11-26 21:04:21               complete  0.457        None          \n",
      "sub37-a.csv      2018-11-26 21:03:26               complete  0.464        None          \n",
      "sub35-max.csv    2018-11-26 00:58:38               complete  0.473        None          \n",
      "sub34b-max.csv   2018-11-24 19:03:59               complete  0.459        None          \n",
      "sub34a-max.csv   2018-11-24 18:50:22               complete  0.469        None          \n",
      "sub34-max.csv    2018-11-24 17:27:36               complete  0.473        None          \n",
      "sub33-h.csv      2018-11-24 06:48:41               complete  0.464        None          \n",
      "sub33-g.csv      2018-11-24 06:46:19               complete  0.472        None          \n",
      "sub33-c.csv      2018-11-23 11:48:41               complete  0.493        None          \n",
      "sub33-bb.csv     2018-11-23 11:47:32               complete  0.493        None          \n",
      "sub33-b.csv      2018-11-23 11:46:26               complete  0.498        None          \n",
      "sub33-a.csv      2018-11-23 11:45:09               complete  0.496        None          \n",
      "sub36-a.csv      2018-11-23 10:12:45               complete  0.287        None          \n",
      "sub35b-c.csv     2018-11-22 08:00:23               complete  0.417        None          \n",
      "sub35b-b.csv     2018-11-22 07:59:44               complete  0.415        None          \n",
      "CPU times: user 439 ms, sys: 210 ms, total: 649 ms\n",
      "Wall time: 39.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub35-max-c.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(20)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 11702/11702 [00:00<00:00, 83258.69it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for score_predict in tqdm(draw_predict):\n",
    "    \n",
    "    label_predict = np.arange(28)[score_predict[0]>=0.25]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "submit['Predicted'] = predicted\n",
    "submit.to_csv('../submissions/sub35-max-d.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 477k/477k [00:13<00:00, 37.3kB/s]\n",
      "Successfully submitted to Human Protein Atlas Image ClassificationfileName         date                 description  status    publicScore  privateScore  \n",
      "---------------  -------------------  -----------  --------  -----------  ------------  \n",
      "sub35-max-d.csv  2018-11-27 09:55:56               complete  0.472        None          \n",
      "sub35-max-c.csv  2018-11-27 09:47:12               complete  0.458        None          \n",
      "sub35-max-b.csv  2018-11-27 09:45:48               complete  0.456        None          \n",
      "sub35-max-a.csv  2018-11-27 09:45:10               complete  0.456        None          \n",
      "sub36-max.csv    2018-11-27 06:40:12               complete  0.469        None          \n",
      "sub37-c.csv      2018-11-26 21:04:50               complete  0.437        None          \n",
      "sub37-b.csv      2018-11-26 21:04:21               complete  0.457        None          \n",
      "sub37-a.csv      2018-11-26 21:03:26               complete  0.464        None          \n",
      "sub35-max.csv    2018-11-26 00:58:38               complete  0.473        None          \n",
      "sub34b-max.csv   2018-11-24 19:03:59               complete  0.459        None          \n",
      "sub34a-max.csv   2018-11-24 18:50:22               complete  0.469        None          \n",
      "sub34-max.csv    2018-11-24 17:27:36               complete  0.473        None          \n",
      "sub33-h.csv      2018-11-24 06:48:41               complete  0.464        None          \n",
      "sub33-g.csv      2018-11-24 06:46:19               complete  0.472        None          \n",
      "sub33-c.csv      2018-11-23 11:48:41               complete  0.493        None          \n",
      "sub33-bb.csv     2018-11-23 11:47:32               complete  0.493        None          \n",
      "sub33-b.csv      2018-11-23 11:46:26               complete  0.498        None          \n",
      "sub33-a.csv      2018-11-23 11:45:09               complete  0.496        None          \n",
      "sub36-a.csv      2018-11-23 10:12:45               complete  0.287        None          \n",
      "sub35b-c.csv     2018-11-22 08:00:23               complete  0.417        None          \n",
      "CPU times: user 484 ms, sys: 267 ms, total: 751 ms\n",
      "Wall time: 42.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kaggle competitions submit -c human-protein-atlas-image-classification -f ../submissions/sub35-max-d.csv -m \"\"\n",
    "from time import sleep\n",
    "sleep(20)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName        date                 description  status    publicScore  privateScore  \r\n",
      "--------------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub35-max.csv   2018-11-26 00:58:38               complete  0.473        None          \r\n",
      "sub34b-max.csv  2018-11-24 19:03:59               complete  0.459        None          \r\n",
      "sub34a-max.csv  2018-11-24 18:50:22               complete  0.469        None          \r\n",
      "sub34-max.csv   2018-11-24 17:27:36               complete  0.473        None          \r\n",
      "sub33-h.csv     2018-11-24 06:48:41               complete  0.464        None          \r\n",
      "sub33-g.csv     2018-11-24 06:46:19               complete  0.472        None          \r\n",
      "sub33-c.csv     2018-11-23 11:48:41               complete  0.493        None          \r\n",
      "sub33-bb.csv    2018-11-23 11:47:32               complete  0.493        None          \r\n",
      "sub33-b.csv     2018-11-23 11:46:26               complete  0.498        None          \r\n",
      "sub33-a.csv     2018-11-23 11:45:09               complete  0.496        None          \r\n",
      "sub36-a.csv     2018-11-23 10:12:45               complete  0.287        None          \r\n",
      "sub35b-c.csv    2018-11-22 08:00:23               complete  0.417        None          \r\n",
      "sub35b-b.csv    2018-11-22 07:59:44               complete  0.415        None          \r\n",
      "sub35b-a.csv    2018-11-22 07:58:57               complete  0.432        None          \r\n",
      "sub35-a.csv     2018-11-20 06:54:01               complete  0.315        None          \r\n",
      "sub8i1-e.csv    2018-11-19 07:12:37               complete  0.457        None          \r\n",
      "sub8i1-d.csv    2018-11-19 07:09:46               complete  0.459        None          \r\n",
      "sub8i1-c.csv    2018-11-19 07:08:49               complete  0.462        None          \r\n",
      "sub8i1-b.csv    2018-11-19 07:08:05               complete  0.462        None          \r\n",
      "sub8i1-a.csv    2018-11-19 07:07:14               complete  0.460        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(60)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.4.7.1 / client 1.3.8)\r\n",
      "fileName  date                 description  status    publicScore  privateScore  \r\n",
      "--------  -------------------  -----------  --------  -----------  ------------  \r\n",
      "sub8.csv  2018-10-20 20:08:45               complete  0.422        None          \r\n",
      "sub7.csv  2018-10-20 17:06:09               complete  0.389        None          \r\n",
      "sub5.csv  2018-10-19 18:27:33               complete  0.387        None          \r\n",
      "sub4.csv  2018-10-19 14:45:15               complete  0.411        None          \r\n",
      "sub3.csv  2018-10-19 10:19:26               complete  0.377        None          \r\n",
      "sub2.csv  2018-10-19 08:07:30               complete  0.135        None          \r\n",
      "sub1.csv  2018-10-19 06:28:57               complete  0.374        None          \r\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(60)\n",
    "!kaggle competitions submissions -c human-protein-atlas-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpg",
   "language": "python",
   "name": "hpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
